% 75
@article{wu_sustainable_2021,
	title = {Sustainable {AI}: {Environmental} {Implications}, {Challenges} and {Opportunities}},
	copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
	shorttitle = {Sustainable {AI}},
	url = {https://arxiv.org/abs/2111.00364},
	doi = {10.48550/ARXIV.2111.00364},
	abstract = {This paper explores the environmental impact of the super-linear growth trends for AI from a holistic perspective, spanning Data, Algorithms, and System Hardware. We characterize the carbon footprint of AI computing by examining the model development cycle across industry-scale machine learning use cases and, at the same time, considering the life cycle of system hardware. Taking a step further, we capture the operational and manufacturing carbon footprint of AI computing and present an end-to-end analysis for what and how hardware-software design and at-scale optimization can help reduce the overall carbon footprint of AI. Based on the industry experience and lessons learned, we share the key challenges and chart out important development directions across the many dimensions of AI. We hope the key messages and insights presented in this paper can inspire the community to advance the field of AI in an environmentally-responsible manner.},
	urldate = {2023-06-20},
	author = {Wu, Carole-Jean and Raghavendra, Ramya and Gupta, Udit and Acun, Bilge and Ardalani, Newsha and Maeng, Kiwan and Chang, Gloria and Behram, Fiona Aga and Huang, James and Bai, Charles and Gschwind, Michael and Gupta, Anurag and Ott, Myle and Melnikov, Anastasia and Candido, Salvatore and Brooks, David and Chauhan, Geeta and Lee, Benjamin and Lee, Hsien-Hsin S. and Akyildiz, Bugra and Balandat, Maximilian and Spisak, Joe and Jain, Ravi and Rabbat, Mike and Hazelwood, Kim},
	year = {2021},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, Hardware Architecture (cs.AR), Machine Learning (cs.LG)},
}

% 76
@inproceedings{zhao_greener_2022,
	address = {Lyon, France},
	title = {A {Green}(er) {World} for {A}.{I}.},
	isbn = {978-1-66549-747-3},
	url = {https://ieeexplore.ieee.org/document/9835179/},
	doi = {10.1109/IPDPSW55747.2022.00126},
	urldate = {2023-06-20},
	booktitle = {2022 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} {Workshops} ({IPDPSW})},
	publisher = {IEEE},
	author = {Zhao, Dan and Frey, Nathan C. and McDonald, Joseph and Hubbell, Matthew and Bestor, David and Jones, Michael and Prout, Andrew and Gadepally, Vijay and Samsi, Siddharth},
	month = {may},
	year = {2022},
	pages = {742--750},
	file = {Versione inviata:C\:\\Users\\alice\\Zotero\\storage\\8346ZE8B\\Zhao et al. - 2022 - A Green(er) World for A.I..pdf:application/pdf},
}

% 77 
@article{anthony_carbontracker_2020,
	title = {Carbontracker: {Tracking} and {Predicting} the {Carbon} {Footprint} of {Training} {Deep} {Learning} {Models}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Carbontracker},
	url = {https://arxiv.org/abs/2007.03051},
	doi = {10.48550/ARXIV.2007.03051},
	abstract = {Deep learning (DL) can achieve impressive results across a wide variety of tasks, but this often comes at the cost of training models for extensive periods on specialized hardware accelerators. This energy-intensive workload has seen immense growth in recent years. Machine learning (ML) may become a significant contributor to climate change if this exponential trend continues. If practitioners are aware of their energy and carbon footprint, then they may actively take steps to reduce it whenever possible. In this work, we present Carbontracker, a tool for tracking and predicting the energy and carbon footprint of training DL models. We propose that energy and carbon footprint of model development and training is reported alongside performance metrics using tools like Carbontracker. We hope this will promote responsible computing in ML and encourage research into energy-efficient deep neural networks.},
	urldate = {2023-06-20},
	author = {Anthony, Lasse F. Wolff and Kanding, Benjamin and Selvan, Raghavendra},
	year = {2020},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Computers and Society (cs.CY), FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, Machine Learning (cs.LG), Machine Learning (stat.ML), Signal Processing (eess.SP)},
}


% 78
@inproceedings{gupta2021chasing,
  title={Chasing carbon: The elusive environmental footprint of computing},
  author={Gupta, Udit and Kim, Young Geun and Lee, Sylvia and Tse, Jordan and Lee, Hsien-Hsin S and Wei, Gu-Yeon and Brooks, David and Wu, Carole-Jean},
  booktitle={2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  pages={854--867},
  year={2021},
  organization={IEEE}
}

% 79
@article{henderson2020towards,
  title={Towards the systematic reporting of the energy and carbon footprints of machine learning},
  author={Henderson, Peter and Hu, Jieru and Romoff, Joshua and Brunskill, Emma and Jurafsky, Dan and Pineau, Joelle},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={10039--10081},
  year={2020},
  publisher={JMLRORG}
}

% 80 
@ARTICLE{9810097,
  author={Patterson, David and Gonzalez, Joseph and Hölzle, Urs and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David R. and Texier, Maud and Dean, Jeff},
  journal={Computer}, 
  title={The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink}, 
  year={2022},
  volume={55},
  number={7},
  pages={18-28},
  doi={10.1109/MC.2022.3148714}}

% 81
@article{van_wynsberghe_sustainable_2021,
	title = {Sustainable {AI}: {AI} for sustainability and the sustainability of {AI}},
	volume = {1},
	issn = {2730-5953, 2730-5961},
	shorttitle = {Sustainable {AI}},
	url = {https://link.springer.com/10.1007/s43681-021-00043-6},
	doi = {10.1007/s43681-021-00043-6},
	abstract = {Abstract
            
              While there is a growing effort towards AI
              for
              Sustainability (e.g. towards the sustainable development goals) it is time to move beyond that and to address the sustainability
              of
              developing and using AI systems. In this paper I propose a definition of Sustainable AI; Sustainable AI is a movement to foster change in the entire lifecycle of AI products (i.e. idea generation, training, re-tuning, implementation, governance) towards greater ecological integrity and social justice. As such, Sustainable AI is focused on more than AI applications; rather, it addresses the whole sociotechnical system of AI. I have suggested here that Sustainable AI is not about how to sustain the development of AI per say but it is about how to develop AI that is compatible with sustaining environmental resources for current and future generations; economic models for societies; and societal values that are fundamental to a given society. I have articulated that the phrase Sustainable AI be understood as having two branches; AI
              for
              sustainability and sustainability
              of
              AI (e.g. reduction of carbon emissions and computing power). I propose that Sustainable AI take sustainable development at the core of its definition with three accompanying tensions between AI innovation and equitable resource distribution; inter and intra-generational justice; and, between environment, society, and economy. This paper is not meant to engage with each of the three pillars of sustainability (i.e. social, economic, environment), and as such the pillars of sustainable AI. Rather, this paper is meant to inspire the reader, the policy maker, the AI ethicist, the AI developer to connect with the environment—to remember that there are environmental costs to AI. Further, to direct funding towards sustainable methods
              of
              AI.},
	language = {en},
	number = {3},
	urldate = {2023-06-25},
	journal = {AI and Ethics},
	author = {Van Wynsberghe, Aimee},
	month = aug,
	year = {2021},
	pages = {213--218},
	file = {Full text:C\:\\Users\\alice\\Zotero\\storage\\M4JCFC97\\Van Wynsberghe - 2021 - Sustainable AI AI for sustainability and the sust.pdf:application/pdf},
}

% 82
@article{rohde_sustainability_2021,
	title = {Sustainability challenges of {Artificial} {Intelligence} and {Policy} {Implications}},
	volume = {36},
	issn = {1430-8800},
	url = {http://oekologisches-wirtschaften.de/index.php/oew/article/view/1792},
	doi = {10.14512/OEWO360136},
	abstract = {Automated decision-making based on Artificial Intelligence is associated with growing expectations and is to contribute to sustainable development goals. Which opportunities and risks for the environment, economy and society are associated with Artificial Intelligence-based applications and how can they be governed?},
	number = {O1},
	urldate = {2023-06-25},
	journal = {Ökologisches Wirtschaften - Fachzeitschrift},
	author = {Rohde, Friederike and Gossen, Maike and Wagner, Josephin and Santarius, Tilman},
	month = feb,
	year = {2021},
	pages = {36--40},
	file = {Full text:C\:\\Users\\alice\\Zotero\\storage\\KMVT7E5M\\Rohde et al. - 2021 - Sustainability challenges of Artificial Intelligen.pdf:application/pdf},
}

%83
@article{noauthor_measuring_2022,
   author = {OECD},
   title = {Measuring the environmental impacts of artificial intelligence compute and applications},
   year = {2022},
   number = {341},
   url = {https://www.oecd-ilibrary.org/content/paper/7babf571-en},
   doi = {https://doi.org/https://doi.org/10.1787/7babf571-en}
}

% 84

@article{perucica_is_2022,
	title = {Is the future of {AI} sustainable? {A} case study of the {European} {Union}},
	volume = {16},
	issn = {1750-6166, 1750-6166},
	shorttitle = {Is the future of {AI} sustainable?},
	url = {https://www.emerald.com/insight/content/doi/10.1108/TG-06-2021-0106/full/html},
	doi = {10.1108/TG-06-2021-0106},
	abstract = {Purpose
              The purpose of this paper is to raise awareness on the need for a more comprehensive approach on the interdependence between artificial intelligence (AI) and environmental sustainability. It provides an overview of existing sustainable AI policy initiatives at the national and regional level. More precisely, it discusses whether existing European Union (EU) environmental policies are suitable for the AI era or whether new regulations are needed in this field. Finally, this paper assesses cross-fertilisation opportunities between the EU and non-EU countries.
            
            
              Design/methodology/approach
              This study is based on a qualitative analysis of sustainable applications of AI and the sustainability of AI. Emphasis is laid on the latter, and a “sustainable by design” approach is proposed, which in essence is a prerequisite for transparent, responsible and human-centred AI systems. The analysis primarily focuses on environmental sustainability.
            
            
              Findings
              The majority of studies focus on how to use AI to protect the environment with very little attention paid to sustainable design of AI. On the other hand, the EU’s comprehensive approach towards sustainable AI is closest to promoting “sustainable by design” AI. Several ways have been identified in which the EU’s actions can be translated beyond its borders.
            
            
              Research limitations/implications
              One of the largest limitations of this study is its moderate scope. This paper is confined to the EU and as such provides a limited assessment of global policies and measures on the interplay between sustainability and AI. Consequently, the paper did not provide an in-depth analysis of environmental policies worldwide that could help provide a better picture of possible cooperation areas or common grounds. Another limitation of this study is that it primarily focuses on environmental aspects and as such accords little attention to the economic and social pillars of sustainability.
            
            
              Social implications
              With less than 10 years to go before reaching the sustainable development goal deadline, this study can help stakeholders better understand what is being done worldwide in terms of sustainable AI. Moreover, given that the technology is still in its early phase, this study can inspire a “sustainable by design” approach to the development of AI technologies.
            
            
              Originality/value
              All national AI strategies published by 1 June 2021 were analysed to identify whether and to what extent they prioritise the interplay between environment and AI. Furthermore, the authors also looked at the EU policy and how it aims to address AI from a sustainable perspective.},
	language = {en},
	number = {3},
	urldate = {2023-06-25},
	journal = {Transforming Government: People, Process and Policy},
	author = {Perucica, Natasa and Andjelkovic, Katarina},
	month = jul,
	year = {2022},
	pages = {347--358},
	file = {Full text:C\:\\Users\\alice\\Zotero\\storage\\DQ7WXAFN\\Perucica e Andjelkovic - 2022 - Is the future of AI sustainable A case study of t.pdf:application/pdf},
}

% 85
@article{piorkowski_quantitative_2022,
	title = {Quantitative {AI} {Risk} {Assessments}: {Opportunities} and {Challenges}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Quantitative {AI} {Risk} {Assessments}},
	url = {https://arxiv.org/abs/2209.06317},
	doi = {10.48550/ARXIV.2209.06317},
	abstract = {Although AI-based systems are increasingly being leveraged to provide value to organizations, individuals, and society, significant attendant risks have been identified. These risks have led to proposed regulations, litigation, and general societal concerns. As with any promising technology, organizations want to benefit from the positive capabilities of AI technology while reducing the risks. The best way to reduce risks is to implement comprehensive AI lifecycle governance where policies and procedures are described and enforced during the design, development, deployment, and monitoring of an AI system. While support for comprehensive governance is beginning to emerge, organizations often need to identify the risks of deploying an already-built model without knowledge of how it was constructed or access to its original developers. Such an assessment will quantitatively assess the risks of an existing model in a manner analogous to how a home inspector might assess the energy efficiency of an already-built home or a physician might assess overall patient health based on a battery of tests. This paper explores the concept of a quantitative AI Risk Assessment, exploring the opportunities, challenges, and potential impacts of such an approach, and discussing how it might improve AI regulations.},
	urldate = {2023-06-25},
	author = {Piorkowski, David and Hind, Michael and Richards, John},
	year = {2022},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences},
}


% 86
@article{lacoste_quantifying_2019,
	title = {Quantifying the {Carbon} {Emissions} of {Machine} {Learning}},
	copyright = {Creative Commons Attribution Share Alike 4.0 International},
	url = {https://arxiv.org/abs/1910.09700},
	doi = {10.48550/ARXIV.1910.09700},
	abstract = {From an environmental standpoint, there are a few crucial aspects of training a neural network that have a major impact on the quantity of carbon that it emits. These factors include: the location of the server used for training and the energy grid that it uses, the length of the training procedure, and even the make and model of hardware on which the training takes place. In order to approximate these emissions, we present our Machine Learning Emissions Calculator, a tool for our community to better understand the environmental impact of training ML models. We accompany this tool with an explanation of the factors cited above, as well as concrete actions that individual practitioners and organizations can take to mitigate their carbon emissions.},
	urldate = {2023-06-25},
	author = {Lacoste, Alexandre and Luccioni, Alexandra and Schmidt, Victor and Dandres, Thomas},
	year = {2019},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {Computers and Society (cs.CY), FOS: Computer and information sciences, Machine Learning (cs.LG)},
}

% 87
@article{lannelongue_green_2021,
	title = {Green {Algorithms}: {Quantifying} the {Carbon} {Footprint} of {Computation}},
	volume = {8},
	issn = {2198-3844, 2198-3844},
	shorttitle = {Green {Algorithms}},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/advs.202100707},
	doi = {10.1002/advs.202100707},
	language = {en},
	number = {12},
	urldate = {2023-06-25},
	journal = {Advanced Science},
	author = {Lannelongue, Loïc and Grealey, Jason and Inouye, Michael},
	month = jun,
	year = {2021},
	pages = {2100707},
	file = {Full text:C\:\\Users\\alice\\Zotero\\storage\\WPD7NS96\\Lannelongue et al. - 2021 - Green Algorithms Quantifying the Carbon Footprint.pdf:application/pdf},
}


% 88
@article{strubell_energy_2020,
	title = {Energy and {Policy} {Considerations} for {Modern} {Deep} {Learning} {Research}},
	volume = {34},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/7123},
	doi = {10.1609/aaai.v34i09.7123},
	abstract = {The field of artificial intelligence has experienced a dramatic methodological shift towards large neural networks trained on plentiful data. This shift has been fueled by recent advances in hardware and techniques enabling remarkable levels of computation, resulting in impressive advances in AI across many applications. However, the massive computation required to obtain these exciting results is costly both financially, due to the price of specialized hardware and electricity or cloud compute time, and to the environment, as a result of non-renewable energy used to fuel modern tensor processing hardware. In a paper published this year at ACL, we brought this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training and tuning neural network models for NLP (Strubell, Ganesh, and McCallum 2019). In this extended abstract, we briefly summarize our findings in NLP, incorporating updated estimates and broader information from recent related publications, and provide actionable recommendations to reduce costs and improve equity in the machine learning and artificial intelligence community.},
	number = {09},
	urldate = {2023-06-25},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
	month = apr,
	year = {2020},
	pages = {13693--13696},
	file = {Full text:C\:\\Users\\alice\\Zotero\\storage\\NJWF93XC\\Strubell et al. - 2020 - Energy and Policy Considerations for Modern Deep L.pdf:application/pdf},
}


% 89
@inproceedings{manotas_empirical_2016,
	address = {Austin Texas},
	title = {An empirical study of practitioners' perspectives on green software engineering},
	isbn = {978-1-4503-3900-1},
	url = {https://dl.acm.org/doi/10.1145/2884781.2884810},
	doi = {10.1145/2884781.2884810},
	language = {en},
	urldate = {2023-06-28},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Manotas, Irene and Bird, Christian and Zhang, Rui and Shepherd, David and Jaspan, Ciera and Sadowski, Caitlin and Pollock, Lori and Clause, James},
	month = may,
	year = {2016},
	pages = {237--248},
	file = {Full text:C\:\\Users\\alice\\Zotero\\storage\\SEYJN25E\\Manotas et al. - 2016 - An empirical study of practitioners' perspectives .pdf:application/pdf},
}

% 90
@inproceedings{georgiou_green_2022,
	address = {Pittsburgh Pennsylvania},
	title = {Green {AI}: do deep learning frameworks have different costs?},
	isbn = {978-1-4503-9221-1},
	shorttitle = {Green {AI}},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510221},
	doi = {10.1145/3510003.3510221},
	language = {en},
	urldate = {2023-02-20},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Georgiou, Stefanos and Kechagia, Maria and Sharma, Tushar and Sarro, Federica and Zou, Ying},
	month = {may},
	year = {2022},
	pages = {1082--1094},
}

% 91
@article{pachot_towards_2022,
	title = {Towards {Sustainable} {Artificial} {Intelligence}: {An} {Overview} of {Environmental} {Protection} {Uses} and {Issues}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Towards {Sustainable} {Artificial} {Intelligence}},
	url = {https://arxiv.org/abs/2212.11738},
	doi = {10.48550/ARXIV.2212.11738},
	abstract = {Artificial Intelligence (AI) is used to create more sustainable production methods and model climate change, making it a valuable tool in the fight against environmental degradation. This paper describes the paradox of an energy-consuming technology serving the ecological challenges of tomorrow. The study provides an overview of the sectors that use AI-based solutions for environmental protection. It draws on numerous examples from AI for Green players to present use cases and concrete examples. In the second part of the study, the negative impacts of AI on the environment and the emerging technological solutions to support Green AI are examined. It is also shown that the research on less energy-consuming AI is motivated more by cost and energy autonomy constraints than by environmental considerations. This leads to a rebound effect that favors an increase in the complexity of models. Finally, the need to integrate environmental indicators into algorithms is discussed. The environmental dimension is part of the broader ethical problem of AI, and addressing it is crucial for ensuring the sustainability of AI in the long term.},
	urldate = {2023-02-10},
	author = {Pachot, Arnault and Patissier, Céline},
	year = {2022},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences},
}

% 92
@article{vinuesa_role_2020,
	title = {The role of artificial intelligence in achieving the {Sustainable} {Development} {Goals}},
	volume = {11},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-019-14108-y},
	doi = {10.1038/s41467-019-14108-y},
	abstract = {Abstract
            The emergence of artificial intelligence (AI) and its progressively wider impact on many sectors requires an assessment of its effect on the achievement of the Sustainable Development Goals. Using a consensus-based expert elicitation process, we find that AI can enable the accomplishment of 134 targets across all the goals, but it may also inhibit 59 targets. However, current research foci overlook important aspects. The fast development of AI needs to be supported by the necessary regulatory insight and oversight for AI-based technologies to enable sustainable development. Failure to do so could result in gaps in transparency, safety, and ethical standards.},
	language = {en},
	number = {1},
	urldate = {2023-06-28},
	journal = {Nature Communications},
	author = {Vinuesa, Ricardo and Azizpour, Hossein and Leite, Iolanda and Balaam, Madeline and Dignum, Virginia and Domisch, Sami and Felländer, Anna and Langhans, Simone Daniela and Tegmark, Max and Fuso Nerini, Francesco},
	month = jan,
	year = {2020},
	pages = {233},
	file = {Full text:C\:\\Users\\alice\\Zotero\\storage\\QDT5YBMJ\\Vinuesa et al. - 2020 - The role of artificial intelligence in achieving t.pdf:application/pdf},
}

% 93
@article{patterson2021carbon,
  title={Carbon emissions and large neural network training},
  author={Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
  journal={arXiv preprint arXiv:2104.10350},
  year={2021}
}

% 94
@inproceedings{verdecchia_data-centric_2022,
	address = {Plovdiv, Bulgaria},
	title = {Data-{Centric} {Green} {AI} {An} {Exploratory} {Empirical} {Study}},
	isbn = {978-1-66548-286-8},
	url = {https://ieeexplore.ieee.org/document/9830097/},
	doi = {10.1109/ICT4S55073.2022.00015},
	urldate = {2023-06-28},
	booktitle = {2022 {International} {Conference} on {ICT} for {Sustainability} ({ICT4S})},
	publisher = {IEEE},
	author = {Verdecchia, Roberto and Cruz, Luis and Sallou, June and Lin, Michelle and Wickenden, James and Hotellier, Estelle},
	month = jun,
	year = {2022},
	pages = {35--45},
	file = {Full text:C\:\\Users\\alice\\Zotero\\storage\\KR37VN3E\\Verdecchia et al. - 2022 - Data-Centric Green AI An Exploratory Empirical Stu.pdf:application/pdf},
}

% 95
@misc{shaikh2021energyvis,
      title={EnergyVis: Interactively Tracking and Exploring Energy Consumption for ML Models}, 
      author={Omar Shaikh and Jon Saad-Falcon and Austin P Wright and Nilaksh Das and Scott Freitas and Omar Isaac Asensio and Duen Horng Chau},
      year={2021},
      eprint={2103.16435},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% 96
@Article{Devlin2018_bert,
  author  = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  title   = {Bert: Pre-training of deep bidirectional transformers for language understanding},
  journal = {arXiv preprint arXiv:1810.04805},
  year    = {2018},
}

% 97
@article{strubell_energy_2019,
	title = {Energy and {Policy} {Considerations} for {Deep} {Learning} in {NLP}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1906.02243},
	doi = {10.48550/ARXIV.1906.02243},
	urldate = {2023-02-09},
	author = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
	year = {2019},
	note = {Publisher: arXiv Version Number: 1},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
}

% 98
@misc{https://doi.org/10.48550/arxiv.2008.00177,
  doi = {10.48550/ARXIV.2008.00177},
  url = {https://arxiv.org/abs/2008.00177},
  author = {Lin, Jiahuang and Li, Xin and Pekhimenko, Gennady},
  title = {Multi-node Bert-pretraining: Cost-efficient Approach},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

%99
@article{10.5555/3455716.3455856,
author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pretraining objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new "Colossal Clean Crawled Corpus", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {140},
numpages = {67},
keywords = {transfer learning, multi-task learning, deep learning, natural language processing, attention based models}
}

%100
@misc{https://doi.org/10.48550/arxiv.1905.05583,
  doi = {10.48550/ARXIV.1905.05583},
  url = {https://arxiv.org/abs/1905.05583},
  author = {Sun, Chi and Qiu, Xipeng and Xu, Yige and Huang, Xuanjing},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {How to Fine-Tune BERT for Text Classification?},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% 101
@article{joshi2020spanbert,
  title={Spanbert: Improving pre-training by representing and predicting spans},
  author={Joshi, Mandar and Chen, Danqi and Liu, Yinhan and Weld, Daniel S and Zettlemoyer, Luke and Levy, Omer},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={64--77},
  year={2020},
  publisher={MIT Press}
}

%102
@article{lee2020biobert,
  title={BioBERT: a pre-trained biomedical language representation model for biomedical text mining},
  author={Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
  journal={Bioinformatics},
  volume={36},
  number={4},
  pages={1234--1240},
  year={2020},
  publisher={Oxford University Press}
}

%103
@article{poerner2020inexpensive,
  title={Inexpensive domain adaptation of pretrained language models: Case studies on biomedical NER and covid-19 QA},
  author={Poerner, Nina and Waltinger, Ulli and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:2004.03354},
  year={2020}
}

%104
@misc{lan2020albert,
      title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations}, 
      author={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
      year={2020},
      eprint={1909.11942},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

%105
@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

%106
@article{li2020efficient,
  title={Efficient transformer-based large scale language representations using hardware-friendly block structured pruning},
  author={Li, Bingbing and Kong, Zhenglun and Zhang, Tianyun and Li, Ji and Li, Zhengang and Liu, Hang and Ding, Caiwen},
  journal={arXiv preprint arXiv:2009.08065},
  year={2020}
}

%107
@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={OpenAI}
}

%108
@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

% 109
@article{brock2018large,
  title={Large scale GAN training for high fidelity natural image synthesis},
  author={Brock, Andrew and Donahue, Jeff and Simonyan, Karen},
  journal={arXiv preprint arXiv:1809.11096},
  year={2018}
}

% 110
@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

%111
@article{chahal2020hitchhiker,
  title={A hitchhiker’s guide on distributed training of deep neural networks},
  author={Chahal, Karanbir Singh and Grover, Manraj Singh and Dey, Kuntal and Shah, Rajiv Ratn},
  journal={Journal of Parallel and Distributed Computing},
  volume={137},
  pages={65--76},
  year={2020},
  publisher={Elsevier}
}

%112
@article{iman2023review,
  title={A review of deep transfer learning and recent advancements},
  author={Iman, Mohammadreza and Arabnia, Hamid Reza and Rasheed, Khaled},
  journal={Technologies},
  volume={11},
  number={2},
  pages={40},
  year={2023},
  publisher={MDPI}
}

%113
@article{chatterjee2017progressive,
  title={Progressive learning for systematic design of large neural networks},
  author={Chatterjee, Saikat and Javid, Alireza M and Sadeghi, Mostafa and Mitra, Partha P and Skoglund, Mikael},
  journal={arXiv preprint arXiv:1710.08177},
  year={2017}
}


% 114
@article{rakka2022mixed,
  title={Mixed-precision neural networks: A survey},
  author={Rakka, Mariam and Fouda, Mohammed E and Khargonekar, Pramod and Kurdahi, Fadi},
  journal={arXiv preprint arXiv:2208.06064},
  year={2022}
}

%115
@article{hoefler2021sparsity,
  title={Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks},
  author={Hoefler, Torsten and Alistarh, Dan and Ben-Nun, Tal and Dryden, Nikoli and Peste, Alexandra},
  journal={The Journal of Machine Learning Research},
  volume={22},
  number={1},
  pages={10882--11005},
  year={2021},
  publisher={JMLRORG}
}



% 116
@article{deng2020model,
  title={Model compression and hardware acceleration for neural networks: A comprehensive survey},
  author={Deng, Lei and Li, Guoqi and Han, Song and Shi, Luping and Xie, Yuan},
  journal={Proceedings of the IEEE},
  volume={108},
  number={4},
  pages={485--532},
  year={2020},
  publisher={IEEE}
}

% 117
@article{gou_knowledge_2021,
	title = {Knowledge {Distillation}: {A} {Survey}},
	volume = {129},
	issn = {0920-5691, 1573-1405},
	shorttitle = {Knowledge {Distillation}},
	url = {https://link.springer.com/10.1007/s11263-021-01453-z},
	doi = {10.1007/s11263-021-01453-z},
	language = {en},
	number = {6},
	urldate = {2023-03-16},
	journal = {International Journal of Computer Vision},
	author = {Gou, Jianping and Yu, Baosheng and Maybank, Stephen J. and Tao, Dacheng},
	month = {jun},
	year = {2021},
	pages = {1789--1819},
	file = {Versione accettata:C\:\\Users\\Enric\\Zotero\\storage\\BFZPDPMN\\Gou et al. - 2021 - Knowledge Distillation A Survey.pdf:application/pdf},
}

% 118
@article{zhou2023comprehensive,
  title={A comprehensive survey on pretrained foundation models: A history from bert to chatgpt},
  author={Zhou, Ce and Li, Qian and Li, Chen and Yu, Jun and Liu, Yixin and Wang, Guangjing and Zhang, Kai and Ji, Cheng and Yan, Qiben and He, Lifang and others},
  journal={arXiv preprint arXiv:2302.09419},
  year={2023}
}


% 119
@article{you2019fast,
  title={Fast deep neural network training on distributed systems and cloud TPUs},
  author={You, Yang and Zhang, Zhao and Hsieh, Cho-Jui and Demmel, James and Keutzer, Kurt},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  volume={30},
  number={11},
  pages={2449--2462},
  year={2019},
  publisher={IEEE}
}

%120
@article{ma2021comparison,
  title={A comparison of approaches to document-level machine translation},
  author={Ma, Zhiyi and Edunov, Sergey and Auli, Michael},
  journal={arXiv preprint arXiv:2101.11040},
  year={2021}
}

&121
@article{dodge2020fine,
  title={Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping},
  author={Dodge, Jesse and Ilharco, Gabriel and Schwartz, Roy and Farhadi, Ali and Hajishirzi, Hannaneh and Smith, Noah},
  journal={arXiv preprint arXiv:2002.06305},
  year={2020}
}

% 122
@article{li2020pytorch,
  title={Pytorch distributed: Experiences on accelerating data parallel training},
  author={Li, Shen and Zhao, Yanli and Varma, Rohan and Salpekar, Omkar and Noordhuis, Pieter and Li, Teng and Paszke, Adam and Smith, Jeff and Vaughan, Brian and Damania, Pritam and others},
  journal={arXiv preprint arXiv:2006.15704},
  year={2020}
}





