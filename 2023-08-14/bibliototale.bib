% 1
@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	language = {en},
	number = {7553},
	urldate = {2023-02-28},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = {may},
	year = {2015},
	pages = {436--444},
}



% 2
@article{10.1145/3459991,
author = {Padakandla, Sindhu},
title = {A Survey of Reinforcement Learning Algorithms for Dynamically Varying Environments},
year = {2021},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3459991},
doi = {10.1145/3459991},
month = {jul},
articleno = {127},
numpages = {25},
keywords = {Reinforcement learning, sequential decision-making, context detection, meta-learning, non-stationary environments, regret computation, Markov decision processes}
}

% 3
@article{wang_generative_2022,
	title = {Generative {Adversarial} {Networks} in {Computer} {Vision}: {A} {Survey} and {Taxonomy}},
	volume = {54},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Generative {Adversarial} {Networks} in {Computer} {Vision}},
	url = {https://dl.acm.org/doi/10.1145/3439723},
	doi = {10.1145/3439723},
	abstract = {Generative adversarial networks (GANs) have been extensively studied in the past few years. Arguably their most significant impact has been in the area of computer vision where great advances have been made in challenges such as plausible image generation, image-to-image translation, facial attribute manipulation, and similar domains. Despite the significant successes achieved to date, applying GANs to real-world problems still poses significant challenges, three of which we focus on here. These are as follows: (1) the generation of high quality images, (2) diversity of image generation, and (3) stabilizing training. Focusing on the degree to which popular GAN technologies have made progress against these challenges, we provide a detailed review of the state-of-the-art in GAN-related research in the published scientific literature. We further structure this review through a convenient taxonomy we have adopted based on variations in GAN architectures and loss functions. While several reviews for GANs have been presented to date, none have considered the status of this field based on their progress toward addressing practical challenges relevant to computer vision. Accordingly, we review and critically discuss the most popular architecture-variant, and loss-variant GANs, for tackling these challenges. Our objective is to provide an overview as well as a critical analysis of the status of GAN research in terms of relevant progress toward critical computer vision application requirements. As we do this we also discuss the most compelling applications in computer vision in which GANs have demonstrated considerable success along with some suggestions for future research directions. Codes related to the GAN-variants studied in this work is summarized on https://github.com/sheqi/GAN\_Review.},
	language = {en},
	number = {2},
	urldate = {2023-02-28},
	journal = {ACM Computing Surveys},
	author = {Wang, Zhengwei and She, Qi and Ward, Tomás E.},
	month = {mar},
	year = {2022},
	pages = {1--38},
	file = {Full text:C\:\\Users\\Enric\\Zotero\\storage\\E25BXL2D\\Wang et al. - 2022 - Generative Adversarial Networks in Computer Vision.pdf:application/pdf},
}

%4
@Article{math10193619,
AUTHOR = {Yu, Fuchao and Xiu, Xianchao and Li, Yunhui},
TITLE = {A Survey on Deep Transfer Learning and Beyond},
JOURNAL = {Mathematics},
VOLUME = {10},
YEAR = {2022},
NUMBER = {19},
ARTICLE-NUMBER = {3619},
URL = {https://www.mdpi.com/2227-7390/10/19/3619},
ISSN = {2227-7390},
ABSTRACT = {Deep transfer learning (DTL), which incorporates new ideas from deep neural networks into transfer learning (TL), has achieved excellent success in computer vision, text classification, behavior recognition, and natural language processing. As a branch of machine learning, DTL applies end-to-end learning to overcome the drawback of traditional machine learning that regards each dataset individually. Although some valuable and impressive general surveys exist on TL, special attention and recent advances in DTL are lacking. In this survey, we first review more than 50 representative approaches of DTL in the last decade and systematically summarize them into four categories. In particular, we further divide each category into subcategories according to models, functions, and operation objects. In addition, we discuss recent advances in TL in other fields and unsupervised TL. Finally, we provide some possible and exciting future research directions.},
DOI = {10.3390/math10193619}
}

% 5
@article{automl,
author = {He, Xin and Zhao, Kaiyong and Chu, Xiaowen},
year = {2021},
month = {01},
pages = {106622},
title = {AutoML: A survey of the state-of-the-art},
volume = {212},
journal = {Knowledge-Based Systems},
doi = {10.1016/j.knosys.2020.106622}
}

% 6
@article{schwartz_green_2020,
	title = {Green {AI}},
	volume = {63},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3381831},
	doi = {10.1145/3381831},
	abstract = {Creating efficiency in AI research will decrease its carbon footprint and increase its inclusivity as deep learning study should not require the deepest pockets.},
	language = {en},
	number = {12},
	urldate = {2023-02-08},
	journal = {Communications of the ACM},
	author = {Schwartz, Roy and Dodge, Jesse and Smith, Noah A. and Etzioni, Oren},
	month = {nov},
	year = {2020},
	pages = {54--63},
	file = {Testo integrale:C\:\\Users\\alice\\Zotero\\storage\\N4U29MDB\\Schwartz et al. - 2020 - Green AI.pdf:application/pdf},
}

%7 e anche 19 ...
@article{xu2021survey,
  title={A survey on green deep learning},
  author={Xu, Jingjing and Zhou, Wangchunshu and Fu, Zhiyi and Zhou, Hao and Li, Lei},
  journal={arXiv preprint arXiv:2111.05193},
  year={2021}
}

%8
@article{patterson_carbon_2022-1,
	title = {The {Carbon} {Footprint} of {Machine} {Learning} {Training} {Will} {Plateau}, {Then} {Shrink}},
	volume = {55},
	issn = {0018-9162, 1558-0814},
	url = {https://ieeexplore.ieee.org/document/9810097/},
	doi = {10.1109/MC.2022.3148714},
	number = {7},
	urldate = {2023-06-20},
	journal = {Computer},
	author = {Patterson, David and Gonzalez, Joseph and Holzle, Urs and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David R. and Texier, Maud and Dean, Jeff},
	month = {jul},
	year = {2022},
	pages = {18--28},
	file = {Versione inviata:C\:\\Users\\alice\\Zotero\\storage\\YDTBM5BT\\Patterson et al. - 2022 - The Carbon Footprint of Machine Learning Training .pdf:application/pdf},
}

%9 
@article{von_neumann_first_1993,
	title = {First draft of a report on the {EDVAC}},
	volume = {15},
	issn = {1058-6180},
	url = {http://ieeexplore.ieee.org/document/238389/},
	doi = {10.1109/85.238389},
	number = {4},
	urldate = {2023-02-16},
	journal = {IEEE Annals of the History of Computing},
	author = {von Neumann, J.},
	year = {1993},
	pages = {27--75},
}

% 10
@article{
backus_can_1978,
title = {Can programming be liberated from the von {Neumann} style?: a functional style and its algebra of programs},
volume = {21},
issn = {0001-0782, 1557-7317},
shorttitle = {Can programming be liberated from the von {Neumann} style?},
url = {https://dl.acm.org/doi/10.1145/359576.359579},
doi = {10.1145/359576.359579},
language = {en},
number = {8},
urldate = {2023-02-28},
journal = {Communications of the ACM},
author = {Backus, John},
month = {aug},
year = {1978},
pages = {613--641}
}

% 11
@article{kahle1989connection,
  title={The connection machine model cm-1 architecture},
  author={Kahle, Brewster A and Hillis, W Daniel},
  journal={IEEE transactions on systems, man, and cybernetics},
  volume={19},
  number={4},
  pages={707--713},
  year={1989},
  publisher={IEEE}
}

%12
@article{pearson_liii_1901,
	title = {{LIII}. \textit{{On} lines and planes of closest fit to systems of points in space}},
	volume = {2},
	issn = {1941-5982, 1941-5990},
	url = {https://www.tandfonline.com/doi/full/10.1080/14786440109462720},
	doi = {10.1080/14786440109462720},
	language = {en},
	number = {11},
	urldate = {2023-02-28},
	journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
	author = {Pearson, Karl},
	month = {nov},
	year = {1901},
	pages = {559--572},
	file = {Versione inviata:C\:\\Users\\Enric\\Zotero\\storage\\RM7USJ9H\\Pearson - 1901 - LIII. On lines and planes of closest fit to sys.pdf:application/pdf},
}

%13
@article{kramer_nonlinear_1991,
	title = {Nonlinear principal component analysis using autoassociative neural networks},
	volume = {37},
	issn = {0001-1541, 1547-5905},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/aic.690370209},
	doi = {10.1002/aic.690370209},
	language = {en},
	number = {2},
	urldate = {2023-02-28},
	journal = {AIChE Journal},
	author = {Kramer, Mark A.},
	month = {feb},
	year = {1991},
	pages = {233--243},
}

%14
@article{CNN,
author = {Hasegawa, Akira and Lo, Shih-Chung and Lin, Jyh-Shyan and Freedman, Matthew and Mun, Seong},
year = {1998},
month = {04},
pages = {241-250},
title = {A Shift-Invariant Neural Network for the Lung Field Segmentation in Chest Radiography},
volume = {18},
journal = {VLSI Signal Processing},
doi = {10.1023/A:1007937214367}
}

%15
@article{RNN,
	title = {A {Novel} {Connectionist} {System} for {Unconstrained} {Handwriting} {Recognition}},
	volume = {31},
	issn = {0162-8828},
	url = {http://ieeexplore.ieee.org/document/4531750/},
	doi = {10.1109/TPAMI.2008.137},
	number = {5},
	urldate = {2023-02-28},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Graves, A. and Liwicki, M. and Fernandez, S. and Bertolami, R. and Bunke, H. and Schmidhuber, J.},
	month = {may},
	year = {2009},
	pages = {855--868},
	file = {Versione inviata:C\:\\Users\\Enric\\Zotero\\storage\\FDNR7EB8\\Graves et al. - 2009 - A Novel Connectionist System for Unconstrained Han.pdf:application/pdf},
}

%16
@article{LSTM,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/9/8/1735-1780/6109},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	language = {en},
	number = {8},
	urldate = {2023-02-28},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = {nov},
	year = {1997},
	pages = {1735--1780},
}

%17 
@book{domingos2015master,
  title={The master algorithm: How the quest for the ultimate learning machine will remake our world},
  author={Domingos, Pedro},
  year={2015},
  publisher={Basic Books}
}

%18
@book{minsky2017perceptrons,
  title={Perceptrons, Reissue of the 1988 Expanded Edition with a new foreword by L{\'e}on Bottou: An Introduction to Computational Geometry},
  author={Minsky, Marvin and Papert, Seymour A},
  year={2017},
  publisher={MIT press}
}

%19
Jingjing Xu, Wangchunshu Zhou, Zhiyi Fu, Hao Zhou, and Lei Li. A
Survey on Green Deep Learning. 2021. Publisher: arXivVersion Number:
2.
Jingjing Xu, Wangchunshu Zhou, Zhiyi Fu, Hao Zhou, and Lei Li. A
survey on green deep learning. arXiv preprint arXiv:2111.05193, 2021.

%20
@article{verdecchia_systematic_2023,
	title = {A {Systematic} {Review} of {Green} {AI}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2301.11047},
	doi = {10.48550/ARXIV.2301.11047},
	abstract = {With the ever-growing adoption of AI-based systems, the carbon footprint of AI is no longer negligible. AI researchers and practitioners are therefore urged to hold themselves accountable for the carbon emissions of the AI models they design and use. This led in recent years to the appearance of researches tackling AI environmental sustainability, a field referred to as Green AI. Despite the rapid growth of interest in the topic, a comprehensive overview of Green AI research is to date still missing. To address this gap, in this paper, we present a systematic review of the Green AI literature. From the analysis of 98 primary studies, different patterns emerge. The topic experienced a considerable growth from 2020 onward. Most studies consider monitoring AI model footprint, tuning hyperparameters to improve model sustainability, or benchmarking models. A mix of position papers, observational studies, and solution papers are present. Most papers focus on the training phase, are algorithm-agnostic or study neural networks, and use image data. Laboratory experiments are the most common research strategy. Reported Green AI energy savings go up to 115\%, with savings over 50\% being rather common. Industrial parties are involved in Green AI studies, albeit most target academic readers. Green AI tool provisioning is scarce. As a conclusion, the Green AI research field results to have reached a considerable level of maturity. Therefore, from this review emerges that the time is suitable to adopt other Green AI research strategies, and port the numerous promising academic results to industrial practice.},
	urldate = {2023-06-28},
	author = {Verdecchia, Roberto and Sallou, June and Cruz, Luís},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences},
}

%21
@article{talati2020mmpu,
  title={mmpu—a real processing-in-memory architecture to combat the von neumann bottleneck},
  author={Talati, Nishil and Ben-Hur, Rotem and Wald, Nimrod and Haj-Ali, Ameer and Reuben, John and Kvatinsky, Shahar},
  journal={Applications of Emerging Memory Technology: Beyond Storage},
  pages={191--213},
  year={2020},
  publisher={Springer}
}

%22
@book{vetter2013contemporary,
  title={Contemporary high performance computing: from Petascale toward exascale},
  author={Vetter, Jeffrey S},
  year={2013},
  publisher={CRC Press}
}

%23
@article{wang2013review,
  title={Review of performance metrics for green data centers: a taxonomy study},
  author={Wang, Lizhe and Khan, Samee U},
  journal={The journal of supercomputing},
  volume={63},
  pages={639--656},
  year={2013},
  publisher={Springer}
}

%24
@article{pawson2022myth,
  title={The myth of the Harvard architecture},
  author={Pawson, Richard},
  journal={IEEE Annals of the History of Computing},
  volume={44},
  number={3},
  pages={59--69},
  year={2022},
  publisher={IEEE}
}

%25
@article{harvardarchitecture,
  title={A Logical Calculus of Ideas Immanent in Nervous Activity},
  author={McCulloch, Warren S. and Pitts, Walter},
  journal={The Bulletin of Mathematical Biophysics},
  volume={5},
  number={4},
  pages={115-133},
  year={1943},
  publisher={Springer}
}

%26
@article{bravyi2022future,
  title={The future of quantum computing with superconducting qubits},
  author={Bravyi, Sergey and Dial, Oliver and Gambetta, Jay M and Gil, Dario and Nazario, Zaira},
  journal={Journal of Applied Physics},
  volume={132},
  number={16},
  pages={160902},
  year={2022},
  publisher={AIP Publishing LLC}
}

% 27
@inproceedings{elsayed2019review,
  title={A review of quantum computer energy efficiency},
  author={Elsayed, Nelly and Maida, Anthony S and Bayoumi, Magdy},
  booktitle={2019 IEEE Green Technologies Conference (GreenTech)},
  pages={1--3},
  year={2019},
  organization={IEEE}
}

% 28
@article{akopyan2015truenorth,
  title={Truenorth: Design and tool flow of a 65 mw 1 million neuron programmable neurosynaptic chip},
  author={Akopyan, Filipp and Sawada, Jun and Cassidy, Andrew and Alvarez-Icaza, Rodrigo and Arthur, John and Merolla, Paul and Imam, Nabil and Nakamura, Yutaka and Datta, Pallab and Nam, Gi-Joon and others},
  journal={IEEE transactions on computer-aided design of integrated circuits and systems},
  volume={34},
  number={10},
  pages={1537--1557},
  year={2015},
  publisher={IEEE}
}

% 29
@article{markram2006blue,
  title={The blue brain project},
  author={Markram, Henry},
  journal={Nature Reviews Neuroscience},
  volume={7},
  number={2},
  pages={153--160},
  year={2006},
  publisher={Nature Publishing Group UK London}
}

%30
@article{maass1997networks,
  title={Networks of spiking neurons: the third generation of neural network models},
  author={Maass, Wolfgang},
  journal={Neural networks},
  volume={10},
  number={9},
  pages={1659--1671},
  year={1997},
  publisher={Elsevier}
}

%31
@article{maass2002real,
  title={Real-time computing without stable states: A new framework for neural computation based on perturbations},
  author={Maass, Wolfgang and Natschl{\"a}ger, Thomas and Markram, Henry},
  journal={Neural computation},
  volume={14},
  number={11},
  pages={2531--2560},
  year={2002},
  publisher={MIT Press}
}

%32
@inproceedings{chua1988cellular,
  title={Cellular neural networks: theory},
  author={Chua, Leon O and Yang, Lin},
  booktitle={IEEE Transactions on Circuits and Systems},
  volume={35},
  number={10},
  pages={1257--1272},
  year={1988},
  organization={IEEE}
}

% 33
@inproceedings{kohonen1982self,
  title={Self-organized formation of topologically correct feature maps},
  author={Kohonen, Teuvo},
  booktitle={Biological cybernetics},
  volume={43},
  number={1},
  pages={59--69},
  year={1982},
  organization={Springer}
}

%34
@article{rosenblatt1960perceptron,
  title={Perceptron simulation experiments},
  author={Rosenblatt, Frank},
  journal={Proceedings of the IRE},
  volume={48},
  number={3},
  pages={301--309},
  year={1960},
  publisher={IEEE}
}

%35
@article{hopfield1982neural,
  title={Neural networks and physical systems with emergent collective computational abilities},
  author={Hopfield, John J},
  journal={Proceedings of the national academy of sciences},
  volume={79},
  number={8},
  pages={2554--2558},
  year={1982},
  publisher={National Acad Sciences}
}

%36
@article{park1991universal,
  title={Universal approximation using radial-basis-function networks},
  author={Park, Jong-Soon and Sandberg, Irwin W.},
  journal={Neural computation},
  volume={3},
  number={2},
  pages={246--257},
  year={1991},
  publisher={MIT Press}
}

%37
@article{graves2014neural,
  title={Neural turing machines},
  author={Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  journal={arXiv preprint arXiv:1410.5401},
  year={2014}
}

%38
@misc{zaremba_reinforcement_2016,
	title = {Reinforcement {Learning} {Neural} {Turing} {Machines} - {Revised}},
	url = {http://arxiv.org/abs/1505.00521},
	abstract = {The Neural Turing Machine (NTM) is more expressive than all previously considered models because of its external memory. It can be viewed as a broader effort to use abstract external Interfaces and to learn a parametric model that interacts with them. The capabilities of a model can be extended by providing it with proper Interfaces that interact with the world. These external Interfaces include memory, a database, a search engine, or a piece of software such as a theorem verifier. Some of these Interfaces are provided by the developers of the model. However, many important existing Interfaces, such as databases and search engines, are discrete. We examine feasibility of learning models to interact with discrete Interfaces. We investigate the following discrete Interfaces: a memory Tape, an input Tape, and an output Tape. We use a Reinforcement Learning algorithm to train a neural network that interacts with such Interfaces to solve simple algorithmic tasks. Our Interfaces are expressive enough to make our model Turing complete.},
	urldate = {2023-03-27},
	publisher = {arXiv},
	author = {Zaremba, Wojciech and Sutskever, Ilya},
	month = {jan},
	year = {2016},
	note = {arXiv:1505.00521 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\alice\\Zotero\\storage\\YLZB4AGR\\Zaremba e Sutskever - 2016 - Reinforcement Learning Neural Turing Machines - Re.pdf:application/pdf},
}

%39
@inproceedings{greve_evolving_2016,
	address = {Denver Colorado USA},
	title = {Evolving {Neural} {Turing} {Machines} for {Reward}-based {Learning}},
	isbn = {978-1-4503-4206-3},
	url = {https://dl.acm.org/doi/10.1145/2908812.2908930},
	doi = {10.1145/2908812.2908930},
	language = {en},
	urldate = {2023-03-27},
	booktitle = {Proceedings of the {Genetic} and {Evolutionary} {Computation} {Conference} 2016},
	publisher = {ACM},
	author = {Greve, Rasmus Boll and Jacobsen, Emil Juul and Risi, Sebastian},
	month = {jul},
	year = {2016},
	pages = {117--124},
}

%40
@misc{yang_lie_2016,
	title = {Lie {Access} {Neural} {Turing} {Machine}},
	url = {http://arxiv.org/abs/1602.08671},
	abstract = {Following the recent trend in explicit neural memory structures, we present a new design of an external memory, wherein memories are stored in an Euclidean key space \${\textbackslash}mathbb R{\textasciicircum}n\$. An LSTM controller performs read and write via specialized read and write heads. It can move a head by either providing a new address in the key space (aka random access) or moving from its previous position via a Lie group action (aka Lie access). In this way, the "L" and "R" instructions of a traditional Turing Machine are generalized to arbitrary elements of a fixed Lie group action. For this reason, we name this new model the Lie Access Neural Turing Machine, or LANTM. We tested two different configurations of LANTM against an LSTM baseline in several basic experiments. We found the right configuration of LANTM to outperform the baseline in all of our experiments. In particular, we trained LANTM on addition of \$k\$-digit numbers for \$2 {\textbackslash}le k {\textbackslash}le 16\$, but it was able to generalize almost perfectly to \$17 {\textbackslash}le k {\textbackslash}le 32\$, all with the number of parameters 2 orders of magnitude below the LSTM baseline.},
	urldate = {2023-03-27},
	publisher = {arXiv},
	author = {Yang, Greg},
	month = {sep},
	year = {2016},
	note = {arXiv:1602.08671 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:C\:\\Users\\alice\\Zotero\\storage\\3F3C3R78\\Yang - 2016 - Lie Access Neural Turing Machine.pdf:application/pdf},
}

%41
@misc{gulcehre_dynamic_2017,
	title = {Dynamic {Neural} {Turing} {Machine} with {Soft} and {Hard} {Addressing} {Schemes}},
	url = {http://arxiv.org/abs/1607.00036},
	abstract = {We extend neural Turing machine (NTM) model into a dynamic neural Turing machine (D-NTM) by introducing a trainable memory addressing scheme. This addressing scheme maintains for each memory cell two separate vectors, content and address vectors. This allows the D-NTM to learn a wide variety of location-based addressing strategies including both linear and nonlinear ones. We implement the D-NTM with both continuous, differentiable and discrete, non-differentiable read/write mechanisms. We investigate the mechanisms and effects of learning to read and write into a memory through experiments on Facebook bAbI tasks using both a feedforward and GRUcontroller. The D-NTM is evaluated on a set of Facebook bAbI tasks and shown to outperform NTM and LSTM baselines. We have done extensive analysis of our model and different variations of NTM on bAbI task. We also provide further experimental results on sequential pMNIST, Stanford Natural Language Inference, associative recall and copy tasks.},
	urldate = {2023-03-27},
	publisher = {arXiv},
	author = {Gulcehre, Caglar and Chandar, Sarath and Cho, Kyunghyun and Bengio, Yoshua},
	month = {mar},
	year = {2017},
	note = {arXiv:1607.00036 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:C\:\\Users\\alice\\Zotero\\storage\\SL9U3D49\\Gulcehre et al. - 2017 - Dynamic Neural Turing Machine with Soft and Hard A.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\alice\\Zotero\\storage\\ZY7JUZTU\\1607.html:text/html},
}

%42
@misc{kurach_neural_2016,
	title = {Neural {Random}-{Access} {Machines}},
	url = {http://arxiv.org/abs/1511.06392},
	urldate = {2023-03-27},
	publisher = {arXiv},
	author = {Kurach, Karol and Andrychowicz, Marcin and Sutskever, Ilya},
	month = {feb},
	year = {2016},
	note = {arXiv:1511.06392 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:C\:\\Users\\alice\\Zotero\\storage\\XPLSQT9K\\Kurach et al. - 2016 - Neural Random-Access Machines.pdf:application/pdf},
}

%43
@article{tyagi2022recurrent,
  title={Recurrent Neural Networks: Concepts and Applications},
  author={Tyagi, Amit Kumar and Abraham, Ajith},
  year={2022},
  publisher={CRC Press}
}

%44
@article{suresh_memory_2022,
	title = {Memory augmented recurrent neural networks for de-novo drug design},
	volume = {17},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0269461},
	doi = {10.1371/journal.pone.0269461},
	abstract = {A recurrent neural network (RNN) is a machine learning model that learns the relationship between elements of an input series, in addition to inferring a relationship between the data input to the model and target output. Memory augmentation allows the RNN to learn the interrelationships between elements of the input over a protracted length of the input series. Inspired by the success of stack augmented RNN (StackRNN) to generate strings for various applications, we present two memory augmented RNN-based architectures: the Neural Turing Machine (NTM) and the Differentiable Neural Computer (DNC) for the
              de-novo
              generation of small molecules. We trained a character-level convolutional neural network (CNN) to predict the properties of a generated string and compute a reward or loss in a deep reinforcement learning setup to bias the Generator to produce molecules with the desired property. Further, we compare the performance of these architectures to gain insight to their relative merits in terms of the validity and novelty of the generated molecules and the degree of property bias towards the computational generation of
              de-novo
              drugs. We also compare the performance of these architectures with simpler recurrent neural networks (Vanilla RNN, LSTM, and GRU) without an external memory component to explore the impact of augmented memory in the task of
              de-novo
              generation of small molecules.},
	language = {en},
	number = {6},
	urldate = {2023-03-27},
	journal = {PLOS ONE},
	author = {Suresh, Naveen and Chinnakonda Ashok Kumar, Neelesh and Subramanian, Srikumar and Srinivasa, Gowri},
	editor = {Nguyen, Binh P.},
	month = {jun},
	year = {2022},
	pages = {e0269461},
	file = {Full text:C\:\\Users\\alice\\Zotero\\storage\\C7ULJRHU\\Suresh et al. - 2022 - Memory augmented recurrent neural networks for de-.pdf:application/pdf},
}

% 45
@article{hammer_adaptive_1962,
	title = {Adaptive {Control} {Processes}: {A} {Guided} {Tour} ({R}. {Bellman})},
	volume = {4},
	issn = {0036-1445, 1095-7200},
	shorttitle = {Adaptive {Control} {Processes}},
	url = {http://epubs.siam.org/doi/10.1137/1004050},
	doi = {10.1137/1004050},
	language = {en},
	number = {2},
	urldate = {2023-03-20},
	journal = {SIAM Review},
	author = {Hammer, P. C.},
	month = {apr},
	year = {1962},
	pages = {163--163},
}

%46
@article{kolda_tensor_2009,
	title = {Tensor {Decompositions} and {Applications}},
	volume = {51},
	issn = {0036-1445, 1095-7200},
	url = {http://epubs.siam.org/doi/10.1137/07070111X},
	doi = {10.1137/07070111X},
	language = {en},
	number = {3},
	urldate = {2023-02-26},
	journal = {SIAM Review},
	author = {Kolda, Tamara G. and Bader, Brett W.},
	month = {aug},
	year = {2009},
	pages = {455--500},
}

%47
@article{rabanser_introduction_2017,
	title = {Introduction to {Tensor} {Decompositions} and their {Applications} in {Machine} {Learning}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1711.10781},
	doi = {10.48550/ARXIV.1711.10781},
	abstract = {Tensors are multidimensional arrays of numerical values and therefore generalize matrices to multiple dimensions. While tensors first emerged in the psychometrics community in the \$20{\textasciicircum}\{{\textbackslash}text\{th\}\}\$ century, they have since then spread to numerous other disciplines, including machine learning. Tensors and their decompositions are especially beneficial in unsupervised learning settings, but are gaining popularity in other sub-disciplines like temporal and multi-relational data analysis, too. The scope of this paper is to give a broad overview of tensors, their decompositions, and how they are used in machine learning. As part of this, we are going to introduce basic tensor concepts, discuss why tensors can be considered more rigid than matrices with respect to the uniqueness of their decomposition, explain the most important factorization algorithms and their properties, provide concrete examples of tensor decomposition applications in machine learning, conduct a case study on tensor-based estimation of mixture models, talk about the current state of research, and provide references to available software libraries.},
	urldate = {2023-02-26},
	author = {Rabanser, Stephan and Shchur, Oleksandr and Günnemann, Stephan},
	year = {2017},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML)},
}

%48
@article{ji_survey_2019,
	title = {A {Survey} on {Tensor} {Techniques} and {Applications} in {Machine} {Learning}},
	volume = {7},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8884203/},
	doi = {10.1109/ACCESS.2019.2949814},
	urldate = {2023-02-26},
	journal = {IEEE Access},
	author = {Ji, Yuwang and Wang, Qiang and Li, Xuan and Liu, Jie},
	year = {2019},
	pages = {162950--162990},
	file = {Full text:C\:\\Users\\alice\\Zotero\\storage\\JPVL3QXL\\Ji et al. - 2019 - A Survey on Tensor Techniques and Applications in .pdf:application/pdf},
}

%49
@article{bernardi_general_2013,
	title = {General tensor decomposition, moment matrices and applications},
	volume = {52},
	issn = {0747-7171},
	url = {https://www.sciencedirect.com/science/article/pii/S0747717112001290},
	doi = {https://doi.org/10.1016/j.jsc.2012.05.012},
	abstract = {The tensor decomposition addressed in this paper may be seen as a generalization of Singular Value Decomposition of matrices. We consider general multilinear and multihomogeneous tensors. We show how to reduce the problem to a truncated moment matrix problem and give a new criterion for flat extension of Quasi-Hankel matrices. We connect this criterion to the commutation characterization of border bases. A new algorithm is described. It applies for general multihomogeneous tensors, extending the approach on binary forms by J.J. Sylvester. An example illustrates the algebraic operations involved in this approach and how the decomposition can be recovered from eigenvector computation.},
	journal = {Journal of Symbolic Computation},
	author = {Bernardi, A. and Brachat, J. and Comon, P. and Mourrain, B.},
	year = {2013},
	keywords = {Decomposition, Flat extension, Hankel operator, Moment matrix, Multihomogeneous polynomial, Rank, Tensor},
	pages = {51--71},
}

%50
@article{lewis2021large,
  title={Large scale distributed linear algebra with tensor processing units},
  author={Lewis, Adam GM and Beall, Jackson and Ganahl, Martin and Hauru, Markus and Mallick, Shrestha Basu and Vidal, Guifre},
  journal={arXiv preprint arXiv:2112.09017},
  year={2021}
}

%51
@article{mayer2020scalable,
  title={Scalable deep learning on distributed infrastructures: Challenges, techniques, and tools},
  author={Mayer, Ruben and Jacobsen, Hans-Arno},
  journal={ACM Computing Surveys (CSUR)},
  volume={53},
  number={1},
  pages={1--37},
  year={2020},
  publisher={ACM New York, NY, USA}
}

%52
@article{gou2021knowledge,
  title={Knowledge distillation: A survey},
  author={Gou, Jianping and Yu, Baosheng and Maybank, Stephen J and Tao, Dacheng},
  journal={International Journal of Computer Vision},
  volume={129},
  pages={1789--1819},
  year={2021},
  publisher={Springer}
}

%53
@misc{he2015deep,
      title={Deep Residual Learning for Image Recognition}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1512.03385},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

%54
@article{howard2017mobilenets,
  title={MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications},
  author={Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  journal={arXiv preprint arXiv:1704.04861},
  year={2017}
}

%55
@article{lecun1989backpropagation,
title={Backpropagation applied to handwritten zip code recognition},
author={LeCun, Yann and Boser, Bernhard and Denker, John S and Henderson, Donnie and Howard, Richard E and Hubbard, Wayne and Jackel, Lawrence D},
journal={Neural computation},
volume={1},
number={4},
pages={541--551},
year={1989},
publisher={MIT Press}
}

%56
@article{wu2016quantized,
title={Quantized convolutional neural networks for mobile devices},
author={Wu, Sheng and Liang, Hong and Zhang, Yiran and Sun, Rui and Wang, Ting and He, Xuming},
journal={arXiv preprint arXiv:1603.05279},
year={2016}
}

%57
@inproceedings{courbariaux2015binaryconnect,
title={Binaryconnect: Training deep neural networks with binary weights during propagations},
author={Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
booktitle={Advances in neural information processing systems},
pages={3123--3131},
year={2015}
}

%58
@inproceedings{sindhwani2015structured,
title={Structured transforms for small-footprint deep learning},
author={Sindhwani, Vikas and Sainath, Tara N and Kumar, Sanjiv},
booktitle={Proceedings of the 32nd International Conference on Machine Learning},
pages={1664--1672},
year={2015},
organization={JMLR Workshop and Conference Proceedings}
}

%59
@article{DBLP:journals/corr/Krizhevsky14,
  author    = {Alex Krizhevsky},
  title     = {One weird trick for parallelizing convolutional neural networks},
  journal   = {CoRR},
  volume    = {abs/1404.5997},
  year      = {2014},
  url       = {http://arxiv.org/abs/1404.5997},
  eprinttype = {arXiv},
  eprint    = {1404.5997},
  timestamp = {Mon, 13 Aug 2018 16:48:41 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/Krizhevsky14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

%60
@article{wiedmann2008definition,
  title={A definition of ‘carbon footprint’},
  author={Wiedmann, Thomas and Minx, Jan},
  journal={Ecological economics research trends},
  volume={1},
  number={2008},
  pages={1--11},
  year={2008},
  publisher={Nova Science Publishers Hauppauge, NY}
}

%61
@Inbook{Jones1994,
author={Jones, Karen Sparck},
title={Natural Language Processing: A Historical Review},
bookTitle={Current Issues in Computational Linguistics: In Honour of Don Walker},
year={1994},
publisher={Springer Netherlands},
address={Dordrecht},
pages={3--16}
}

%62
@incollection{Schank75,	
  AUTHOR     = 	{R. C. Schank},	
  TITLE      =  {Conceptual Dependency Theory},	
  YEAR       =  {1975},	
  BOOKTITLE  = 	{Conceptual Information Processing},	
  EDITOR     = 	{R. C. Schank},	
  PUBLISHER  = 	{North-Holland and Elsevier},	
  ADDRESS    = 	{Amsterdam and New York},	
  PAGES      = 	{22-82},   	
  KEYWORDS   = 	{},
  ANNOTE     =	{ The book presents the Conceptual Dependency Theory which can be used by a machine for conceptual understanding of the given text.}
  }	

%63
@inproceedings{johri2021natural,
  title={Natural language processing: History, evolution, application, and future work},
  author={Johri, Prashant and Khatri, Sunil K and Al-Taani, Ahmad T and Sabharwal, Munish and Suvanov, Shakhzod and Kumar, Avneesh},
  booktitle={Proceedings of 3rd International Conference on Computing Informatics and Networks: ICCIN 2020},
  pages={365--375},
  year={2021},
  organization={Springer}
}

%64
@misc{https://doi.org/10.48550/arxiv.1906.08237,
  doi = {10.48550/ARXIV.1906.08237},
  url = {https://arxiv.org/abs/1906.08237},
  author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
  publisher = {arXiv},
  year = {2019},
  copyright = {Creative Commons Attribution 4.0 International}
}

%65
@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}

%66
@article{rogers2021primer,
  title={A primer in BERTology: What we know about how BERT works},
  author={Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={842--866},
  year={2021},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

%67
@article{lin2022survey,
  title={A survey of transformers},
  author={Lin, Tianyang and Wang, Yuxin and Liu, Xiangyang and Qiu, Xipeng},
  journal={AI Open},
  year={2022},
  publisher={Elsevier}
}

%68
@misc{https://doi.org/10.48550/arxiv.2101.00063,
  doi = {10.48550/ARXIV.2101.00063},
  url = {https://arxiv.org/abs/2101.00063},
  author = {Chen, Xiaohan and Cheng, Yu and Wang, Shuohang and Gan, Zhe and Wang, Zhangyang and Liu, Jingjing},
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {EarlyBERT: Efficient BERT Training via Early-bird Lottery Tickets},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

%69
@article{frankle2018lottery,
  title={The lottery ticket hypothesis: Finding sparse, trainable neural networks},
  author={Frankle, Jonathan and Carbin, Michael},
  journal={arXiv preprint arXiv:1803.03635},
  year={2018}
}

%70
@article{wu2023brief,
  title={A brief overview of ChatGPT: The history, status quo and potential future development},
  author={Wu, Tianyu and He, Shizhu and Liu, Jingping and Sun, Siqi and Liu, Kang and Han, Qing-Long and Tang, Yang},
  journal={IEEE/CAA Journal of Automatica Sinica},
  volume={10},
  number={5},
  pages={1122--1136},
  year={2023},
  publisher={IEEE}
}

%71
@article{kirmani2022artificial,
  title={Artificial Intelligence-Enabled Science Poetry},
  author={Kirmani, Ahmad R},
  journal={ACS Energy Letters},
  volume={8},
  pages={574--576},
  year={2022},
  publisher={ACS Publications}
}

%72
@article{surameery2023use,
  title={Use chat gpt to solve programming bugs},
  author={Surameery, Nigar M Shafiq and Shakor, Mohammed Y},
  journal={International Journal of Information Technology \& Computer Engineering (IJITC) ISSN: 2455-5290},
  volume={3},
  number={01},
  pages={17--22},
  year={2023}
}

%73
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

%74
@article{goodfellow2020generative,
  title={Generative adversarial networks},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={Communications of the ACM},
  volume={63},
  number={11},
  pages={139--144},
  year={2020},
  publisher={ACM New York, NY, USA}
}

% 75
@article{wu_sustainable_2021,
	title = {Sustainable {AI}: {Environmental} {Implications}, {Challenges} and {Opportunities}},
	copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
	shorttitle = {Sustainable {AI}},
	url = {https://arxiv.org/abs/2111.00364},
	doi = {10.48550/ARXIV.2111.00364},
	abstract = {This paper explores the environmental impact of the super-linear growth trends for AI from a holistic perspective, spanning Data, Algorithms, and System Hardware. We characterize the carbon footprint of AI computing by examining the model development cycle across industry-scale machine learning use cases and, at the same time, considering the life cycle of system hardware. Taking a step further, we capture the operational and manufacturing carbon footprint of AI computing and present an end-to-end analysis for what and how hardware-software design and at-scale optimization can help reduce the overall carbon footprint of AI. Based on the industry experience and lessons learned, we share the key challenges and chart out important development directions across the many dimensions of AI. We hope the key messages and insights presented in this paper can inspire the community to advance the field of AI in an environmentally-responsible manner.},
	urldate = {2023-06-20},
	author = {Wu, Carole-Jean and Raghavendra, Ramya and Gupta, Udit and Acun, Bilge and Ardalani, Newsha and Maeng, Kiwan and Chang, Gloria and Behram, Fiona Aga and Huang, James and Bai, Charles and Gschwind, Michael and Gupta, Anurag and Ott, Myle and Melnikov, Anastasia and Candido, Salvatore and Brooks, David and Chauhan, Geeta and Lee, Benjamin and Lee, Hsien-Hsin S. and Akyildiz, Bugra and Balandat, Maximilian and Spisak, Joe and Jain, Ravi and Rabbat, Mike and Hazelwood, Kim},
	year = {2021},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, Hardware Architecture (cs.AR), Machine Learning (cs.LG)},
}

% 76
@inproceedings{zhao_greener_2022,
	address = {Lyon, France},
	title = {A {Green}(er) {World} for {A}.{I}.},
	isbn = {978-1-66549-747-3},
	url = {https://ieeexplore.ieee.org/document/9835179/},
	doi = {10.1109/IPDPSW55747.2022.00126},
	urldate = {2023-06-20},
	booktitle = {2022 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} {Workshops} ({IPDPSW})},
	publisher = {IEEE},
	author = {Zhao, Dan and Frey, Nathan C. and McDonald, Joseph and Hubbell, Matthew and Bestor, David and Jones, Michael and Prout, Andrew and Gadepally, Vijay and Samsi, Siddharth},
	month = {may},
	year = {2022},
	pages = {742--750},
	file = {Versione inviata:C\:\\Users\\alice\\Zotero\\storage\\8346ZE8B\\Zhao et al. - 2022 - A Green(er) World for A.I..pdf:application/pdf},
}

% 77 
@article{anthony_carbontracker_2020,
	title = {Carbontracker: {Tracking} and {Predicting} the {Carbon} {Footprint} of {Training} {Deep} {Learning} {Models}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Carbontracker},
	url = {https://arxiv.org/abs/2007.03051},
	doi = {10.48550/ARXIV.2007.03051},
	abstract = {Deep learning (DL) can achieve impressive results across a wide variety of tasks, but this often comes at the cost of training models for extensive periods on specialized hardware accelerators. This energy-intensive workload has seen immense growth in recent years. Machine learning (ML) may become a significant contributor to climate change if this exponential trend continues. If practitioners are aware of their energy and carbon footprint, then they may actively take steps to reduce it whenever possible. In this work, we present Carbontracker, a tool for tracking and predicting the energy and carbon footprint of training DL models. We propose that energy and carbon footprint of model development and training is reported alongside performance metrics using tools like Carbontracker. We hope this will promote responsible computing in ML and encourage research into energy-efficient deep neural networks.},
	urldate = {2023-06-20},
	author = {Anthony, Lasse F. Wolff and Kanding, Benjamin and Selvan, Raghavendra},
	year = {2020},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Computers and Society (cs.CY), FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, Machine Learning (cs.LG), Machine Learning (stat.ML), Signal Processing (eess.SP)},
}


% 78
@inproceedings{gupta2021chasing,
  title={Chasing carbon: The elusive environmental footprint of computing},
  author={Gupta, Udit and Kim, Young Geun and Lee, Sylvia and Tse, Jordan and Lee, Hsien-Hsin S and Wei, Gu-Yeon and Brooks, David and Wu, Carole-Jean},
  booktitle={2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  pages={854--867},
  year={2021},
  organization={IEEE}
}

% 79
@article{henderson2020towards,
  title={Towards the systematic reporting of the energy and carbon footprints of machine learning},
  author={Henderson, Peter and Hu, Jieru and Romoff, Joshua and Brunskill, Emma and Jurafsky, Dan and Pineau, Joelle},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={10039--10081},
  year={2020},
  publisher={JMLRORG}
}

% 80 
@ARTICLE{9810097,
  author={Patterson, David and Gonzalez, Joseph and Hölzle, Urs and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David R. and Texier, Maud and Dean, Jeff},
  journal={Computer}, 
  title={The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink}, 
  year={2022},
  volume={55},
  number={7},
  pages={18-28},
  doi={10.1109/MC.2022.3148714}}

% 81
@article{van_wynsberghe_sustainable_2021,
	title = {Sustainable {AI}: {AI} for sustainability and the sustainability of {AI}},
	volume = {1},
	issn = {2730-5953, 2730-5961},
	shorttitle = {Sustainable {AI}},
	url = {https://link.springer.com/10.1007/s43681-021-00043-6},
	doi = {10.1007/s43681-021-00043-6},
	abstract = {Abstract
            
              While there is a growing effort towards AI
              for
              Sustainability (e.g. towards the sustainable development goals) it is time to move beyond that and to address the sustainability
              of
              developing and using AI systems. In this paper I propose a definition of Sustainable AI; Sustainable AI is a movement to foster change in the entire lifecycle of AI products (i.e. idea generation, training, re-tuning, implementation, governance) towards greater ecological integrity and social justice. As such, Sustainable AI is focused on more than AI applications; rather, it addresses the whole sociotechnical system of AI. I have suggested here that Sustainable AI is not about how to sustain the development of AI per say but it is about how to develop AI that is compatible with sustaining environmental resources for current and future generations; economic models for societies; and societal values that are fundamental to a given society. I have articulated that the phrase Sustainable AI be understood as having two branches; AI
              for
              sustainability and sustainability
              of
              AI (e.g. reduction of carbon emissions and computing power). I propose that Sustainable AI take sustainable development at the core of its definition with three accompanying tensions between AI innovation and equitable resource distribution; inter and intra-generational justice; and, between environment, society, and economy. This paper is not meant to engage with each of the three pillars of sustainability (i.e. social, economic, environment), and as such the pillars of sustainable AI. Rather, this paper is meant to inspire the reader, the policy maker, the AI ethicist, the AI developer to connect with the environment—to remember that there are environmental costs to AI. Further, to direct funding towards sustainable methods
              of
              AI.},
	language = {en},
	number = {3},
	urldate = {2023-06-25},
	journal = {AI and Ethics},
	author = {Van Wynsberghe, Aimee},
	month = aug,
	year = {2021},
	pages = {213--218},
	file = {Full text:C\:\\Users\\alice\\Zotero\\storage\\M4JCFC97\\Van Wynsberghe - 2021 - Sustainable AI AI for sustainability and the sust.pdf:application/pdf},
}

% 82
@article{rohde_sustainability_2021,
	title = {Sustainability challenges of {Artificial} {Intelligence} and {Policy} {Implications}},
	volume = {36},
	issn = {1430-8800},
	url = {http://oekologisches-wirtschaften.de/index.php/oew/article/view/1792},
	doi = {10.14512/OEWO360136},
	abstract = {Automated decision-making based on Artificial Intelligence is associated with growing expectations and is to contribute to sustainable development goals. Which opportunities and risks for the environment, economy and society are associated with Artificial Intelligence-based applications and how can they be governed?},
	number = {O1},
	urldate = {2023-06-25},
	journal = {Ökologisches Wirtschaften - Fachzeitschrift},
	author = {Rohde, Friederike and Gossen, Maike and Wagner, Josephin and Santarius, Tilman},
	month = feb,
	year = {2021},
	pages = {36--40},
	file = {Full text:C\:\\Users\\alice\\Zotero\\storage\\KMVT7E5M\\Rohde et al. - 2021 - Sustainability challenges of Artificial Intelligen.pdf:application/pdf},
}

%83
@article{noauthor_measuring_2022,
   author = {OECD},
   title = {Measuring the environmental impacts of artificial intelligence compute and applications},
   year = {2022},
   number = {341},
   url = {https://www.oecd-ilibrary.org/content/paper/7babf571-en},
   doi = {https://doi.org/https://doi.org/10.1787/7babf571-en}
}

% 84

@article{perucica_is_2022,
	title = {Is the future of {AI} sustainable? {A} case study of the {European} {Union}},
	volume = {16},
	issn = {1750-6166, 1750-6166},
	shorttitle = {Is the future of {AI} sustainable?},
	url = {https://www.emerald.com/insight/content/doi/10.1108/TG-06-2021-0106/full/html},
	doi = {10.1108/TG-06-2021-0106},
	abstract = {Purpose
              The purpose of this paper is to raise awareness on the need for a more comprehensive approach on the interdependence between artificial intelligence (AI) and environmental sustainability. It provides an overview of existing sustainable AI policy initiatives at the national and regional level. More precisely, it discusses whether existing European Union (EU) environmental policies are suitable for the AI era or whether new regulations are needed in this field. Finally, this paper assesses cross-fertilisation opportunities between the EU and non-EU countries.
            
            
              Design/methodology/approach
              This study is based on a qualitative analysis of sustainable applications of AI and the sustainability of AI. Emphasis is laid on the latter, and a “sustainable by design” approach is proposed, which in essence is a prerequisite for transparent, responsible and human-centred AI systems. The analysis primarily focuses on environmental sustainability.
            
            
              Findings
              The majority of studies focus on how to use AI to protect the environment with very little attention paid to sustainable design of AI. On the other hand, the EU’s comprehensive approach towards sustainable AI is closest to promoting “sustainable by design” AI. Several ways have been identified in which the EU’s actions can be translated beyond its borders.
            
            
              Research limitations/implications
              One of the largest limitations of this study is its moderate scope. This paper is confined to the EU and as such provides a limited assessment of global policies and measures on the interplay between sustainability and AI. Consequently, the paper did not provide an in-depth analysis of environmental policies worldwide that could help provide a better picture of possible cooperation areas or common grounds. Another limitation of this study is that it primarily focuses on environmental aspects and as such accords little attention to the economic and social pillars of sustainability.
            
            
              Social implications
              With less than 10 years to go before reaching the sustainable development goal deadline, this study can help stakeholders better understand what is being done worldwide in terms of sustainable AI. Moreover, given that the technology is still in its early phase, this study can inspire a “sustainable by design” approach to the development of AI technologies.
            
            
              Originality/value
              All national AI strategies published by 1 June 2021 were analysed to identify whether and to what extent they prioritise the interplay between environment and AI. Furthermore, the authors also looked at the EU policy and how it aims to address AI from a sustainable perspective.},
	language = {en},
	number = {3},
	urldate = {2023-06-25},
	journal = {Transforming Government: People, Process and Policy},
	author = {Perucica, Natasa and Andjelkovic, Katarina},
	month = jul,
	year = {2022},
	pages = {347--358},
	file = {Full text:C\:\\Users\\alice\\Zotero\\storage\\DQ7WXAFN\\Perucica e Andjelkovic - 2022 - Is the future of AI sustainable A case study of t.pdf:application/pdf},
}

% 85
@article{piorkowski_quantitative_2022,
	title = {Quantitative {AI} {Risk} {Assessments}: {Opportunities} and {Challenges}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Quantitative {AI} {Risk} {Assessments}},
	url = {https://arxiv.org/abs/2209.06317},
	doi = {10.48550/ARXIV.2209.06317},
	abstract = {Although AI-based systems are increasingly being leveraged to provide value to organizations, individuals, and society, significant attendant risks have been identified. These risks have led to proposed regulations, litigation, and general societal concerns. As with any promising technology, organizations want to benefit from the positive capabilities of AI technology while reducing the risks. The best way to reduce risks is to implement comprehensive AI lifecycle governance where policies and procedures are described and enforced during the design, development, deployment, and monitoring of an AI system. While support for comprehensive governance is beginning to emerge, organizations often need to identify the risks of deploying an already-built model without knowledge of how it was constructed or access to its original developers. Such an assessment will quantitatively assess the risks of an existing model in a manner analogous to how a home inspector might assess the energy efficiency of an already-built home or a physician might assess overall patient health based on a battery of tests. This paper explores the concept of a quantitative AI Risk Assessment, exploring the opportunities, challenges, and potential impacts of such an approach, and discussing how it might improve AI regulations.},
	urldate = {2023-06-25},
	author = {Piorkowski, David and Hind, Michael and Richards, John},
	year = {2022},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences},
}


% 86
@article{lacoste_quantifying_2019,
	title = {Quantifying the {Carbon} {Emissions} of {Machine} {Learning}},
	copyright = {Creative Commons Attribution Share Alike 4.0 International},
	url = {https://arxiv.org/abs/1910.09700},
	doi = {10.48550/ARXIV.1910.09700},
	abstract = {From an environmental standpoint, there are a few crucial aspects of training a neural network that have a major impact on the quantity of carbon that it emits. These factors include: the location of the server used for training and the energy grid that it uses, the length of the training procedure, and even the make and model of hardware on which the training takes place. In order to approximate these emissions, we present our Machine Learning Emissions Calculator, a tool for our community to better understand the environmental impact of training ML models. We accompany this tool with an explanation of the factors cited above, as well as concrete actions that individual practitioners and organizations can take to mitigate their carbon emissions.},
	urldate = {2023-06-25},
	author = {Lacoste, Alexandre and Luccioni, Alexandra and Schmidt, Victor and Dandres, Thomas},
	year = {2019},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {Computers and Society (cs.CY), FOS: Computer and information sciences, Machine Learning (cs.LG)},
}

% 87
@article{lannelongue_green_2021,
	title = {Green {Algorithms}: {Quantifying} the {Carbon} {Footprint} of {Computation}},
	volume = {8},
	issn = {2198-3844, 2198-3844},
	shorttitle = {Green {Algorithms}},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/advs.202100707},
	doi = {10.1002/advs.202100707},
	language = {en},
	number = {12},
	urldate = {2023-06-25},
	journal = {Advanced Science},
	author = {Lannelongue, Loïc and Grealey, Jason and Inouye, Michael},
	month = jun,
	year = {2021},
	pages = {2100707},
	file = {Full text:C\:\\Users\\alice\\Zotero\\storage\\WPD7NS96\\Lannelongue et al. - 2021 - Green Algorithms Quantifying the Carbon Footprint.pdf:application/pdf},
}


% 88
@article{strubell_energy_2020,
	title = {Energy and {Policy} {Considerations} for {Modern} {Deep} {Learning} {Research}},
	volume = {34},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/7123},
	doi = {10.1609/aaai.v34i09.7123},
	abstract = {The field of artificial intelligence has experienced a dramatic methodological shift towards large neural networks trained on plentiful data. This shift has been fueled by recent advances in hardware and techniques enabling remarkable levels of computation, resulting in impressive advances in AI across many applications. However, the massive computation required to obtain these exciting results is costly both financially, due to the price of specialized hardware and electricity or cloud compute time, and to the environment, as a result of non-renewable energy used to fuel modern tensor processing hardware. In a paper published this year at ACL, we brought this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training and tuning neural network models for NLP (Strubell, Ganesh, and McCallum 2019). In this extended abstract, we briefly summarize our findings in NLP, incorporating updated estimates and broader information from recent related publications, and provide actionable recommendations to reduce costs and improve equity in the machine learning and artificial intelligence community.},
	number = {09},
	urldate = {2023-06-25},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
	month = apr,
	year = {2020},
	pages = {13693--13696},
	file = {Full text:C\:\\Users\\alice\\Zotero\\storage\\NJWF93XC\\Strubell et al. - 2020 - Energy and Policy Considerations for Modern Deep L.pdf:application/pdf},
}


% 89
@inproceedings{manotas_empirical_2016,
	address = {Austin Texas},
	title = {An empirical study of practitioners' perspectives on green software engineering},
	isbn = {978-1-4503-3900-1},
	url = {https://dl.acm.org/doi/10.1145/2884781.2884810},
	doi = {10.1145/2884781.2884810},
	language = {en},
	urldate = {2023-06-28},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Manotas, Irene and Bird, Christian and Zhang, Rui and Shepherd, David and Jaspan, Ciera and Sadowski, Caitlin and Pollock, Lori and Clause, James},
	month = may,
	year = {2016},
	pages = {237--248},
	file = {Full text:C\:\\Users\\alice\\Zotero\\storage\\SEYJN25E\\Manotas et al. - 2016 - An empirical study of practitioners' perspectives .pdf:application/pdf},
}

% 90
@inproceedings{georgiou_green_2022,
	address = {Pittsburgh Pennsylvania},
	title = {Green {AI}: do deep learning frameworks have different costs?},
	isbn = {978-1-4503-9221-1},
	shorttitle = {Green {AI}},
	url = {https://dl.acm.org/doi/10.1145/3510003.3510221},
	doi = {10.1145/3510003.3510221},
	language = {en},
	urldate = {2023-02-20},
	booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Georgiou, Stefanos and Kechagia, Maria and Sharma, Tushar and Sarro, Federica and Zou, Ying},
	month = {may},
	year = {2022},
	pages = {1082--1094},
}

% 91
@article{pachot_towards_2022,
	title = {Towards {Sustainable} {Artificial} {Intelligence}: {An} {Overview} of {Environmental} {Protection} {Uses} and {Issues}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Towards {Sustainable} {Artificial} {Intelligence}},
	url = {https://arxiv.org/abs/2212.11738},
	doi = {10.48550/ARXIV.2212.11738},
	abstract = {Artificial Intelligence (AI) is used to create more sustainable production methods and model climate change, making it a valuable tool in the fight against environmental degradation. This paper describes the paradox of an energy-consuming technology serving the ecological challenges of tomorrow. The study provides an overview of the sectors that use AI-based solutions for environmental protection. It draws on numerous examples from AI for Green players to present use cases and concrete examples. In the second part of the study, the negative impacts of AI on the environment and the emerging technological solutions to support Green AI are examined. It is also shown that the research on less energy-consuming AI is motivated more by cost and energy autonomy constraints than by environmental considerations. This leads to a rebound effect that favors an increase in the complexity of models. Finally, the need to integrate environmental indicators into algorithms is discussed. The environmental dimension is part of the broader ethical problem of AI, and addressing it is crucial for ensuring the sustainability of AI in the long term.},
	urldate = {2023-02-10},
	author = {Pachot, Arnault and Patissier, Céline},
	year = {2022},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences},
}

% 92
@article{vinuesa_role_2020,
	title = {The role of artificial intelligence in achieving the {Sustainable} {Development} {Goals}},
	volume = {11},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-019-14108-y},
	doi = {10.1038/s41467-019-14108-y},
	abstract = {Abstract
            The emergence of artificial intelligence (AI) and its progressively wider impact on many sectors requires an assessment of its effect on the achievement of the Sustainable Development Goals. Using a consensus-based expert elicitation process, we find that AI can enable the accomplishment of 134 targets across all the goals, but it may also inhibit 59 targets. However, current research foci overlook important aspects. The fast development of AI needs to be supported by the necessary regulatory insight and oversight for AI-based technologies to enable sustainable development. Failure to do so could result in gaps in transparency, safety, and ethical standards.},
	language = {en},
	number = {1},
	urldate = {2023-06-28},
	journal = {Nature Communications},
	author = {Vinuesa, Ricardo and Azizpour, Hossein and Leite, Iolanda and Balaam, Madeline and Dignum, Virginia and Domisch, Sami and Felländer, Anna and Langhans, Simone Daniela and Tegmark, Max and Fuso Nerini, Francesco},
	month = jan,
	year = {2020},
	pages = {233},
	file = {Full text:C\:\\Users\\alice\\Zotero\\storage\\QDT5YBMJ\\Vinuesa et al. - 2020 - The role of artificial intelligence in achieving t.pdf:application/pdf},
}

% 93
@article{patterson2021carbon,
  title={Carbon emissions and large neural network training},
  author={Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
  journal={arXiv preprint arXiv:2104.10350},
  year={2021}
}

% 94
@inproceedings{verdecchia_data-centric_2022,
	address = {Plovdiv, Bulgaria},
	title = {Data-{Centric} {Green} {AI} {An} {Exploratory} {Empirical} {Study}},
	isbn = {978-1-66548-286-8},
	url = {https://ieeexplore.ieee.org/document/9830097/},
	doi = {10.1109/ICT4S55073.2022.00015},
	urldate = {2023-06-28},
	booktitle = {2022 {International} {Conference} on {ICT} for {Sustainability} ({ICT4S})},
	publisher = {IEEE},
	author = {Verdecchia, Roberto and Cruz, Luis and Sallou, June and Lin, Michelle and Wickenden, James and Hotellier, Estelle},
	month = jun,
	year = {2022},
	pages = {35--45},
	file = {Full text:C\:\\Users\\alice\\Zotero\\storage\\KR37VN3E\\Verdecchia et al. - 2022 - Data-Centric Green AI An Exploratory Empirical Stu.pdf:application/pdf},
}

% 95
@misc{shaikh2021energyvis,
      title={EnergyVis: Interactively Tracking and Exploring Energy Consumption for ML Models}, 
      author={Omar Shaikh and Jon Saad-Falcon and Austin P Wright and Nilaksh Das and Scott Freitas and Omar Isaac Asensio and Duen Horng Chau},
      year={2021},
      eprint={2103.16435},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% 96
@Article{Devlin2018_bert,
  author  = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  title   = {Bert: Pre-training of deep bidirectional transformers for language understanding},
  journal = {arXiv preprint arXiv:1810.04805},
  year    = {2018},
}

% 97
@article{strubell_energy_2019,
	title = {Energy and {Policy} {Considerations} for {Deep} {Learning} in {NLP}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1906.02243},
	doi = {10.48550/ARXIV.1906.02243},
	urldate = {2023-02-09},
	author = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
	year = {2019},
	note = {Publisher: arXiv Version Number: 1},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
}

% 98
@misc{https://doi.org/10.48550/arxiv.2008.00177,
  doi = {10.48550/ARXIV.2008.00177},
  url = {https://arxiv.org/abs/2008.00177},
  author = {Lin, Jiahuang and Li, Xin and Pekhimenko, Gennady},
  title = {Multi-node Bert-pretraining: Cost-efficient Approach},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

%99
@article{10.5555/3455716.3455856,
author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pretraining objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new "Colossal Clean Crawled Corpus", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {140},
numpages = {67},
keywords = {transfer learning, multi-task learning, deep learning, natural language processing, attention based models}
}

%100
@misc{https://doi.org/10.48550/arxiv.1905.05583,
  doi = {10.48550/ARXIV.1905.05583},
  url = {https://arxiv.org/abs/1905.05583},
  author = {Sun, Chi and Qiu, Xipeng and Xu, Yige and Huang, Xuanjing},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {How to Fine-Tune BERT for Text Classification?},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% 101
@article{joshi2020spanbert,
  title={Spanbert: Improving pre-training by representing and predicting spans},
  author={Joshi, Mandar and Chen, Danqi and Liu, Yinhan and Weld, Daniel S and Zettlemoyer, Luke and Levy, Omer},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={64--77},
  year={2020},
  publisher={MIT Press}
}

%102
@article{lee2020biobert,
  title={BioBERT: a pre-trained biomedical language representation model for biomedical text mining},
  author={Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
  journal={Bioinformatics},
  volume={36},
  number={4},
  pages={1234--1240},
  year={2020},
  publisher={Oxford University Press}
}

%103
@article{poerner2020inexpensive,
  title={Inexpensive domain adaptation of pretrained language models: Case studies on biomedical NER and covid-19 QA},
  author={Poerner, Nina and Waltinger, Ulli and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:2004.03354},
  year={2020}
}

%104
@misc{lan2020albert,
      title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations}, 
      author={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
      year={2020},
      eprint={1909.11942},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

%105
@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

%106
@article{li2020efficient,
  title={Efficient transformer-based large scale language representations using hardware-friendly block structured pruning},
  author={Li, Bingbing and Kong, Zhenglun and Zhang, Tianyun and Li, Ji and Li, Zhengang and Liu, Hang and Ding, Caiwen},
  journal={arXiv preprint arXiv:2009.08065},
  year={2020}
}

%107
@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={OpenAI}
}

%108
@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

% 109
@article{brock2018large,
  title={Large scale GAN training for high fidelity natural image synthesis},
  author={Brock, Andrew and Donahue, Jeff and Simonyan, Karen},
  journal={arXiv preprint arXiv:1809.11096},
  year={2018}
}

% 110
@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

%111
@article{chahal2020hitchhiker,
  title={A hitchhiker’s guide on distributed training of deep neural networks},
  author={Chahal, Karanbir Singh and Grover, Manraj Singh and Dey, Kuntal and Shah, Rajiv Ratn},
  journal={Journal of Parallel and Distributed Computing},
  volume={137},
  pages={65--76},
  year={2020},
  publisher={Elsevier}
}

%112
@article{iman2023review,
  title={A review of deep transfer learning and recent advancements},
  author={Iman, Mohammadreza and Arabnia, Hamid Reza and Rasheed, Khaled},
  journal={Technologies},
  volume={11},
  number={2},
  pages={40},
  year={2023},
  publisher={MDPI}
}

%113
@article{chatterjee2017progressive,
  title={Progressive learning for systematic design of large neural networks},
  author={Chatterjee, Saikat and Javid, Alireza M and Sadeghi, Mostafa and Mitra, Partha P and Skoglund, Mikael},
  journal={arXiv preprint arXiv:1710.08177},
  year={2017}
}


% 114
@article{rakka2022mixed,
  title={Mixed-precision neural networks: A survey},
  author={Rakka, Mariam and Fouda, Mohammed E and Khargonekar, Pramod and Kurdahi, Fadi},
  journal={arXiv preprint arXiv:2208.06064},
  year={2022}
}

%115
@article{hoefler2021sparsity,
  title={Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks},
  author={Hoefler, Torsten and Alistarh, Dan and Ben-Nun, Tal and Dryden, Nikoli and Peste, Alexandra},
  journal={The Journal of Machine Learning Research},
  volume={22},
  number={1},
  pages={10882--11005},
  year={2021},
  publisher={JMLRORG}
}



% 116
@article{deng2020model,
  title={Model compression and hardware acceleration for neural networks: A comprehensive survey},
  author={Deng, Lei and Li, Guoqi and Han, Song and Shi, Luping and Xie, Yuan},
  journal={Proceedings of the IEEE},
  volume={108},
  number={4},
  pages={485--532},
  year={2020},
  publisher={IEEE}
}

% 117
@article{gou_knowledge_2021,
	title = {Knowledge {Distillation}: {A} {Survey}},
	volume = {129},
	issn = {0920-5691, 1573-1405},
	shorttitle = {Knowledge {Distillation}},
	url = {https://link.springer.com/10.1007/s11263-021-01453-z},
	doi = {10.1007/s11263-021-01453-z},
	language = {en},
	number = {6},
	urldate = {2023-03-16},
	journal = {International Journal of Computer Vision},
	author = {Gou, Jianping and Yu, Baosheng and Maybank, Stephen J. and Tao, Dacheng},
	month = {jun},
	year = {2021},
	pages = {1789--1819},
	file = {Versione accettata:C\:\\Users\\Enric\\Zotero\\storage\\BFZPDPMN\\Gou et al. - 2021 - Knowledge Distillation A Survey.pdf:application/pdf},
}

% 118
@article{zhou2023comprehensive,
  title={A comprehensive survey on pretrained foundation models: A history from bert to chatgpt},
  author={Zhou, Ce and Li, Qian and Li, Chen and Yu, Jun and Liu, Yixin and Wang, Guangjing and Zhang, Kai and Ji, Cheng and Yan, Qiben and He, Lifang and others},
  journal={arXiv preprint arXiv:2302.09419},
  year={2023}
}


% 119
@article{you2019fast,
  title={Fast deep neural network training on distributed systems and cloud TPUs},
  author={You, Yang and Zhang, Zhao and Hsieh, Cho-Jui and Demmel, James and Keutzer, Kurt},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  volume={30},
  number={11},
  pages={2449--2462},
  year={2019},
  publisher={IEEE}
}

%120
@article{ma2021comparison,
  title={A comparison of approaches to document-level machine translation},
  author={Ma, Zhiyi and Edunov, Sergey and Auli, Michael},
  journal={arXiv preprint arXiv:2101.11040},
  year={2021}
}

&121
@article{dodge2020fine,
  title={Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping},
  author={Dodge, Jesse and Ilharco, Gabriel and Schwartz, Roy and Farhadi, Ali and Hajishirzi, Hannaneh and Smith, Noah},
  journal={arXiv preprint arXiv:2002.06305},
  year={2020}
}

% 122
@article{li2020pytorch,
  title={Pytorch distributed: Experiences on accelerating data parallel training},
  author={Li, Shen and Zhao, Yanli and Varma, Rohan and Salpekar, Omkar and Noordhuis, Pieter and Li, Teng and Paszke, Adam and Smith, Jeff and Vaughan, Brian and Damania, Pritam and others},
  journal={arXiv preprint arXiv:2006.15704},
  year={2020}
}





