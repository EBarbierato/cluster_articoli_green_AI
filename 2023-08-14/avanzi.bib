% DOPPIONE: Ã¨ sia al 7 che al 19
@article{xu_survey_2021,
	title = {A {Survey} on {Green} {Deep} {Learning}},
	copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
	url = {https://arxiv.org/abs/2111.05193},
	doi = {10.48550/ARXIV.2111.05193},
	abstract = {In recent years, larger and deeper models are springing up and continuously pushing state-of-the-art (SOTA) results across various fields like natural language processing (NLP) and computer vision (CV). However, despite promising results, it needs to be noted that the computations required by SOTA models have been increased at an exponential rate. Massive computations not only have a surprisingly large carbon footprint but also have negative effects on research inclusiveness and deployment on real-world applications. Green deep learning is an increasingly hot research field that appeals to researchers to pay attention to energy usage and carbon emission during model training and inference. The target is to yield novel results with lightweight and efficient technologies. Many technologies can be used to achieve this goal, like model compression and knowledge distillation. This paper focuses on presenting a systematic review of the development of Green deep learning technologies. We classify these approaches into four categories: (1) compact networks, (2) energy-efficient training strategies, (3) energy-efficient inference approaches, and (4) efficient data usage. For each category, we discuss the progress that has been achieved and the unresolved challenges.},
	urldate = {2023-02-20},
	author = {Xu, Jingjing and Zhou, Wangchunshu and Fu, Zhiyi and Zhou, Hao and Li, Lei},
	year = {2021},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG)},
}



% ----------------------------------------------------------
% non usati?



% Operational energy costs
@article{crawford2018anatomy,
  title={Anatomy of an AI System},
  author={Crawford, Kate and Joler, Vladan},
  journal={Retrieved September},
  volume={18},
  pages={2018},
  year={2018}
}


% Approaches. Algorithms
% Parallel training
@article{https://doi.org/10.48550/arxiv.1811.03600,
  doi = {10.48550/ARXIV.1811.03600},
  url = {https://arxiv.org/abs/1811.03600},
  author = {Shallue, Christopher J. and Lee, Jaehoon and Antognini, Joseph and Sohl-Dickstein, Jascha and Frostig, Roy and Dahl, George E.},
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Measuring the Effects of Data Parallelism on Neural Network Training},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{koutsoukou2021energy,
  title={On the Energy Consumption of Large-Scale Language Models},
  author={Koutsoukou-Argyraki, Angeliki and Ghosh, Debarghya and Arvind, V and Raffel, Colin},
  journal={arXiv preprint arXiv:2102.06171},
  year={2021}
}

@misc{https://doi.org/10.48550/arxiv.1802.05668,
  doi = {10.48550/ARXIV.1802.05668}, 
  url = {https://arxiv.org/abs/1802.05668}, 
  author = {Polino, Antonio and Pascanu, Razvan and Alistarh, Dan},  
  keywords = {Neural and Evolutionary Computing (cs.NE), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},  
  title = {Model compression via distillation and quantization},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

% Modified Hardware architectures
@article{hennessy1996computer,
  title={Computer architecture: a quantitative approach},
  author={Hennessy, John L and Patterson, David A},
  year={1996},
  publisher={Morgan Kaufmann}
}

@article{turing_computable_1937,
	title = {On {Computable} {Numbers}, with an {Application} to the {Entscheidungsproblem}},
	volume = {s2-42},
	issn = {00246115},
	url = {http://doi.wiley.com/10.1112/plms/s2-42.1.230},
	doi = {10.1112/plms/s2-42.1.230},
	language = {en},
	number = {1},
	urldate = {2023-03-22},
	journal = {Proceedings of the London Mathematical Society},
	author = {Turing, A. M.},
	year = {1937},
	pages = {230--265},
}

@article{graves_neural_2014,
	title = {Neural {Turing} {Machines}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1410.5401},
	doi = {10.48550/ARXIV.1410.5401},
	abstract = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
	urldate = {2023-03-22},
	author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
	year = {2014},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {FOS: Computer and information sciences, Neural and Evolutionary Computing (cs.NE)},
}

% Revealing the Dark Secrets of BERT https://arxiv.org/pdf/1908.08593.pdf
@article{kovaleva2019revealing,
  title={Revealing the dark secrets of BERT},
  author={Kovaleva, Olga and Romanov, Alexey and Rogers, Anna and Rumshisky, Anna},
  journal={arXiv preprint arXiv:1908.08593},
  year={2019}
}

% BERT Pre training of Deep Bidirectional Transformers for Language Understanding
%https://arxiv.org/pdf/1810.04805.pdf&usg=ALkJrhhzxlCL6yTht2BRmH9atgvKFxHsxQ
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

% DistilBERT  a distilled version of BERT smaller  faster  cheaper and lighter
%https://arxiv.org/pdf/1910.01108.pdf%3C/p%3E
@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

% Unsupervised Cross lingual Representation Learning at Scale
%https://arxiv.org/pdf/1911.02116.pdf
@article{conneau2019unsupervised,
  title={Unsupervised cross-lingual representation learning at scale},
  author={Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\'a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1911.02116},
  year={2019}
}

% EarlyBERT Efficient BERT Training via Early bird Lottery Tickets
%https://arxiv.org/pdf/2101.00063.pdf
@article{chen2020earlybert,
  title={Earlybert: Efficient bert training via early-bird lottery tickets},
  author={Chen, Xiaohan and Cheng, Yu and Wang, Shuohang and Gan, Zhe and Wang, Zhangyang and Liu, Jingjing},
  journal={arXiv preprint arXiv:2101.00063},
  year={2020}
}


@techreport{noauthor_online_2023,
	type = {{OECD} {Digital} {Economy} {Papers}},
	title = {Online product safety sweep report},
	url = {https://www.oecd-ilibrary.org/science-and-technology/online-product-safety-sweep-report_c1faa51e-en},
	language = {en},
	number = {354},
	urldate = {2023-06-25},
	month = jun,
	year = {2023},
	doi = {10.1787/c1faa51e-en},
	note = {Series: OECD Digital Economy Papers
Volume: 354},
}


@article{barbierato2023multiformalism,
  title={A Multiformalism-Based Model for Performance Evaluation of Green Data Centres},
  author={Barbierato, Enrico and Manini, Daniele and Gribaudo, Marco},
  journal={Electronics},
  volume={12},
  number={10},
  pages={2169},
  year={2023},
  publisher={MDPI}
}
