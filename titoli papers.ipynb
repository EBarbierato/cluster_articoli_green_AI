{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% First Draft of a Report on the EDVAC\n",
      "% DOI da IEEE del 1993: https://ieeexplore.ieee.org/document/238389\n",
      "@article{von_neumann_first_1993,\n",
      "title = {First draft of a report on the {EDVAC}},\n",
      "volume = {15},\n",
      "issn = {1058-6180},\n",
      "url = {http://ieeexplore.ieee.org/document/238389/},\n",
      "doi = {10.1109/85.238389},\n",
      "number = {4},\n",
      "urldate = {2023-02-16},\n",
      "journal = {IEEE Annals of the History of Computing},\n",
      "author = {von Neumann, J.},\n",
      "year = {1993},\n",
      "pages = {27--75},\n",
      "}\n",
      "\n",
      "@article{talati2020mmpu,\n",
      "title={mmpu—a real processing-in-memory architecture to combat the von neumann bottleneck},\n",
      "author={Talati, Nishil and Ben-Hur, Rotem and Wald, Nimrod and Haj-Ali, Ameer and Reuben, John and Kvatinsky, Shahar},\n",
      "journal={Applications of Emerging Memory Technology: Beyond Storage},\n",
      "pages={191--213},\n",
      "year={2020},\n",
      "publisher={Springer}\n",
      "}\n",
      "\n",
      "% Schwartz Green AI (primissimo articolo su green ai, 2019)\n",
      "% nell'introduzione\n",
      "@misc{schwartz_green_2019,\n",
      "title = {Green {AI}},\n",
      "url = {http://arxiv.org/abs/1907.10597},\n",
      "abstract = {The computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018 [2]. These computations have a surprisingly large carbon footprint [38]. Ironically, deep learning was inspired by the human brain, which is remarkably energy efficient. Moreover, the financial cost of the computations can make it difficult for academics, students, and researchers, in particular those from emerging economies, to engage in deep learning research. This position paper advocates a practical solution by making efficiency an evaluation criterion for research alongside accuracy and related measures. In addition, we propose reporting the financial cost or \"price tag\" of developing, training, and running models to provide baselines for the investigation of increasingly efficient methods. Our goal is to make AI both greener and more inclusive---enabling any inspired undergraduate with a laptop to write high-quality research papers. Green AI is an emerging focus at the Allen Institute for AI.},\n",
      "urldate = {2023-02-08},\n",
      "publisher = {arXiv},\n",
      "author = {Schwartz, Roy and Dodge, Jesse and Smith, Noah A. and Etzioni, Oren},\n",
      "month = {aug},\n",
      "year = {2019},\n",
      "note = {arXiv:1907.10597 [cs, stat]},\n",
      "keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computers and Society, Computer Science - Machine Learning, Statistics - Methodology},\n",
      "} \n",
      "\n",
      "\n",
      "% Schwartz Green AI 2020 (trovato 8-9/2)\n",
      "% nell'introduzione\n",
      "@article{schwartz_green_2020,\n",
      "title = {Green {AI}},\n",
      "volume = {63},\n",
      "issn = {0001-0782, 1557-7317},\n",
      "url = {https://dl.acm.org/doi/10.1145/3381831},\n",
      "doi = {10.1145/3381831},\n",
      "abstract = {Creating efficiency in AI research will decrease its carbon footprint and increase its inclusivity as deep learning study should not require the deepest pockets.},\n",
      "language = {en},\n",
      "number = {12},\n",
      "urldate = {2023-02-08},\n",
      "journal = {Communications of the ACM},\n",
      "author = {Schwartz, Roy and Dodge, Jesse and Smith, Noah A. and Etzioni, Oren},\n",
      "month = {nov},\n",
      "year = {2020},\n",
      "pages = {54--63},\n",
      "file = {Testo integrale:C\\:\\\\Users\\\\alice\\\\Zotero\\\\storage\\\\N4U29MDB\\\\Schwartz et al. - 2020 - Green AI.pdf:application/pdf},\n",
      "}\n",
      "\n",
      "\n",
      "% Energy and Policy Considerations for Deep Learning in NLP\n",
      "% nell'introduzione\n",
      "@article{strubell_energy_2019,\n",
      "title = {Energy and {Policy} {Considerations} for {Deep} {Learning} in {NLP}},\n",
      "copyright = {arXiv.org perpetual, non-exclusive license},\n",
      "url = {https://arxiv.org/abs/1906.02243},\n",
      "doi = {10.48550/ARXIV.1906.02243},\n",
      "urldate = {2023-02-09},\n",
      "author = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},\n",
      "year = {2019},\n",
      "note = {Publisher: arXiv Version Number: 1},\n",
      "keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},\n",
      "}\n",
      "\n",
      "\n",
      "@article{poerner2020inexpensive,\n",
      "title={Inexpensive domain adaptation of pretrained language models: Case studies on biomedical NER and covid-19 QA},\n",
      "author={Poerner, Nina and Waltinger, Ulli and Sch{\\\"u}tze, Hinrich},\n",
      "journal={arXiv preprint arXiv:2004.03354},\n",
      "year={2020}\n",
      "}\n",
      "\n",
      "\n",
      "% Articolo corto leggero su usi vari ai (es. agricoltura...), uno dei prmiissimi.\n",
      "% per introduzione\n",
      "@article{pachot_towards_2022,\n",
      "title = {Towards {Sustainable} {Artificial} {Intelligence}: {An} {Overview} of {Environmental} {Protection} {Uses} and {Issues}},\n",
      "copyright = {arXiv.org perpetual, non-exclusive license},\n",
      "shorttitle = {Towards {Sustainable} {Artificial} {Intelligence}},\n",
      "url = {https://arxiv.org/abs/2212.11738},\n",
      "doi = {10.48550/ARXIV.2212.11738},\n",
      "abstract = {Artificial Intelligence (AI) is used to create more sustainable production methods and model climate change, making it a valuable tool in the fight against environmental degradation. This paper describes the paradox of an energy-consuming technology serving the ecological challenges of tomorrow. The study provides an overview of the sectors that use AI-based solutions for environmental protection. It draws on numerous examples from AI for Green players to present use cases and concrete examples. In the second part of the study, the negative impacts of AI on the environment and the emerging technological solutions to support Green AI are examined. It is also shown that the research on less energy-consuming AI is motivated more by cost and energy autonomy constraints than by environmental considerations. This leads to a rebound effect that favors an increase in the complexity of models. Finally, the need to integrate environmental indicators into algorithms is discussed. The environmental dimension is part of the broader ethical problem of AI, and addressing it is crucial for ensuring the sustainability of AI in the long term.},\n",
      "urldate = {2023-02-10},\n",
      "author = {Pachot, Arnault and Patissier, Céline},\n",
      "year = {2022},\n",
      "note = {Publisher: arXiv\n",
      "Version Number: 1},\n",
      "keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences},\n",
      "}\n",
      "\n",
      "% nell'introduzione\n",
      "% red ai\n",
      "@article{xu_survey_2021,\n",
      "title = {A {Survey} on {Green} {Deep} {Learning}},\n",
      "copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},\n",
      "url = {https://arxiv.org/abs/2111.05193},\n",
      "doi = {10.48550/ARXIV.2111.05193},\n",
      "abstract = {In recent years, larger and deeper models are springing up and continuously pushing state-of-the-art (SOTA) results across various fields like natural language processing (NLP) and computer vision (CV). However, despite promising results, it needs to be noted that the computations required by SOTA models have been increased at an exponential rate. Massive computations not only have a surprisingly large carbon footprint but also have negative effects on research inclusiveness and deployment on real-world applications. Green deep learning is an increasingly hot research field that appeals to researchers to pay attention to energy usage and carbon emission during model training and inference. The target is to yield novel results with lightweight and efficient technologies. Many technologies can be used to achieve this goal, like model compression and knowledge distillation. This paper focuses on presenting a systematic review of the development of Green deep learning technologies. We classify these approaches into four categories: (1) compact networks, (2) energy-efficient training strategies, (3) energy-efficient inference approaches, and (4) efficient data usage. For each category, we discuss the progress that has been achieved and the unresolved challenges.},\n",
      "urldate = {2023-02-20},\n",
      "author = {Xu, Jingjing and Zhou, Wangchunshu and Fu, Zhiyi and Zhou, Hao and Li, Lei},\n",
      "year = {2021},\n",
      "note = {Publisher: arXiv\n",
      "Version Number: 2},\n",
      "keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG)},\n",
      "}\n",
      "@article{lin2022survey,\n",
      "title={A survey of transformers},\n",
      "author={Lin, Tianyang and Wang, Yuxin and Liu, Xiangyang and Qiu, Xipeng},\n",
      "journal={AI Open},\n",
      "year={2022},\n",
      "publisher={Elsevier}\n",
      "}\n",
      "\n",
      "@article{kirmani2022artificial,\n",
      "title={Artificial Intelligence-Enabled Science Poetry},\n",
      "author={Kirmani, Ahmad R},\n",
      "journal={ACS Energy Letters},\n",
      "volume={8},\n",
      "pages={574--576},\n",
      "year={2022},\n",
      "publisher={ACS Publications}\n",
      "}\n",
      "\n",
      "@article{goodfellow2020generative,\n",
      "title={Generative adversarial networks},\n",
      "author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},\n",
      "journal={Communications of the ACM},\n",
      "volume={63},\n",
      "number={11},\n",
      "pages={139--144},\n",
      "year={2020},\n",
      "publisher={ACM New York, NY, USA}\n",
      "}\n",
      "\n",
      "@article{brock2018large,\n",
      "title={Large scale GAN training for high fidelity natural image synthesis},\n",
      "author={Brock, Andrew and Donahue, Jeff and Simonyan, Karen},\n",
      "journal={arXiv preprint arXiv:1809.11096},\n",
      "year={2018}\n",
      "}\n",
      "\n",
      "@article{surameery2023use,\n",
      "title={Use chat gpt to solve programming bugs},\n",
      "author={Surameery, Nigar M Shafiq and Shakor, Mohammed Y},\n",
      "journal={International Journal of Information Technology \\& Computer Engineering (IJITC) ISSN: 2455-5290},\n",
      "volume={3},\n",
      "number={01},\n",
      "pages={17--22},\n",
      "year={2023}\n",
      "}\n",
      "\n",
      "@article{wu2023brief,\n",
      "title={A brief overview of ChatGPT: The history, status quo and potential future development},\n",
      "author={Wu, Tianyu and He, Shizhu and Liu, Jingping and Sun, Siqi and Liu, Kang and Han, Qing-Long and Tang, Yang},\n",
      "journal={IEEE/CAA Journal of Automatica Sinica},\n",
      "volume={10},\n",
      "number={5},\n",
      "pages={1122--1136},\n",
      "year={2023},\n",
      "publisher={IEEE}\n",
      "}\n",
      "% nell'introduzione\n",
      "% red ai\n",
      "@inproceedings{georgiou_green_2022,\n",
      "address = {Pittsburgh Pennsylvania},\n",
      "title = {Green {AI}: do deep learning frameworks have different costs?},\n",
      "isbn = {978-1-4503-9221-1},\n",
      "shorttitle = {Green {AI}},\n",
      "url = {https://dl.acm.org/doi/10.1145/3510003.3510221},\n",
      "doi = {10.1145/3510003.3510221},\n",
      "language = {en},\n",
      "urldate = {2023-02-20},\n",
      "booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},\n",
      "publisher = {ACM},\n",
      "author = {Georgiou, Stefanos and Kechagia, Maria and Sharma, Tushar and Sarro, Federica and Zou, Ying},\n",
      "month = {may},\n",
      "year = {2022},\n",
      "pages = {1082--1094},\n",
      "}\n",
      "\n",
      "% nell'introduzione\n",
      "% red ai\n",
      "% bib preso da ieee (copiato da online, non usato zotero)\n",
      "@ARTICLE{9810097,\n",
      "author={Patterson, David and Gonzalez, Joseph and Hölzle, Urs and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David R. and Texier, Maud and Dean, Jeff},\n",
      "journal={Computer}, \n",
      "title={The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink}, \n",
      "year={2022},\n",
      "volume={55},\n",
      "number={7},\n",
      "pages={18-28},\n",
      "doi={10.1109/MC.2022.3148714}}\n",
      "\n",
      "% Deep Learning\n",
      "@article{lecun_deep_2015,\n",
      "title = {Deep learning},\n",
      "volume = {521},\n",
      "issn = {0028-0836, 1476-4687},\n",
      "url = {http://www.nature.com/articles/nature14539},\n",
      "doi = {10.1038/nature14539},\n",
      "language = {en},\n",
      "number = {7553},\n",
      "urldate = {2023-02-28},\n",
      "journal = {Nature},\n",
      "author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},\n",
      "month = {may},\n",
      "year = {2015},\n",
      "pages = {436--444},\n",
      "}\n",
      "\n",
      "% Reinforcement learning\n",
      "@article{10.1145/3459991,\n",
      "author = {Padakandla, Sindhu},\n",
      "title = {A Survey of Reinforcement Learning Algorithms for Dynamically Varying Environments},\n",
      "year = {2021},\n",
      "issue_date = {July 2022},\n",
      "publisher = {Association for Computing Machinery},\n",
      "address = {New York, NY, USA},\n",
      "volume = {54},\n",
      "number = {6},\n",
      "issn = {0360-0300},\n",
      "url = {https://doi.org/10.1145/3459991},\n",
      "doi = {10.1145/3459991},\n",
      "month = {jul},\n",
      "articleno = {127},\n",
      "numpages = {25},\n",
      "keywords = {Reinforcement learning, sequential decision-making, context detection, meta-learning, non-stationary environments, regret computation, Markov decision processes}\n",
      "}\n",
      "\n",
      "% GANS\n",
      "\n",
      "@article{wang_generative_2022,\n",
      "title = {Generative {Adversarial} {Networks} in {Computer} {Vision}: {A} {Survey} and {Taxonomy}},\n",
      "volume = {54},\n",
      "issn = {0360-0300, 1557-7341},\n",
      "shorttitle = {Generative {Adversarial} {Networks} in {Computer} {Vision}},\n",
      "url = {https://dl.acm.org/doi/10.1145/3439723},\n",
      "doi = {10.1145/3439723},\n",
      "abstract = {Generative adversarial networks (GANs) have been extensively studied in the past few years. Arguably their most significant impact has been in the area of computer vision where great advances have been made in challenges such as plausible image generation, image-to-image translation, facial attribute manipulation, and similar domains. Despite the significant successes achieved to date, applying GANs to real-world problems still poses significant challenges, three of which we focus on here. These are as follows: (1) the generation of high quality images, (2) diversity of image generation, and (3) stabilizing training. Focusing on the degree to which popular GAN technologies have made progress against these challenges, we provide a detailed review of the state-of-the-art in GAN-related research in the published scientific literature. We further structure this review through a convenient taxonomy we have adopted based on variations in GAN architectures and loss functions. While several reviews for GANs have been presented to date, none have considered the status of this field based on their progress toward addressing practical challenges relevant to computer vision. Accordingly, we review and critically discuss the most popular architecture-variant, and loss-variant GANs, for tackling these challenges. Our objective is to provide an overview as well as a critical analysis of the status of GAN research in terms of relevant progress toward critical computer vision application requirements. As we do this we also discuss the most compelling applications in computer vision in which GANs have demonstrated considerable success along with some suggestions for future research directions. Codes related to the GAN-variants studied in this work is summarized on https://github.com/sheqi/GAN\\_Review.},\n",
      "language = {en},\n",
      "number = {2},\n",
      "urldate = {2023-02-28},\n",
      "journal = {ACM Computing Surveys},\n",
      "author = {Wang, Zhengwei and She, Qi and Ward, Tomás E.},\n",
      "month = {mar},\n",
      "year = {2022},\n",
      "pages = {1--38},\n",
      "file = {Full text:C\\:\\\\Users\\\\Enric\\\\Zotero\\\\storage\\\\E25BXL2D\\\\Wang et al. - 2022 - Generative Adversarial Networks in Computer Vision.pdf:application/pdf},\n",
      "}\n",
      "\n",
      "% Transfer learning\n",
      "@Article{math10193619,\n",
      "AUTHOR = {Yu, Fuchao and Xiu, Xianchao and Li, Yunhui},\n",
      "TITLE = {A Survey on Deep Transfer Learning and Beyond},\n",
      "JOURNAL = {Mathematics},\n",
      "VOLUME = {10},\n",
      "YEAR = {2022},\n",
      "NUMBER = {19},\n",
      "ARTICLE-NUMBER = {3619},\n",
      "URL = {https://www.mdpi.com/2227-7390/10/19/3619},\n",
      "ISSN = {2227-7390},\n",
      "ABSTRACT = {Deep transfer learning (DTL), which incorporates new ideas from deep neural networks into transfer learning (TL), has achieved excellent success in computer vision, text classification, behavior recognition, and natural language processing. As a branch of machine learning, DTL applies end-to-end learning to overcome the drawback of traditional machine learning that regards each dataset individually. Although some valuable and impressive general surveys exist on TL, special attention and recent advances in DTL are lacking. In this survey, we first review more than 50 representative approaches of DTL in the last decade and systematically summarize them into four categories. In particular, we further divide each category into subcategories according to models, functions, and operation objects. In addition, we discuss recent advances in TL in other fields and unsupervised TL. Finally, we provide some possible and exciting future research directions.},\n",
      "DOI = {10.3390/math10193619}\n",
      "}\n",
      "\n",
      "% AutoML\n",
      "@article{automl,\n",
      "author = {He, Xin and Zhao, Kaiyong and Chu, Xiaowen},\n",
      "year = {2021},\n",
      "month = {01},\n",
      "pages = {106622},\n",
      "title = {AutoML: A survey of the state-of-the-art},\n",
      "volume = {212},\n",
      "journal = {Knowledge-Based Systems},\n",
      "doi = {10.1016/j.knosys.2020.106622}\n",
      "}\n",
      "\n",
      "% Von Neumann bottleneck\n",
      "@article{\n",
      "backus_can_1978,\n",
      "title = {Can programming be liberated from the von {Neumann} style?: a functional style and its algebra of programs},\n",
      "volume = {21},\n",
      "issn = {0001-0782, 1557-7317},\n",
      "shorttitle = {Can programming be liberated from the von {Neumann} style?},\n",
      "url = {https://dl.acm.org/doi/10.1145/359576.359579},\n",
      "doi = {10.1145/359576.359579},\n",
      "language = {en},\n",
      "number = {8},\n",
      "urldate = {2023-02-28},\n",
      "journal = {Communications of the ACM},\n",
      "author = {Backus, John},\n",
      "month = {aug},\n",
      "year = {1978},\n",
      "pages = {613--641}\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "@article{vaswani2017attention,\n",
      "title={Attention is all you need},\n",
      "author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\\L}ukasz and Polosukhin, Illia},\n",
      "journal={Advances in neural information processing systems},\n",
      "volume={30},\n",
      "year={2017}\n",
      "}\n",
      "\n",
      "% Connection machine\n",
      "@article{kahle1989connection,\n",
      "title={The connection machine model cm-1 architecture},\n",
      "author={Kahle, Brewster A and Hillis, W Daniel},\n",
      "journal={IEEE transactions on systems, man, and cybernetics},\n",
      "volume={19},\n",
      "number={4},\n",
      "pages={707--713},\n",
      "year={1989},\n",
      "publisher={IEEE}\n",
      "}\n",
      "\n",
      "% PCA\n",
      "@article{pearson_liii_1901,\n",
      "title = {{LIII}. \\textit{{On} lines and planes of closest fit to systems of points in space}},\n",
      "volume = {2},\n",
      "issn = {1941-5982, 1941-5990},\n",
      "url = {https://www.tandfonline.com/doi/full/10.1080/14786440109462720},\n",
      "doi = {10.1080/14786440109462720},\n",
      "language = {en},\n",
      "number = {11},\n",
      "urldate = {2023-02-28},\n",
      "journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},\n",
      "author = {Pearson, Karl},\n",
      "month = {nov},\n",
      "year = {1901},\n",
      "pages = {559--572},\n",
      "file = {Versione inviata:C\\:\\\\Users\\\\Enric\\\\Zotero\\\\storage\\\\RM7USJ9H\\\\Pearson - 1901 - LIII. On lines and planes of closest fit to sys.pdf:application/pdf},\n",
      "}\n",
      "% autoencoders\n",
      "@article{kramer_nonlinear_1991,\n",
      "title = {Nonlinear principal component analysis using autoassociative neural networks},\n",
      "volume = {37},\n",
      "issn = {0001-1541, 1547-5905},\n",
      "url = {https://onlinelibrary.wiley.com/doi/10.1002/aic.690370209},\n",
      "doi = {10.1002/aic.690370209},\n",
      "language = {en},\n",
      "number = {2},\n",
      "urldate = {2023-02-28},\n",
      "journal = {AIChE Journal},\n",
      "author = {Kramer, Mark A.},\n",
      "month = {feb},\n",
      "year = {1991},\n",
      "pages = {233--243},\n",
      "}\n",
      "\n",
      "% CNN\n",
      "@article{CNN,\n",
      "author = {Hasegawa, Akira and Lo, Shih-Chung and Lin, Jyh-Shyan and Freedman, Matthew and Mun, Seong},\n",
      "year = {1998},\n",
      "month = {04},\n",
      "pages = {241-250},\n",
      "title = {A Shift-Invariant Neural Network for the Lung Field Segmentation in Chest Radiography},\n",
      "volume = {18},\n",
      "journal = {VLSI Signal Processing},\n",
      "doi = {10.1023/A:1007937214367}\n",
      "}\n",
      "\n",
      "% RNN\n",
      "@article{RNN,\n",
      "title = {A {Novel} {Connectionist} {System} for {Unconstrained} {Handwriting} {Recognition}},\n",
      "volume = {31},\n",
      "issn = {0162-8828},\n",
      "url = {http://ieeexplore.ieee.org/document/4531750/},\n",
      "doi = {10.1109/TPAMI.2008.137},\n",
      "number = {5},\n",
      "urldate = {2023-02-28},\n",
      "journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n",
      "author = {Graves, A. and Liwicki, M. and Fernandez, S. and Bertolami, R. and Bunke, H. and Schmidhuber, J.},\n",
      "month = {may},\n",
      "year = {2009},\n",
      "pages = {855--868},\n",
      "file = {Versione inviata:C\\:\\\\Users\\\\Enric\\\\Zotero\\\\storage\\\\FDNR7EB8\\\\Graves et al. - 2009 - A Novel Connectionist System for Unconstrained Han.pdf:application/pdf},\n",
      "}\n",
      "\n",
      "\n",
      "@article{LSTM,\n",
      "title = {Long {Short}-{Term} {Memory}},\n",
      "volume = {9},\n",
      "issn = {0899-7667, 1530-888X},\n",
      "url = {https://direct.mit.edu/neco/article/9/8/1735-1780/6109},\n",
      "doi = {10.1162/neco.1997.9.8.1735},\n",
      "abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},\n",
      "language = {en},\n",
      "number = {8},\n",
      "urldate = {2023-02-28},\n",
      "journal = {Neural Computation},\n",
      "author = {Hochreiter, Sepp and Schmidhuber, Jürgen},\n",
      "month = {nov},\n",
      "year = {1997},\n",
      "pages = {1735--1780},\n",
      "}\n",
      "% Definizione di \"carbon footprint\"\n",
      "@article{wiedmann2008definition,\n",
      "title={A definition of ‘carbon footprint’},\n",
      "author={Wiedmann, Thomas and Minx, Jan},\n",
      "journal={Ecological economics research trends},\n",
      "volume={1},\n",
      "number={2008},\n",
      "pages={1--11},\n",
      "year={2008},\n",
      "publisher={Nova Science Publishers Hauppauge, NY}\n",
      "}\n",
      "\n",
      "% Articolo che descrive curse of dimensionality. Bellman 1962\n",
      "@article{hammer_adaptive_1962,\n",
      "title = {Adaptive {Control} {Processes}: {A} {Guided} {Tour} ({R}. {Bellman})},\n",
      "volume = {4},\n",
      "issn = {0036-1445, 1095-7200},\n",
      "shorttitle = {Adaptive {Control} {Processes}},\n",
      "url = {http://epubs.siam.org/doi/10.1137/1004050},\n",
      "doi = {10.1137/1004050},\n",
      "language = {en},\n",
      "number = {2},\n",
      "urldate = {2023-03-20},\n",
      "journal = {SIAM Review},\n",
      "author = {Hammer, P. C.},\n",
      "month = {apr},\n",
      "year = {1962},\n",
      "pages = {163--163},\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "@article{noauthor_measuring_2022,\n",
      "author = {OECD},\n",
      "title = {Measuring the environmental impacts of artificial intelligence compute and applications},\n",
      "year = {2022},\n",
      "number = {341},\n",
      "url = {https://www.oecd-ilibrary.org/content/paper/7babf571-en},\n",
      "doi = {https://doi.org/https://doi.org/10.1787/7babf571-en}\n",
      "}\n",
      "\n",
      "\n",
      "% Operational energy costs\n",
      "@article{crawford2018anatomy,\n",
      "title={Anatomy of an AI System},\n",
      "author={Crawford, Kate and Joler, Vladan},\n",
      "journal={Retrieved September},\n",
      "volume={18},\n",
      "pages={2018},\n",
      "year={2018}\n",
      "}\n",
      "\n",
      "% BERT\n",
      "@Article{Devlin2018_bert,\n",
      "author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},\n",
      "title = {Bert: Pre-training of deep bidirectional transformers for language understanding},\n",
      "journal = {arXiv preprint arXiv:1810.04805},\n",
      "year = {2018},\n",
      "}\n",
      "\n",
      "\n",
      "% History of NLP\n",
      "@Inbook{Jones1994,\n",
      "author={Jones, Karen Sparck},\n",
      "title={Natural Language Processing: A Historical Review},\n",
      "bookTitle={Current Issues in Computational Linguistics: In Honour of Don Walker},\n",
      "year={1994},\n",
      "publisher={Springer Netherlands},\n",
      "address={Dordrecht},\n",
      "pages={3--16}\n",
      "}\n",
      "\n",
      "% Shank (MT)\n",
      "@incollection{Schank75,\t\n",
      "AUTHOR = \t{R. C. Schank},\t\n",
      "TITLE = {Conceptual Dependency Theory},\t\n",
      "YEAR = {1975},\t\n",
      "BOOKTITLE = \t{Conceptual Information Processing},\t\n",
      "EDITOR = \t{R. C. Schank},\t\n",
      "PUBLISHER = \t{North-Holland and Elsevier},\t\n",
      "ADDRESS = \t{Amsterdam and New York},\t\n",
      "PAGES = \t{22-82}, \t\n",
      "KEYWORDS = \t{},\n",
      "ANNOTE =\t{ The book presents the Conceptual Dependency Theory which can be used by a machine for conceptual understanding of the given text.}\n",
      "}\t\n",
      "\n",
      "@inproceedings{johri2021natural,\n",
      "title={Natural language processing: History, evolution, application, and future work},\n",
      "author={Johri, Prashant and Khatri, Sunil K and Al-Taani, Ahmad T and Sabharwal, Munish and Suvanov, Shakhzod and Kumar, Avneesh},\n",
      "booktitle={Proceedings of 3rd International Conference on Computing Informatics and Networks: ICCIN 2020},\n",
      "pages={365--375},\n",
      "year={2021},\n",
      "organization={Springer}\n",
      "}\n",
      "\n",
      "% XLNet\n",
      "@misc{https://doi.org/10.48550/arxiv.1906.08237,\n",
      "doi = {10.48550/ARXIV.1906.08237},\n",
      "url = {https://arxiv.org/abs/1906.08237},\n",
      "author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},\n",
      "keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},\n",
      "title = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},\n",
      "publisher = {arXiv},\n",
      "year = {2019},\n",
      "copyright = {Creative Commons Attribution 4.0 International}\n",
      "}\n",
      "\n",
      "@article{raffel2020exploring,\n",
      "title={Exploring the limits of transfer learning with a unified text-to-text transformer},\n",
      "author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},\n",
      "journal={The Journal of Machine Learning Research},\n",
      "volume={21},\n",
      "number={1},\n",
      "pages={5485--5551},\n",
      "year={2020},\n",
      "publisher={JMLRORG}\n",
      "}\n",
      "\n",
      "% Come funziona BERT\n",
      "@article{rogers2021primer,\n",
      "title={A primer in BERTology: What we know about how BERT works},\n",
      "author={Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},\n",
      "journal={Transactions of the Association for Computational Linguistics},\n",
      "volume={8},\n",
      "pages={842--866},\n",
      "year={2021},\n",
      "publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}\n",
      "}\n",
      "\n",
      "% Proposta di training HW per BERT\n",
      "@misc{https://doi.org/10.48550/arxiv.2008.00177,\n",
      "doi = {10.48550/ARXIV.2008.00177},\n",
      "url = {https://arxiv.org/abs/2008.00177},\n",
      "author = {Lin, Jiahuang and Li, Xin and Pekhimenko, Gennady},\n",
      "title = {Multi-node Bert-pretraining: Cost-efficient Approach},\n",
      "publisher = {arXiv},\n",
      "year = {2020},\n",
      "copyright = {arXiv.org perpetual, non-exclusive license}\n",
      "}\n",
      "\n",
      "\n",
      "%%%%%%%%%%%%%%%%%%%%%%%\n",
      "% APPROACHES\n",
      "%%%%%%%%%%%%%%%%%%%%%%\n",
      "% Miglioramento training di BERT\n",
      "@misc{https://doi.org/10.48550/arxiv.2101.00063,\n",
      "doi = {10.48550/ARXIV.2101.00063},\n",
      "url = {https://arxiv.org/abs/2101.00063},\n",
      "author = {Chen, Xiaohan and Cheng, Yu and Wang, Shuohang and Gan, Zhe and Wang, Zhangyang and Liu, Jingjing},\n",
      "keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},\n",
      "title = {EarlyBERT: Efficient BERT Training via Early-bird Lottery Tickets},\n",
      "publisher = {arXiv},\n",
      "year = {2021},\n",
      "copyright = {arXiv.org perpetual, non-exclusive license}\n",
      "}\n",
      "\n",
      "\n",
      "% Approaches. Algorithms\n",
      "% Parallel training\n",
      "@article{https://doi.org/10.48550/arxiv.1811.03600,\n",
      "doi = {10.48550/ARXIV.1811.03600},\n",
      "url = {https://arxiv.org/abs/1811.03600},\n",
      "author = {Shallue, Christopher J. and Lee, Jaehoon and Antognini, Joseph and Sohl-Dickstein, Jascha and Frostig, Roy and Dahl, George E.},\n",
      "keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},\n",
      "title = {Measuring the Effects of Data Parallelism on Neural Network Training},\n",
      "publisher = {arXiv},\n",
      "year = {2018},\n",
      "copyright = {arXiv.org perpetual, non-exclusive license}\n",
      "}\n",
      "% Parallel model\n",
      "@article{DBLP:journals/corr/Krizhevsky14,\n",
      "author = {Alex Krizhevsky},\n",
      "title = {One weird trick for parallelizing convolutional neural networks},\n",
      "journal = {CoRR},\n",
      "volume = {abs/1404.5997},\n",
      "year = {2014},\n",
      "url = {http://arxiv.org/abs/1404.5997},\n",
      "eprinttype = {arXiv},\n",
      "eprint = {1404.5997},\n",
      "timestamp = {Mon, 13 Aug 2018 16:48:41 +0200},\n",
      "biburl = {https://dblp.org/rec/journals/corr/Krizhevsky14.bib},\n",
      "bibsource = {dblp computer science bibliography, https://dblp.org}\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "% data structures - tensor\n",
      "@article{lewis2021large,\n",
      "title={Large scale distributed linear algebra with tensor processing units},\n",
      "author={Lewis, Adam GM and Beall, Jackson and Ganahl, Martin and Hauru, Markus and Mallick, Shrestha Basu and Vidal, Guifre},\n",
      "journal={arXiv preprint arXiv:2112.09017},\n",
      "year={2021}\n",
      "}\n",
      "\n",
      "@article{ji_survey_2019,\n",
      "title = {A {Survey} on {Tensor} {Techniques} and {Applications} in {Machine} {Learning}},\n",
      "volume = {7},\n",
      "issn = {2169-3536},\n",
      "url = {https://ieeexplore.ieee.org/document/8884203/},\n",
      "doi = {10.1109/ACCESS.2019.2949814},\n",
      "urldate = {2023-02-26},\n",
      "journal = {IEEE Access},\n",
      "author = {Ji, Yuwang and Wang, Qiang and Li, Xuan and Liu, Jie},\n",
      "year = {2019},\n",
      "pages = {162950--162990},\n",
      "file = {Full text:C\\:\\\\Users\\\\alice\\\\Zotero\\\\storage\\\\JPVL3QXL\\\\Ji et al. - 2019 - A Survey on Tensor Techniques and Applications in .pdf:application/pdf},\n",
      "}\n",
      "\n",
      "% data structures - tensor\n",
      "@article{rabanser_introduction_2017,\n",
      "title = {Introduction to {Tensor} {Decompositions} and their {Applications} in {Machine} {Learning}},\n",
      "copyright = {arXiv.org perpetual, non-exclusive license},\n",
      "url = {https://arxiv.org/abs/1711.10781},\n",
      "doi = {10.48550/ARXIV.1711.10781},\n",
      "abstract = {Tensors are multidimensional arrays of numerical values and therefore generalize matrices to multiple dimensions. While tensors first emerged in the psychometrics community in the \\$20{\\textasciicircum}\\{{\\textbackslash}text\\{th\\}\\}\\$ century, they have since then spread to numerous other disciplines, including machine learning. Tensors and their decompositions are especially beneficial in unsupervised learning settings, but are gaining popularity in other sub-disciplines like temporal and multi-relational data analysis, too. The scope of this paper is to give a broad overview of tensors, their decompositions, and how they are used in machine learning. As part of this, we are going to introduce basic tensor concepts, discuss why tensors can be considered more rigid than matrices with respect to the uniqueness of their decomposition, explain the most important factorization algorithms and their properties, provide concrete examples of tensor decomposition applications in machine learning, conduct a case study on tensor-based estimation of mixture models, talk about the current state of research, and provide references to available software libraries.},\n",
      "urldate = {2023-02-26},\n",
      "author = {Rabanser, Stephan and Shchur, Oleksandr and Günnemann, Stephan},\n",
      "year = {2017},\n",
      "note = {Publisher: arXiv\n",
      "Version Number: 1},\n",
      "keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML)},\n",
      "}\n",
      "\n",
      "% data structures - tensor\n",
      "@article{kolda_tensor_2009,\n",
      "title = {Tensor {Decompositions} and {Applications}},\n",
      "volume = {51},\n",
      "issn = {0036-1445, 1095-7200},\n",
      "url = {http://epubs.siam.org/doi/10.1137/07070111X},\n",
      "doi = {10.1137/07070111X},\n",
      "language = {en},\n",
      "number = {3},\n",
      "urldate = {2023-02-26},\n",
      "journal = {SIAM Review},\n",
      "author = {Kolda, Tamara G. and Bader, Brett W.},\n",
      "month = {aug},\n",
      "year = {2009},\n",
      "pages = {455--500},\n",
      "}\n",
      "\n",
      "% data structures - tensor\n",
      "@article{bernardi_general_2013,\n",
      "title = {General tensor decomposition, moment matrices and applications},\n",
      "volume = {52},\n",
      "issn = {0747-7171},\n",
      "url = {https://www.sciencedirect.com/science/article/pii/S0747717112001290},\n",
      "doi = {https://doi.org/10.1016/j.jsc.2012.05.012},\n",
      "abstract = {The tensor decomposition addressed in this paper may be seen as a generalization of Singular Value Decomposition of matrices. We consider general multilinear and multihomogeneous tensors. We show how to reduce the problem to a truncated moment matrix problem and give a new criterion for flat extension of Quasi-Hankel matrices. We connect this criterion to the commutation characterization of border bases. A new algorithm is described. It applies for general multihomogeneous tensors, extending the approach on binary forms by J.J. Sylvester. An example illustrates the algebraic operations involved in this approach and how the decomposition can be recovered from eigenvector computation.},\n",
      "journal = {Journal of Symbolic Computation},\n",
      "author = {Bernardi, A. and Brachat, J. and Comon, P. and Mourrain, B.},\n",
      "year = {2013},\n",
      "keywords = {Decomposition, Flat extension, Hankel operator, Moment matrix, Multihomogeneous polynomial, Rank, Tensor},\n",
      "pages = {51--71},\n",
      "}\n",
      "@article{10.5555/3455716.3455856,\n",
      "author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},\n",
      "title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},\n",
      "year = {2020},\n",
      "issue_date = {January 2020},\n",
      "publisher = {JMLR.org},\n",
      "volume = {21},\n",
      "number = {1},\n",
      "issn = {1532-4435},\n",
      "abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pretraining objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},\n",
      "journal = {J. Mach. Learn. Res.},\n",
      "month = {jan},\n",
      "articleno = {140},\n",
      "numpages = {67},\n",
      "keywords = {transfer learning, multi-task learning, deep learning, natural language processing, attention based models}\n",
      "}\n",
      "\n",
      "@misc{https://doi.org/10.48550/arxiv.1905.05583,\n",
      "doi = {10.48550/ARXIV.1905.05583},\n",
      "url = {https://arxiv.org/abs/1905.05583},\n",
      "author = {Sun, Chi and Qiu, Xipeng and Xu, Yige and Huang, Xuanjing},\n",
      "keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},\n",
      "title = {How to Fine-Tune BERT for Text Classification?},\n",
      "publisher = {arXiv},\n",
      "year = {2019},\n",
      "copyright = {arXiv.org perpetual, non-exclusive license}\n",
      "}\n",
      "\n",
      "@article{koutsoukou2021energy,\n",
      "title={On the Energy Consumption of Large-Scale Language Models},\n",
      "author={Koutsoukou-Argyraki, Angeliki and Ghosh, Debarghya and Arvind, V and Raffel, Colin},\n",
      "journal={arXiv preprint arXiv:2102.06171},\n",
      "year={2021}\n",
      "}\n",
      "\n",
      "\n",
      "@misc{https://doi.org/10.48550/arxiv.1802.05668,\n",
      "doi = {10.48550/ARXIV.1802.05668}, \n",
      "url = {https://arxiv.org/abs/1802.05668}, \n",
      "author = {Polino, Antonio and Pascanu, Razvan and Alistarh, Dan}, \n",
      "keywords = {Neural and Evolutionary Computing (cs.NE), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences}, \n",
      "title = {Model compression via distillation and quantization},\n",
      "publisher = {arXiv},\n",
      "year = {2018},\n",
      "copyright = {arXiv.org perpetual, non-exclusive license}\n",
      "}\n",
      "\n",
      "% Algorithms\n",
      "@article{mayer2020scalable,\n",
      "title={Scalable deep learning on distributed infrastructures: Challenges, techniques, and tools},\n",
      "author={Mayer, Ruben and Jacobsen, Hans-Arno},\n",
      "journal={ACM Computing Surveys (CSUR)},\n",
      "volume={53},\n",
      "number={1},\n",
      "pages={1--37},\n",
      "year={2020},\n",
      "publisher={ACM New York, NY, USA}\n",
      "}\n",
      "\n",
      "@article{gou_knowledge_2021,\n",
      "title = {Knowledge {Distillation}: {A} {Survey}},\n",
      "volume = {129},\n",
      "issn = {0920-5691, 1573-1405},\n",
      "shorttitle = {Knowledge {Distillation}},\n",
      "url = {https://link.springer.com/10.1007/s11263-021-01453-z},\n",
      "doi = {10.1007/s11263-021-01453-z},\n",
      "language = {en},\n",
      "number = {6},\n",
      "urldate = {2023-03-16},\n",
      "journal = {International Journal of Computer Vision},\n",
      "author = {Gou, Jianping and Yu, Baosheng and Maybank, Stephen J. and Tao, Dacheng},\n",
      "month = {jun},\n",
      "year = {2021},\n",
      "pages = {1789--1819},\n",
      "file = {Versione accettata:C\\:\\\\Users\\\\Enric\\\\Zotero\\\\storage\\\\BFZPDPMN\\\\Gou et al. - 2021 - Knowledge Distillation A Survey.pdf:application/pdf},\n",
      "}\n",
      "\n",
      "% Residual connection\n",
      "@misc{he2015deep,\n",
      "title={Deep Residual Learning for Image Recognition}, \n",
      "author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},\n",
      "year={2015},\n",
      "eprint={1512.03385},\n",
      "archivePrefix={arXiv},\n",
      "primaryClass={cs.CV}\n",
      "}\n",
      "\n",
      "% Depthwise convolutiion for MobiNet\n",
      "@article{howard2017mobilenets,\n",
      "title={MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications},\n",
      "author={Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},\n",
      "journal={arXiv preprint arXiv:1704.04861},\n",
      "year={2017}\n",
      "}\n",
      "\n",
      "% Weight sharing\n",
      "@article{lecun1989backpropagation,\n",
      "title={Backpropagation applied to handwritten zip code recognition},\n",
      "author={LeCun, Yann and Boser, Bernhard and Denker, John S and Henderson, Donnie and Howard, Richard E and Hubbard, Wayne and Jackel, Lawrence D},\n",
      "journal={Neural computation},\n",
      "volume={1},\n",
      "number={4},\n",
      "pages={541--551},\n",
      "year={1989},\n",
      "publisher={MIT Press}\n",
      "}\n",
      "\n",
      "% QUantisation\n",
      "@article{wu2016quantized,\n",
      "title={Quantized convolutional neural networks for mobile devices},\n",
      "author={Wu, Sheng and Liang, Hong and Zhang, Yiran and Sun, Rui and Wang, Ting and He, Xuming},\n",
      "journal={arXiv preprint arXiv:1603.05279},\n",
      "year={2016}\n",
      "}\n",
      "\n",
      "% Model binarization\n",
      "@inproceedings{courbariaux2015binaryconnect,\n",
      "title={Binaryconnect: Training deep neural networks with binary weights during propagations},\n",
      "author={Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},\n",
      "booktitle={Advances in neural information processing systems},\n",
      "pages={3123--3131},\n",
      "year={2015}\n",
      "}\n",
      "\n",
      "% Structural matrices\n",
      "@inproceedings{sindhwani2015structured,\n",
      "title={Structured transforms for small-footprint deep learning},\n",
      "author={Sindhwani, Vikas and Sainath, Tara N and Kumar, Sanjiv},\n",
      "booktitle={Proceedings of the 32nd International Conference on Machine Learning},\n",
      "pages={1664--1672},\n",
      "year={2015},\n",
      "organization={JMLR Workshop and Conference Proceedings}\n",
      "}\n",
      "\n",
      "% Computer architectures\n",
      "% Hardware architecture\n",
      "\n",
      "% Questa citazione sembra svagliata! Non la uso\n",
      "@article{harvardarchitecture,\n",
      "title={A Logical Calculus of Ideas Immanent in Nervous Activity},\n",
      "author={McCulloch, Warren S. and Pitts, Walter},\n",
      "journal={The Bulletin of Mathematical Biophysics},\n",
      "volume={5},\n",
      "number={4},\n",
      "pages={115-133},\n",
      "year={1943},\n",
      "publisher={Springer}\n",
      "}\n",
      "@article{rosenblatt1960perceptron,\n",
      "title={Perceptron simulation experiments},\n",
      "author={Rosenblatt, Frank},\n",
      "journal={Proceedings of the IRE},\n",
      "volume={48},\n",
      "number={3},\n",
      "pages={301--309},\n",
      "year={1960},\n",
      "publisher={IEEE}\n",
      "}\n",
      "\n",
      "@article{bravyi2022future,\n",
      "title={The future of quantum computing with superconducting qubits},\n",
      "author={Bravyi, Sergey and Dial, Oliver and Gambetta, Jay M and Gil, Dario and Nazario, Zaira},\n",
      "journal={Journal of Applied Physics},\n",
      "volume={132},\n",
      "number={16},\n",
      "pages={160902},\n",
      "year={2022},\n",
      "publisher={AIP Publishing LLC}\n",
      "}\n",
      "\n",
      "@inproceedings{elsayed2019review,\n",
      "title={A review of quantum computer energy efficiency},\n",
      "author={Elsayed, Nelly and Maida, Anthony S and Bayoumi, Magdy},\n",
      "booktitle={2019 IEEE Green Technologies Conference (GreenTech)},\n",
      "pages={1--3},\n",
      "year={2019},\n",
      "organization={IEEE}\n",
      "}\n",
      "\n",
      "@article{wang2013review,\n",
      "title={Review of performance metrics for green data centers: a taxonomy study},\n",
      "author={Wang, Lizhe and Khan, Samee U},\n",
      "journal={The journal of supercomputing},\n",
      "volume={63},\n",
      "pages={639--656},\n",
      "year={2013},\n",
      "publisher={Springer}\n",
      "}\n",
      "\n",
      "@book{vetter2013contemporary,\n",
      "title={Contemporary high performance computing: from Petascale toward exascale},\n",
      "author={Vetter, Jeffrey S},\n",
      "year={2013},\n",
      "publisher={CRC Press}\n",
      "}\n",
      "\n",
      "@article{pawson2022myth,\n",
      "title={The myth of the Harvard architecture},\n",
      "author={Pawson, Richard},\n",
      "journal={IEEE Annals of the History of Computing},\n",
      "volume={44},\n",
      "number={3},\n",
      "pages={59--69},\n",
      "year={2022},\n",
      "publisher={IEEE}\n",
      "}\n",
      "\n",
      "% Modified Hardware architectures\n",
      "@article{hennessy1996computer,\n",
      "title={Computer architecture: a quantitative approach},\n",
      "author={Hennessy, John L and Patterson, David A},\n",
      "year={1996},\n",
      "publisher={Morgan Kaufmann}\n",
      "}\n",
      "\n",
      "% Neuromorphjic\n",
      "@article{markram2006blue,\n",
      "title={The blue brain project},\n",
      "author={Markram, Henry},\n",
      "journal={Nature Reviews Neuroscience},\n",
      "volume={7},\n",
      "number={2},\n",
      "pages={153--160},\n",
      "year={2006},\n",
      "publisher={Nature Publishing Group UK London}\n",
      "}\n",
      "\n",
      "% IBM TrueNorth\n",
      "@article{akopyan2015truenorth,\n",
      "title={Truenorth: Design and tool flow of a 65 mw 1 million neuron programmable neurosynaptic chip},\n",
      "author={Akopyan, Filipp and Sawada, Jun and Cassidy, Andrew and Alvarez-Icaza, Rodrigo and Arthur, John and Merolla, Paul and Imam, Nabil and Nakamura, Yutaka and Datta, Pallab and Nam, Gi-Joon and others},\n",
      "journal={IEEE transactions on computer-aided design of integrated circuits and systems},\n",
      "volume={34},\n",
      "number={10},\n",
      "pages={1537--1557},\n",
      "year={2015},\n",
      "publisher={IEEE}\n",
      "}\n",
      "\n",
      "% Spiking Neural Networks (SNNs) Architecture:\n",
      "@article{maass1997networks,\n",
      "title={Networks of spiking neurons: the third generation of neural network models},\n",
      "author={Maass, Wolfgang},\n",
      "journal={Neural networks},\n",
      "volume={10},\n",
      "number={9},\n",
      "pages={1659--1671},\n",
      "year={1997},\n",
      "publisher={Elsevier}\n",
      "}\n",
      "\n",
      "%Liquid State Machines (LSMs) Architecture:\n",
      "@article{maass2002real,\n",
      "title={Real-time computing without stable states: A new framework for neural computation based on perturbations},\n",
      "author={Maass, Wolfgang and Natschl{\\\"a}ger, Thomas and Markram, Henry},\n",
      "journal={Neural computation},\n",
      "volume={14},\n",
      "number={11},\n",
      "pages={2531--2560},\n",
      "year={2002},\n",
      "publisher={MIT Press}\n",
      "}\n",
      "\n",
      "% Cellular Neural Networks (CNNs) Architecture:\n",
      "@inproceedings{chua1988cellular,\n",
      "title={Cellular neural networks: theory},\n",
      "author={Chua, Leon O and Yang, Lin},\n",
      "booktitle={IEEE Transactions on Circuits and Systems},\n",
      "volume={35},\n",
      "number={10},\n",
      "pages={1257--1272},\n",
      "year={1988},\n",
      "organization={IEEE}\n",
      "}\n",
      "\n",
      "%Self-Organizing Maps (SOMs) Architecture:\n",
      "@inproceedings{kohonen1982self,\n",
      "title={Self-organized formation of topologically correct feature maps},\n",
      "author={Kohonen, Teuvo},\n",
      "booktitle={Biological cybernetics},\n",
      "volume={43},\n",
      "number={1},\n",
      "pages={59--69},\n",
      "year={1982},\n",
      "organization={Springer}\n",
      "}\n",
      "\n",
      "% Hopfield Networks (HNs) Architecture:\n",
      "@article{hopfield1982neural,\n",
      "title={Neural networks and physical systems with emergent collective computational abilities},\n",
      "author={Hopfield, John J},\n",
      "journal={Proceedings of the national academy of sciences},\n",
      "volume={79},\n",
      "number={8},\n",
      "pages={2554--2558},\n",
      "year={1982},\n",
      "publisher={National Acad Sciences}\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "% Radial Basis Function Networks (RBFNs) Architecture:\n",
      "@article{park1991universal,\n",
      "title={Universal approximation using radial-basis-function networks},\n",
      "author={Park, Jong-Soon and Sandberg, Irwin W.},\n",
      "journal={Neural computation},\n",
      "volume={3},\n",
      "number={2},\n",
      "pages={246--257},\n",
      "year={1991},\n",
      "publisher={MIT Press}\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "% Neural Turing Machines (NTMs) Architecture:\n",
      "@article{graves2014neural,\n",
      "title={Neural turing machines},\n",
      "author={Graves, Alex and Wayne, Greg and Danihelka, Ivo},\n",
      "journal={arXiv preprint arXiv:1410.5401},\n",
      "year={2014}\n",
      "}\n",
      "\n",
      "\n",
      "@article{turing_computable_1937,\n",
      "title = {On {Computable} {Numbers}, with an {Application} to the {Entscheidungsproblem}},\n",
      "volume = {s2-42},\n",
      "issn = {00246115},\n",
      "url = {http://doi.wiley.com/10.1112/plms/s2-42.1.230},\n",
      "doi = {10.1112/plms/s2-42.1.230},\n",
      "language = {en},\n",
      "number = {1},\n",
      "urldate = {2023-03-22},\n",
      "journal = {Proceedings of the London Mathematical Society},\n",
      "author = {Turing, A. M.},\n",
      "year = {1937},\n",
      "pages = {230--265},\n",
      "}\n",
      "\n",
      "@article{graves_neural_2014,\n",
      "title = {Neural {Turing} {Machines}},\n",
      "copyright = {arXiv.org perpetual, non-exclusive license},\n",
      "url = {https://arxiv.org/abs/1410.5401},\n",
      "doi = {10.48550/ARXIV.1410.5401},\n",
      "abstract = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},\n",
      "urldate = {2023-03-22},\n",
      "author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},\n",
      "year = {2014},\n",
      "note = {Publisher: arXiv\n",
      "Version Number: 2},\n",
      "keywords = {FOS: Computer and information sciences, Neural and Evolutionary Computing (cs.NE)},\n",
      "}\n",
      "\n",
      "\n",
      "% NTM\n",
      "@article{suresh_memory_2022,\n",
      "title = {Memory augmented recurrent neural networks for de-novo drug design},\n",
      "volume = {17},\n",
      "issn = {1932-6203},\n",
      "url = {https://dx.plos.org/10.1371/journal.pone.0269461},\n",
      "doi = {10.1371/journal.pone.0269461},\n",
      "abstract = {A recurrent neural network (RNN) is a machine learning model that learns the relationship between elements of an input series, in addition to inferring a relationship between the data input to the model and target output. Memory augmentation allows the RNN to learn the interrelationships between elements of the input over a protracted length of the input series. Inspired by the success of stack augmented RNN (StackRNN) to generate strings for various applications, we present two memory augmented RNN-based architectures: the Neural Turing Machine (NTM) and the Differentiable Neural Computer (DNC) for the\n",
      "de-novo\n",
      "generation of small molecules. We trained a character-level convolutional neural network (CNN) to predict the properties of a generated string and compute a reward or loss in a deep reinforcement learning setup to bias the Generator to produce molecules with the desired property. Further, we compare the performance of these architectures to gain insight to their relative merits in terms of the validity and novelty of the generated molecules and the degree of property bias towards the computational generation of\n",
      "de-novo\n",
      "drugs. We also compare the performance of these architectures with simpler recurrent neural networks (Vanilla RNN, LSTM, and GRU) without an external memory component to explore the impact of augmented memory in the task of\n",
      "de-novo\n",
      "generation of small molecules.},\n",
      "language = {en},\n",
      "number = {6},\n",
      "urldate = {2023-03-27},\n",
      "journal = {PLOS ONE},\n",
      "author = {Suresh, Naveen and Chinnakonda Ashok Kumar, Neelesh and Subramanian, Srikumar and Srinivasa, Gowri},\n",
      "editor = {Nguyen, Binh P.},\n",
      "month = {jun},\n",
      "year = {2022},\n",
      "pages = {e0269461},\n",
      "file = {Full text:C\\:\\\\Users\\\\alice\\\\Zotero\\\\storage\\\\C7ULJRHU\\\\Suresh et al. - 2022 - Memory augmented recurrent neural networks for de-.pdf:application/pdf},\n",
      "}\n",
      "\n",
      "% NTM\n",
      "@misc{gulcehre_dynamic_2017,\n",
      "title = {Dynamic {Neural} {Turing} {Machine} with {Soft} and {Hard} {Addressing} {Schemes}},\n",
      "url = {http://arxiv.org/abs/1607.00036},\n",
      "abstract = {We extend neural Turing machine (NTM) model into a dynamic neural Turing machine (D-NTM) by introducing a trainable memory addressing scheme. This addressing scheme maintains for each memory cell two separate vectors, content and address vectors. This allows the D-NTM to learn a wide variety of location-based addressing strategies including both linear and nonlinear ones. We implement the D-NTM with both continuous, differentiable and discrete, non-differentiable read/write mechanisms. We investigate the mechanisms and effects of learning to read and write into a memory through experiments on Facebook bAbI tasks using both a feedforward and GRUcontroller. The D-NTM is evaluated on a set of Facebook bAbI tasks and shown to outperform NTM and LSTM baselines. We have done extensive analysis of our model and different variations of NTM on bAbI task. We also provide further experimental results on sequential pMNIST, Stanford Natural Language Inference, associative recall and copy tasks.},\n",
      "urldate = {2023-03-27},\n",
      "publisher = {arXiv},\n",
      "author = {Gulcehre, Caglar and Chandar, Sarath and Cho, Kyunghyun and Bengio, Yoshua},\n",
      "month = {mar},\n",
      "year = {2017},\n",
      "note = {arXiv:1607.00036 [cs]},\n",
      "keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},\n",
      "file = {arXiv Fulltext PDF:C\\:\\\\Users\\\\alice\\\\Zotero\\\\storage\\\\SL9U3D49\\\\Gulcehre et al. - 2017 - Dynamic Neural Turing Machine with Soft and Hard A.pdf:application/pdf;arXiv.org Snapshot:C\\:\\\\Users\\\\alice\\\\Zotero\\\\storage\\\\ZY7JUZTU\\\\1607.html:text/html},\n",
      "}\n",
      "\n",
      "@article{tyagi2022recurrent,\n",
      "title={Recurrent Neural Networks: Concepts and Applications},\n",
      "author={Tyagi, Amit Kumar and Abraham, Ajith},\n",
      "year={2022},\n",
      "publisher={CRC Press}\n",
      "}\n",
      "\n",
      "% da qui alcune evoluzioni della NTM\n",
      "\n",
      "@misc{zaremba_reinforcement_2016,\n",
      "title = {Reinforcement {Learning} {Neural} {Turing} {Machines} - {Revised}},\n",
      "url = {http://arxiv.org/abs/1505.00521},\n",
      "abstract = {The Neural Turing Machine (NTM) is more expressive than all previously considered models because of its external memory. It can be viewed as a broader effort to use abstract external Interfaces and to learn a parametric model that interacts with them. The capabilities of a model can be extended by providing it with proper Interfaces that interact with the world. These external Interfaces include memory, a database, a search engine, or a piece of software such as a theorem verifier. Some of these Interfaces are provided by the developers of the model. However, many important existing Interfaces, such as databases and search engines, are discrete. We examine feasibility of learning models to interact with discrete Interfaces. We investigate the following discrete Interfaces: a memory Tape, an input Tape, and an output Tape. We use a Reinforcement Learning algorithm to train a neural network that interacts with such Interfaces to solve simple algorithmic tasks. Our Interfaces are expressive enough to make our model Turing complete.},\n",
      "urldate = {2023-03-27},\n",
      "publisher = {arXiv},\n",
      "author = {Zaremba, Wojciech and Sutskever, Ilya},\n",
      "month = {jan},\n",
      "year = {2016},\n",
      "note = {arXiv:1505.00521 [cs]},\n",
      "keywords = {Computer Science - Machine Learning},\n",
      "file = {arXiv Fulltext PDF:C\\:\\\\Users\\\\alice\\\\Zotero\\\\storage\\\\YLZB4AGR\\\\Zaremba e Sutskever - 2016 - Reinforcement Learning Neural Turing Machines - Re.pdf:application/pdf},\n",
      "}\n",
      "\n",
      "\n",
      "@inproceedings{greve_evolving_2016,\n",
      "address = {Denver Colorado USA},\n",
      "title = {Evolving {Neural} {Turing} {Machines} for {Reward}-based {Learning}},\n",
      "isbn = {978-1-4503-4206-3},\n",
      "url = {https://dl.acm.org/doi/10.1145/2908812.2908930},\n",
      "doi = {10.1145/2908812.2908930},\n",
      "language = {en},\n",
      "urldate = {2023-03-27},\n",
      "booktitle = {Proceedings of the {Genetic} and {Evolutionary} {Computation} {Conference} 2016},\n",
      "publisher = {ACM},\n",
      "author = {Greve, Rasmus Boll and Jacobsen, Emil Juul and Risi, Sebastian},\n",
      "month = {jul},\n",
      "year = {2016},\n",
      "pages = {117--124},\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "@misc{yang_lie_2016,\n",
      "title = {Lie {Access} {Neural} {Turing} {Machine}},\n",
      "url = {http://arxiv.org/abs/1602.08671},\n",
      "abstract = {Following the recent trend in explicit neural memory structures, we present a new design of an external memory, wherein memories are stored in an Euclidean key space \\${\\textbackslash}mathbb R{\\textasciicircum}n\\$. An LSTM controller performs read and write via specialized read and write heads. It can move a head by either providing a new address in the key space (aka random access) or moving from its previous position via a Lie group action (aka Lie access). In this way, the \"L\" and \"R\" instructions of a traditional Turing Machine are generalized to arbitrary elements of a fixed Lie group action. For this reason, we name this new model the Lie Access Neural Turing Machine, or LANTM. We tested two different configurations of LANTM against an LSTM baseline in several basic experiments. We found the right configuration of LANTM to outperform the baseline in all of our experiments. In particular, we trained LANTM on addition of \\$k\\$-digit numbers for \\$2 {\\textbackslash}le k {\\textbackslash}le 16\\$, but it was able to generalize almost perfectly to \\$17 {\\textbackslash}le k {\\textbackslash}le 32\\$, all with the number of parameters 2 orders of magnitude below the LSTM baseline.},\n",
      "urldate = {2023-03-27},\n",
      "publisher = {arXiv},\n",
      "author = {Yang, Greg},\n",
      "month = {sep},\n",
      "year = {2016},\n",
      "note = {arXiv:1602.08671 [cs]},\n",
      "keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},\n",
      "file = {arXiv Fulltext PDF:C\\:\\\\Users\\\\alice\\\\Zotero\\\\storage\\\\3F3C3R78\\\\Yang - 2016 - Lie Access Neural Turing Machine.pdf:application/pdf},\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "@misc{kurach_neural_2016,\n",
      "title = {Neural {Random}-{Access} {Machines}},\n",
      "url = {http://arxiv.org/abs/1511.06392},\n",
      "urldate = {2023-03-27},\n",
      "publisher = {arXiv},\n",
      "author = {Kurach, Karol and Andrychowicz, Marcin and Sutskever, Ilya},\n",
      "month = {feb},\n",
      "year = {2016},\n",
      "note = {arXiv:1511.06392 [cs]},\n",
      "keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},\n",
      "file = {arXiv Fulltext PDF:C\\:\\\\Users\\\\alice\\\\Zotero\\\\storage\\\\XPLSQT9K\\\\Kurach et al. - 2016 - Neural Random-Access Machines.pdf:application/pdf},\n",
      "}\n",
      "\n",
      "\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "% Enrico Cluster 0\n",
      "% SpanBERT Improving Pre training by Representing and Predicting Spans\n",
      "@article{joshi2020spanbert,\n",
      "title={Spanbert: Improving pre-training by representing and predicting spans},\n",
      "author={Joshi, Mandar and Chen, Danqi and Liu, Yinhan and Weld, Daniel S and Zettlemoyer, Luke and Levy, Omer},\n",
      "journal={Transactions of the Association for Computational Linguistics},\n",
      "volume={8},\n",
      "pages={64--77},\n",
      "year={2020},\n",
      "publisher={MIT Press}\n",
      "}\n",
      "% BioBERT a pre trained biomedical language representation model for biomedical text mining\n",
      "% ALBERT A Lite BERT for Self supervised Learning of Language Representations\n",
      "@misc{lan2020albert,\n",
      "title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations}, \n",
      "author={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},\n",
      "year={2020},\n",
      "eprint={1909.11942},\n",
      "archivePrefix={arXiv},\n",
      "primaryClass={cs.CL}\n",
      "}\n",
      "\n",
      "@article{chowdhery2022palm,\n",
      "title={Palm: Scaling language modeling with pathways},\n",
      "author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},\n",
      "journal={arXiv preprint arXiv:2204.02311},\n",
      "year={2022}\n",
      "}\n",
      "% Efficient Transformer based Large Scale Language Representations using Hardware friendly Block Structured Pruning INTRUSO?????? NON ESATTAMENTE BERT PERCHE' VIENE CITATO 121 VOLTE! https://arxiv.org/pdf/2009.08065.pdf\n",
      "@article{li2020efficient,\n",
      "title={Efficient transformer-based large scale language representations using hardware-friendly block structured pruning},\n",
      "author={Li, Bingbing and Kong, Zhenglun and Zhang, Tianyun and Li, Ji and Li, Zhengang and Liu, Hang and Ding, Caiwen},\n",
      "journal={arXiv preprint arXiv:2009.08065},\n",
      "year={2020}\n",
      "}\n",
      "% Revealing the Dark Secrets of BERT https://arxiv.org/pdf/1908.08593.pdf\n",
      "@article{kovaleva2019revealing,\n",
      "title={Revealing the dark secrets of BERT},\n",
      "author={Kovaleva, Olga and Romanov, Alexey and Rogers, Anna and Rumshisky, Anna},\n",
      "journal={arXiv preprint arXiv:1908.08593},\n",
      "year={2019}\n",
      "}\n",
      "% A Survey on Green Deep Learning INTRUSO??????? NON ESATTAMENTE PERCHE' BERT VIENE CITATO 76 VOLTE\n",
      "@article{xu2021survey,\n",
      "title={A survey on green deep learning},\n",
      "author={Xu, Jingjing and Zhou, Wangchunshu and Fu, Zhiyi and Zhou, Hao and Li, Lei},\n",
      "journal={arXiv preprint arXiv:2111.05193},\n",
      "year={2021}\n",
      "}\n",
      "\n",
      "% RoBERTa A Robustly Optimized BERT Pretraining Approach https://arxiv.org/pdf/1907.11692.pdf%5C\n",
      "@article{liu2019roberta,\n",
      "title={Roberta: A robustly optimized bert pretraining approach},\n",
      "author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},\n",
      "journal={arXiv preprint arXiv:1907.11692},\n",
      "year={2019}\n",
      "}\n",
      "% Exploring the Limits of Transfer Learning with a Unified Text to Text Transformer (già citato)\n",
      "%https://www.jmlr.org/papers/volume21/20-074/20-074.pdf INTRUSO??????? NON ESATTAMENTE PERCHE' BERT VIENE CITATO CIRCA 100 VOLTE\n",
      "\n",
      "% BERT Pre training of Deep Bidirectional Transformers for Language Understanding\n",
      "%https://arxiv.org/pdf/1810.04805.pdf&usg=ALkJrhhzxlCL6yTht2BRmH9atgvKFxHsxQ\n",
      "@article{devlin2018bert,\n",
      "title={Bert: Pre-training of deep bidirectional transformers for language understanding},\n",
      "author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},\n",
      "journal={arXiv preprint arXiv:1810.04805},\n",
      "year={2018}\n",
      "}\n",
      "\n",
      "\n",
      "% A Primer in BERTology What We Know About How BERT Works\n",
      "%https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=A+Primer+in+BERTology+What+We+Know+About+How+BERT+Works&btnG= GIA' CITATO\n",
      "\n",
      "\n",
      "% Knowledge Distillation A Survey\n",
      "%https://link.springer.com/article/10.1007/s11263-021-01453-z\n",
      "@article{gou2021knowledge,\n",
      "title={Knowledge distillation: A survey},\n",
      "author={Gou, Jianping and Yu, Baosheng and Maybank, Stephen J and Tao, Dacheng},\n",
      "journal={International Journal of Computer Vision},\n",
      "volume={129},\n",
      "pages={1789--1819},\n",
      "year={2021},\n",
      "publisher={Springer}\n",
      "}\n",
      "% Improving Language Understanding by Generative Pre Training\n",
      "%https://link.springer.com/article/10.1007/s11263-021-01453-z\n",
      "@article{radford2018improving,\n",
      "title={Improving language understanding by generative pre-training},\n",
      "author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},\n",
      "year={2018},\n",
      "publisher={OpenAI}\n",
      "}\n",
      "\n",
      "@article{radford2019language,\n",
      "title={Language models are unsupervised multitask learners},\n",
      "author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},\n",
      "journal={OpenAI blog},\n",
      "volume={1},\n",
      "number={8},\n",
      "pages={9},\n",
      "year={2019}\n",
      "}\n",
      "\n",
      "\n",
      "% DistilBERT a distilled version of BERT smaller faster cheaper and lighter\n",
      "%https://arxiv.org/pdf/1910.01108.pdf%3C/p%3E\n",
      "@article{sanh2019distilbert,\n",
      "title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},\n",
      "author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},\n",
      "journal={arXiv preprint arXiv:1910.01108},\n",
      "year={2019}\n",
      "}\n",
      "\n",
      "% BioBert\n",
      "@article{lee2020biobert,\n",
      "title={BioBERT: a pre-trained biomedical language representation model for biomedical text mining},\n",
      "author={Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},\n",
      "journal={Bioinformatics},\n",
      "volume={36},\n",
      "number={4},\n",
      "pages={1234--1240},\n",
      "year={2020},\n",
      "publisher={Oxford University Press}\n",
      "}\n",
      "\n",
      "\n",
      "% Unsupervised Cross lingual Representation Learning at Scale\n",
      "%https://arxiv.org/pdf/1911.02116.pdf\n",
      "@article{conneau2019unsupervised,\n",
      "title={Unsupervised cross-lingual representation learning at scale},\n",
      "author={Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\\'a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},\n",
      "journal={arXiv preprint arXiv:1911.02116},\n",
      "year={2019}\n",
      "}\n",
      "% EarlyBERT Efficient BERT Training via Early bird Lottery Tickets\n",
      "%https://arxiv.org/pdf/2101.00063.pdf\n",
      "@article{chen2020earlybert,\n",
      "title={Earlybert: Efficient bert training via early-bird lottery tickets},\n",
      "author={Chen, Xiaohan and Cheng, Yu and Wang, Shuohang and Gan, Zhe and Wang, Zhangyang and Liu, Jingjing},\n",
      "journal={arXiv preprint arXiv:2101.00063},\n",
      "year={2020}\n",
      "}\n",
      "\n",
      "@book{domingos2015master,\n",
      "title={The master algorithm: How the quest for the ultimate learning machine will remake our world},\n",
      "author={Domingos, Pedro},\n",
      "year={2015},\n",
      "publisher={Basic Books}\n",
      "}\n",
      "\n",
      "@book{minsky2017perceptrons,\n",
      "title={Perceptrons, Reissue of the 1988 Expanded Edition with a new foreword by L{\\'e}on Bottou: An Introduction to Computational Geometry},\n",
      "author={Minsky, Marvin and Papert, Seymour A},\n",
      "year={2017},\n",
      "publisher={MIT press}\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "% CARBON FOOTPRINT\n",
      "\n",
      "@article{anthony_carbontracker_2020,\n",
      "title = {Carbontracker: {Tracking} and {Predicting} the {Carbon} {Footprint} of {Training} {Deep} {Learning} {Models}},\n",
      "copyright = {arXiv.org perpetual, non-exclusive license},\n",
      "shorttitle = {Carbontracker},\n",
      "url = {https://arxiv.org/abs/2007.03051},\n",
      "doi = {10.48550/ARXIV.2007.03051},\n",
      "abstract = {Deep learning (DL) can achieve impressive results across a wide variety of tasks, but this often comes at the cost of training models for extensive periods on specialized hardware accelerators. This energy-intensive workload has seen immense growth in recent years. Machine learning (ML) may become a significant contributor to climate change if this exponential trend continues. If practitioners are aware of their energy and carbon footprint, then they may actively take steps to reduce it whenever possible. In this work, we present Carbontracker, a tool for tracking and predicting the energy and carbon footprint of training DL models. We propose that energy and carbon footprint of model development and training is reported alongside performance metrics using tools like Carbontracker. We hope this will promote responsible computing in ML and encourage research into energy-efficient deep neural networks.},\n",
      "urldate = {2023-06-20},\n",
      "author = {Anthony, Lasse F. Wolff and Kanding, Benjamin and Selvan, Raghavendra},\n",
      "year = {2020},\n",
      "note = {Publisher: arXiv\n",
      "Version Number: 1},\n",
      "keywords = {Computers and Society (cs.CY), FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, Machine Learning (cs.LG), Machine Learning (stat.ML), Signal Processing (eess.SP)},\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "@inproceedings{zhao_greener_2022,\n",
      "address = {Lyon, France},\n",
      "title = {A {Green}(er) {World} for {A}.{I}.},\n",
      "isbn = {978-1-66549-747-3},\n",
      "url = {https://ieeexplore.ieee.org/document/9835179/},\n",
      "doi = {10.1109/IPDPSW55747.2022.00126},\n",
      "urldate = {2023-06-20},\n",
      "booktitle = {2022 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} {Workshops} ({IPDPSW})},\n",
      "publisher = {IEEE},\n",
      "author = {Zhao, Dan and Frey, Nathan C. and McDonald, Joseph and Hubbell, Matthew and Bestor, David and Jones, Michael and Prout, Andrew and Gadepally, Vijay and Samsi, Siddharth},\n",
      "month = {may},\n",
      "year = {2022},\n",
      "pages = {742--750},\n",
      "file = {Versione inviata:C\\:\\\\Users\\\\alice\\\\Zotero\\\\storage\\\\8346ZE8B\\\\Zhao et al. - 2022 - A Green(er) World for A.I..pdf:application/pdf},\n",
      "}\n",
      "\n",
      "@article{patterson2021carbon,\n",
      "title={Carbon emissions and large neural network training},\n",
      "author={Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},\n",
      "journal={arXiv preprint arXiv:2104.10350},\n",
      "year={2021}\n",
      "}\n",
      "\n",
      "@inproceedings{gupta2021chasing,\n",
      "title={Chasing carbon: The elusive environmental footprint of computing},\n",
      "author={Gupta, Udit and Kim, Young Geun and Lee, Sylvia and Tse, Jordan and Lee, Hsien-Hsin S and Wei, Gu-Yeon and Brooks, David and Wu, Carole-Jean},\n",
      "booktitle={2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},\n",
      "pages={854--867},\n",
      "year={2021},\n",
      "organization={IEEE}\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "@article{wu_sustainable_2021,\n",
      "title = {Sustainable {AI}: {Environmental} {Implications}, {Challenges} and {Opportunities}},\n",
      "copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},\n",
      "shorttitle = {Sustainable {AI}},\n",
      "url = {https://arxiv.org/abs/2111.00364},\n",
      "doi = {10.48550/ARXIV.2111.00364},\n",
      "abstract = {This paper explores the environmental impact of the super-linear growth trends for AI from a holistic perspective, spanning Data, Algorithms, and System Hardware. We characterize the carbon footprint of AI computing by examining the model development cycle across industry-scale machine learning use cases and, at the same time, considering the life cycle of system hardware. Taking a step further, we capture the operational and manufacturing carbon footprint of AI computing and present an end-to-end analysis for what and how hardware-software design and at-scale optimization can help reduce the overall carbon footprint of AI. Based on the industry experience and lessons learned, we share the key challenges and chart out important development directions across the many dimensions of AI. We hope the key messages and insights presented in this paper can inspire the community to advance the field of AI in an environmentally-responsible manner.},\n",
      "urldate = {2023-06-20},\n",
      "author = {Wu, Carole-Jean and Raghavendra, Ramya and Gupta, Udit and Acun, Bilge and Ardalani, Newsha and Maeng, Kiwan and Chang, Gloria and Behram, Fiona Aga and Huang, James and Bai, Charles and Gschwind, Michael and Gupta, Anurag and Ott, Myle and Melnikov, Anastasia and Candido, Salvatore and Brooks, David and Chauhan, Geeta and Lee, Benjamin and Lee, Hsien-Hsin S. and Akyildiz, Bugra and Balandat, Maximilian and Spisak, Joe and Jain, Ravi and Rabbat, Mike and Hazelwood, Kim},\n",
      "year = {2021},\n",
      "note = {Publisher: arXiv\n",
      "Version Number: 2},\n",
      "keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, Hardware Architecture (cs.AR), Machine Learning (cs.LG)},\n",
      "}\n",
      "\n",
      "\n",
      "@article{patterson_carbon_2022-1,\n",
      "title = {The {Carbon} {Footprint} of {Machine} {Learning} {Training} {Will} {Plateau}, {Then} {Shrink}},\n",
      "volume = {55},\n",
      "issn = {0018-9162, 1558-0814},\n",
      "url = {https://ieeexplore.ieee.org/document/9810097/},\n",
      "doi = {10.1109/MC.2022.3148714},\n",
      "number = {7},\n",
      "urldate = {2023-06-20},\n",
      "journal = {Computer},\n",
      "author = {Patterson, David and Gonzalez, Joseph and Holzle, Urs and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David R. and Texier, Maud and Dean, Jeff},\n",
      "month = {jul},\n",
      "year = {2022},\n",
      "pages = {18--28},\n",
      "file = {Versione inviata:C\\:\\\\Users\\\\alice\\\\Zotero\\\\storage\\\\YDTBM5BT\\\\Patterson et al. - 2022 - The Carbon Footprint of Machine Learning Training .pdf:application/pdf},\n",
      "}\n",
      "\n",
      "@article{henderson2020towards,\n",
      "title={Towards the systematic reporting of the energy and carbon footprints of machine learning},\n",
      "author={Henderson, Peter and Hu, Jieru and Romoff, Joshua and Brunskill, Emma and Jurafsky, Dan and Pineau, Joelle},\n",
      "journal={The Journal of Machine Learning Research},\n",
      "volume={21},\n",
      "number={1},\n",
      "pages={10039--10081},\n",
      "year={2020},\n",
      "publisher={JMLRORG}\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "@article{perucica_is_2022,\n",
      "title = {Is the future of {AI} sustainable? {A} case study of the {European} {Union}},\n",
      "volume = {16},\n",
      "issn = {1750-6166, 1750-6166},\n",
      "shorttitle = {Is the future of {AI} sustainable?},\n",
      "url = {https://www.emerald.com/insight/content/doi/10.1108/TG-06-2021-0106/full/html},\n",
      "doi = {10.1108/TG-06-2021-0106},\n",
      "abstract = {Purpose\n",
      "The purpose of this paper is to raise awareness on the need for a more comprehensive approach on the interdependence between artificial intelligence (AI) and environmental sustainability. It provides an overview of existing sustainable AI policy initiatives at the national and regional level. More precisely, it discusses whether existing European Union (EU) environmental policies are suitable for the AI era or whether new regulations are needed in this field. Finally, this paper assesses cross-fertilisation opportunities between the EU and non-EU countries.\n",
      "\n",
      "\n",
      "Design/methodology/approach\n",
      "This study is based on a qualitative analysis of sustainable applications of AI and the sustainability of AI. Emphasis is laid on the latter, and a “sustainable by design” approach is proposed, which in essence is a prerequisite for transparent, responsible and human-centred AI systems. The analysis primarily focuses on environmental sustainability.\n",
      "\n",
      "\n",
      "Findings\n",
      "The majority of studies focus on how to use AI to protect the environment with very little attention paid to sustainable design of AI. On the other hand, the EU’s comprehensive approach towards sustainable AI is closest to promoting “sustainable by design” AI. Several ways have been identified in which the EU’s actions can be translated beyond its borders.\n",
      "\n",
      "\n",
      "Research limitations/implications\n",
      "One of the largest limitations of this study is its moderate scope. This paper is confined to the EU and as such provides a limited assessment of global policies and measures on the interplay between sustainability and AI. Consequently, the paper did not provide an in-depth analysis of environmental policies worldwide that could help provide a better picture of possible cooperation areas or common grounds. Another limitation of this study is that it primarily focuses on environmental aspects and as such accords little attention to the economic and social pillars of sustainability.\n",
      "\n",
      "\n",
      "Social implications\n",
      "With less than 10 years to go before reaching the sustainable development goal deadline, this study can help stakeholders better understand what is being done worldwide in terms of sustainable AI. Moreover, given that the technology is still in its early phase, this study can inspire a “sustainable by design” approach to the development of AI technologies.\n",
      "\n",
      "\n",
      "Originality/value\n",
      "All national AI strategies published by 1 June 2021 were analysed to identify whether and to what extent they prioritise the interplay between environment and AI. Furthermore, the authors also looked at the EU policy and how it aims to address AI from a sustainable perspective.},\n",
      "language = {en},\n",
      "number = {3},\n",
      "urldate = {2023-06-25},\n",
      "journal = {Transforming Government: People, Process and Policy},\n",
      "author = {Perucica, Natasa and Andjelkovic, Katarina},\n",
      "month = jul,\n",
      "year = {2022},\n",
      "pages = {347--358},\n",
      "file = {Full text:C\\:\\\\Users\\\\alice\\\\Zotero\\\\storage\\\\DQ7WXAFN\\\\Perucica e Andjelkovic - 2022 - Is the future of AI sustainable A case study of t.pdf:application/pdf},\n",
      "}\n",
      "\n",
      "@techreport{noauthor_online_2023,\n",
      "type = {{OECD} {Digital} {Economy} {Papers}},\n",
      "title = {Online product safety sweep report},\n",
      "url = {https://www.oecd-ilibrary.org/science-and-technology/online-product-safety-sweep-report_c1faa51e-en},\n",
      "language = {en},\n",
      "number = {354},\n",
      "urldate = {2023-06-25},\n",
      "month = jun,\n",
      "year = {2023},\n",
      "doi = {10.1787/c1faa51e-en},\n",
      "note = {Series: OECD Digital Economy Papers\n",
      "Volume: 354},\n",
      "}\n",
      "\n",
      "@article{piorkowski_quantitative_2022,\n",
      "title = {Quantitative {AI} {Risk} {Assessments}: {Opportunities} and {Challenges}},\n",
      "copyright = {arXiv.org perpetual, non-exclusive license},\n",
      "shorttitle = {Quantitative {AI} {Risk} {Assessments}},\n",
      "url = {https://arxiv.org/abs/2209.06317},\n",
      "doi = {10.48550/ARXIV.2209.06317},\n",
      "abstract = {Although AI-based systems are increasingly being leveraged to provide value to organizations, individuals, and society, significant attendant risks have been identified. These risks have led to proposed regulations, litigation, and general societal concerns. As with any promising technology, organizations want to benefit from the positive capabilities of AI technology while reducing the risks. The best way to reduce risks is to implement comprehensive AI lifecycle governance where policies and procedures are described and enforced during the design, development, deployment, and monitoring of an AI system. While support for comprehensive governance is beginning to emerge, organizations often need to identify the risks of deploying an already-built model without knowledge of how it was constructed or access to its original developers. Such an assessment will quantitatively assess the risks of an existing model in a manner analogous to how a home inspector might assess the energy efficiency of an already-built home or a physician might assess overall patient health based on a battery of tests. This paper explores the concept of a quantitative AI Risk Assessment, exploring the opportunities, challenges, and potential impacts of such an approach, and discussing how it might improve AI regulations.},\n",
      "urldate = {2023-06-25},\n",
      "author = {Piorkowski, David and Hind, Michael and Richards, John},\n",
      "year = {2022},\n",
      "note = {Publisher: arXiv\n",
      "Version Number: 2},\n",
      "keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences},\n",
      "}\n",
      "\n",
      "@article{rohde_sustainability_2021,\n",
      "title = {Sustainability challenges of {Artificial} {Intelligence} and {Policy} {Implications}},\n",
      "volume = {36},\n",
      "issn = {1430-8800},\n",
      "url = {http://oekologisches-wirtschaften.de/index.php/oew/article/view/1792},\n",
      "doi = {10.14512/OEWO360136},\n",
      "abstract = {Automated decision-making based on Artificial Intelligence is associated with growing expectations and is to contribute to sustainable development goals. Which opportunities and risks for the environment, economy and society are associated with Artificial Intelligence-based applications and how can they be governed?},\n",
      "number = {O1},\n",
      "urldate = {2023-06-25},\n",
      "journal = {Ökologisches Wirtschaften - Fachzeitschrift},\n",
      "author = {Rohde, Friederike and Gossen, Maike and Wagner, Josephin and Santarius, Tilman},\n",
      "month = feb,\n",
      "year = {2021},\n",
      "pages = {36--40},\n",
      "file = {Full text:C\\:\\\\Users\\\\alice\\\\Zotero\\\\storage\\\\KMVT7E5M\\\\Rohde et al. - 2021 - Sustainability challenges of Artificial Intelligen.pdf:application/pdf},\n",
      "}\n",
      "\n",
      "@article{van_wynsberghe_sustainable_2021,\n",
      "title = {Sustainable {AI}: {AI} for sustainability and the sustainability of {AI}},\n",
      "volume = {1},\n",
      "issn = {2730-5953, 2730-5961},\n",
      "shorttitle = {Sustainable {AI}},\n",
      "url = {https://link.springer.com/10.1007/s43681-021-00043-6},\n",
      "doi = {10.1007/s43681-021-00043-6},\n",
      "abstract = {Abstract\n",
      "\n",
      "While there is a growing effort towards AI\n",
      "for\n",
      "Sustainability (e.g. towards the sustainable development goals) it is time to move beyond that and to address the sustainability\n",
      "of\n",
      "developing and using AI systems. In this paper I propose a definition of Sustainable AI; Sustainable AI is a movement to foster change in the entire lifecycle of AI products (i.e. idea generation, training, re-tuning, implementation, governance) towards greater ecological integrity and social justice. As such, Sustainable AI is focused on more than AI applications; rather, it addresses the whole sociotechnical system of AI. I have suggested here that Sustainable AI is not about how to sustain the development of AI per say but it is about how to develop AI that is compatible with sustaining environmental resources for current and future generations; economic models for societies; and societal values that are fundamental to a given society. I have articulated that the phrase Sustainable AI be understood as having two branches; AI\n",
      "for\n",
      "sustainability and sustainability\n",
      "of\n",
      "AI (e.g. reduction of carbon emissions and computing power). I propose that Sustainable AI take sustainable development at the core of its definition with three accompanying tensions between AI innovation and equitable resource distribution; inter and intra-generational justice; and, between environment, society, and economy. This paper is not meant to engage with each of the three pillars of sustainability (i.e. social, economic, environment), and as such the pillars of sustainable AI. Rather, this paper is meant to inspire the reader, the policy maker, the AI ethicist, the AI developer to connect with the environment—to remember that there are environmental costs to AI. Further, to direct funding towards sustainable methods\n",
      "of\n",
      "AI.},\n",
      "language = {en},\n",
      "number = {3},\n",
      "urldate = {2023-06-25},\n",
      "journal = {AI and Ethics},\n",
      "author = {Van Wynsberghe, Aimee},\n",
      "month = aug,\n",
      "year = {2021},\n",
      "pages = {213--218},\n",
      "file = {Full text:C\\:\\\\Users\\\\alice\\\\Zotero\\\\storage\\\\M4JCFC97\\\\Van Wynsberghe - 2021 - Sustainable AI AI for sustainability and the sust.pdf:application/pdf},\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "@article{strubell_energy_2020,\n",
      "title = {Energy and {Policy} {Considerations} for {Modern} {Deep} {Learning} {Research}},\n",
      "volume = {34},\n",
      "issn = {2374-3468, 2159-5399},\n",
      "url = {https://ojs.aaai.org/index.php/AAAI/article/view/7123},\n",
      "doi = {10.1609/aaai.v34i09.7123},\n",
      "abstract = {The field of artificial intelligence has experienced a dramatic methodological shift towards large neural networks trained on plentiful data. This shift has been fueled by recent advances in hardware and techniques enabling remarkable levels of computation, resulting in impressive advances in AI across many applications. However, the massive computation required to obtain these exciting results is costly both financially, due to the price of specialized hardware and electricity or cloud compute time, and to the environment, as a result of non-renewable energy used to fuel modern tensor processing hardware. In a paper published this year at ACL, we brought this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training and tuning neural network models for NLP (Strubell, Ganesh, and McCallum 2019). In this extended abstract, we briefly summarize our findings in NLP, incorporating updated estimates and broader information from recent related publications, and provide actionable recommendations to reduce costs and improve equity in the machine learning and artificial intelligence community.},\n",
      "number = {09},\n",
      "urldate = {2023-06-25},\n",
      "journal = {Proceedings of the AAAI Conference on Artificial Intelligence},\n",
      "author = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},\n",
      "month = apr,\n",
      "year = {2020},\n",
      "pages = {13693--13696},\n",
      "file = {Full text:C\\:\\\\Users\\\\alice\\\\Zotero\\\\storage\\\\NJWF93XC\\\\Strubell et al. - 2020 - Energy and Policy Considerations for Modern Deep L.pdf:application/pdf},\n",
      "}\n",
      "\n",
      "\n",
      "@article{lannelongue_green_2021,\n",
      "title = {Green {Algorithms}: {Quantifying} the {Carbon} {Footprint} of {Computation}},\n",
      "volume = {8},\n",
      "issn = {2198-3844, 2198-3844},\n",
      "shorttitle = {Green {Algorithms}},\n",
      "url = {https://onlinelibrary.wiley.com/doi/10.1002/advs.202100707},\n",
      "doi = {10.1002/advs.202100707},\n",
      "language = {en},\n",
      "number = {12},\n",
      "urldate = {2023-06-25},\n",
      "journal = {Advanced Science},\n",
      "author = {Lannelongue, Loïc and Grealey, Jason and Inouye, Michael},\n",
      "month = jun,\n",
      "year = {2021},\n",
      "pages = {2100707},\n",
      "file = {Full text:C\\:\\\\Users\\\\alice\\\\Zotero\\\\storage\\\\WPD7NS96\\\\Lannelongue et al. - 2021 - Green Algorithms Quantifying the Carbon Footprint.pdf:application/pdf},\n",
      "}\n",
      "\n",
      "@article{lacoste_quantifying_2019,\n",
      "title = {Quantifying the {Carbon} {Emissions} of {Machine} {Learning}},\n",
      "copyright = {Creative Commons Attribution Share Alike 4.0 International},\n",
      "url = {https://arxiv.org/abs/1910.09700},\n",
      "doi = {10.48550/ARXIV.1910.09700},\n",
      "abstract = {From an environmental standpoint, there are a few crucial aspects of training a neural network that have a major impact on the quantity of carbon that it emits. These factors include: the location of the server used for training and the energy grid that it uses, the length of the training procedure, and even the make and model of hardware on which the training takes place. In order to approximate these emissions, we present our Machine Learning Emissions Calculator, a tool for our community to better understand the environmental impact of training ML models. We accompany this tool with an explanation of the factors cited above, as well as concrete actions that individual practitioners and organizations can take to mitigate their carbon emissions.},\n",
      "urldate = {2023-06-25},\n",
      "author = {Lacoste, Alexandre and Luccioni, Alexandra and Schmidt, Victor and Dandres, Thomas},\n",
      "year = {2019},\n",
      "note = {Publisher: arXiv\n",
      "Version Number: 2},\n",
      "keywords = {Computers and Society (cs.CY), FOS: Computer and information sciences, Machine Learning (cs.LG)},\n",
      "}\n",
      "\n",
      "@article{barbierato2023multiformalism,\n",
      "title={A Multiformalism-Based Model for Performance Evaluation of Green Data Centres},\n",
      "author={Barbierato, Enrico and Manini, Daniele and Gribaudo, Marco},\n",
      "journal={Electronics},\n",
      "volume={12},\n",
      "number={10},\n",
      "pages={2169},\n",
      "year={2023},\n",
      "publisher={MDPI}\n",
      "}\n",
      "\n",
      "@misc{shaikh2021energyvis,\n",
      "title={EnergyVis: Interactively Tracking and Exploring Energy Consumption for ML Models}, \n",
      "author={Omar Shaikh and Jon Saad-Falcon and Austin P Wright and Nilaksh Das and Scott Freitas and Omar Isaac Asensio and Duen Horng Chau},\n",
      "year={2021},\n",
      "eprint={2103.16435},\n",
      "archivePrefix={arXiv},\n",
      "primaryClass={cs.LG}\n",
      "}\n",
      "\n",
      "@article{frankle2018lottery,\n",
      "title={The lottery ticket hypothesis: Finding sparse, trainable neural networks},\n",
      "author={Frankle, Jonathan and Carbin, Michael},\n",
      "journal={arXiv preprint arXiv:1803.03635},\n",
      "year={2018}\n",
      "}\n",
      "\n",
      "@article{verdecchia_systematic_2023,\n",
      "title = {A {Systematic} {Review} of {Green} {AI}},\n",
      "copyright = {arXiv.org perpetual, non-exclusive license},\n",
      "url = {https://arxiv.org/abs/2301.11047},\n",
      "doi = {10.48550/ARXIV.2301.11047},\n",
      "abstract = {With the ever-growing adoption of AI-based systems, the carbon footprint of AI is no longer negligible. AI researchers and practitioners are therefore urged to hold themselves accountable for the carbon emissions of the AI models they design and use. This led in recent years to the appearance of researches tackling AI environmental sustainability, a field referred to as Green AI. Despite the rapid growth of interest in the topic, a comprehensive overview of Green AI research is to date still missing. To address this gap, in this paper, we present a systematic review of the Green AI literature. From the analysis of 98 primary studies, different patterns emerge. The topic experienced a considerable growth from 2020 onward. Most studies consider monitoring AI model footprint, tuning hyperparameters to improve model sustainability, or benchmarking models. A mix of position papers, observational studies, and solution papers are present. Most papers focus on the training phase, are algorithm-agnostic or study neural networks, and use image data. Laboratory experiments are the most common research strategy. Reported Green AI energy savings go up to 115\\%, with savings over 50\\% being rather common. Industrial parties are involved in Green AI studies, albeit most target academic readers. Green AI tool provisioning is scarce. As a conclusion, the Green AI research field results to have reached a considerable level of maturity. Therefore, from this review emerges that the time is suitable to adopt other Green AI research strategies, and port the numerous promising academic results to industrial practice.},\n",
      "urldate = {2023-06-28},\n",
      "author = {Verdecchia, Roberto and Sallou, June and Cruz, Luís},\n",
      "year = {2023},\n",
      "note = {Publisher: arXiv\n",
      "Version Number: 3},\n",
      "keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences},\n",
      "}\n",
      "\n",
      "@inproceedings{manotas_empirical_2016,\n",
      "address = {Austin Texas},\n",
      "title = {An empirical study of practitioners' perspectives on green software engineering},\n",
      "isbn = {978-1-4503-3900-1},\n",
      "url = {https://dl.acm.org/doi/10.1145/2884781.2884810},\n",
      "doi = {10.1145/2884781.2884810},\n",
      "language = {en},\n",
      "urldate = {2023-06-28},\n",
      "booktitle = {Proceedings of the 38th {International} {Conference} on {Software} {Engineering}},\n",
      "publisher = {ACM},\n",
      "author = {Manotas, Irene and Bird, Christian and Zhang, Rui and Shepherd, David and Jaspan, Ciera and Sadowski, Caitlin and Pollock, Lori and Clause, James},\n",
      "month = may,\n",
      "year = {2016},\n",
      "pages = {237--248},\n",
      "file = {Full text:C\\:\\\\Users\\\\alice\\\\Zotero\\\\storage\\\\SEYJN25E\\\\Manotas et al. - 2016 - An empirical study of practitioners' perspectives .pdf:application/pdf},\n",
      "}\n",
      "\n",
      "@inproceedings{verdecchia_data-centric_2022,\n",
      "address = {Plovdiv, Bulgaria},\n",
      "title = {Data-{Centric} {Green} {AI} {An} {Exploratory} {Empirical} {Study}},\n",
      "isbn = {978-1-66548-286-8},\n",
      "url = {https://ieeexplore.ieee.org/document/9830097/},\n",
      "doi = {10.1109/ICT4S55073.2022.00015},\n",
      "urldate = {2023-06-28},\n",
      "booktitle = {2022 {International} {Conference} on {ICT} for {Sustainability} ({ICT4S})},\n",
      "publisher = {IEEE},\n",
      "author = {Verdecchia, Roberto and Cruz, Luis and Sallou, June and Lin, Michelle and Wickenden, James and Hotellier, Estelle},\n",
      "month = jun,\n",
      "year = {2022},\n",
      "pages = {35--45},\n",
      "file = {Full text:C\\:\\\\Users\\\\alice\\\\Zotero\\\\storage\\\\KR37VN3E\\\\Verdecchia et al. - 2022 - Data-Centric Green AI An Exploratory Empirical Stu.pdf:application/pdf},\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "@article{vinuesa_role_2020,\n",
      "title = {The role of artificial intelligence in achieving the {Sustainable} {Development} {Goals}},\n",
      "volume = {11},\n",
      "issn = {2041-1723},\n",
      "url = {https://www.nature.com/articles/s41467-019-14108-y},\n",
      "doi = {10.1038/s41467-019-14108-y},\n",
      "abstract = {Abstract\n",
      "The emergence of artificial intelligence (AI) and its progressively wider impact on many sectors requires an assessment of its effect on the achievement of the Sustainable Development Goals. Using a consensus-based expert elicitation process, we find that AI can enable the accomplishment of 134 targets across all the goals, but it may also inhibit 59 targets. However, current research foci overlook important aspects. The fast development of AI needs to be supported by the necessary regulatory insight and oversight for AI-based technologies to enable sustainable development. Failure to do so could result in gaps in transparency, safety, and ethical standards.},\n",
      "language = {en},\n",
      "number = {1},\n",
      "urldate = {2023-06-28},\n",
      "journal = {Nature Communications},\n",
      "author = {Vinuesa, Ricardo and Azizpour, Hossein and Leite, Iolanda and Balaam, Madeline and Dignum, Virginia and Domisch, Sami and Felländer, Anna and Langhans, Simone Daniela and Tegmark, Max and Fuso Nerini, Francesco},\n",
      "month = jan,\n",
      "year = {2020},\n",
      "pages = {233},\n",
      "file = {Full text:C\\:\\\\Users\\\\alice\\\\Zotero\\\\storage\\\\QDT5YBMJ\\\\Vinuesa et al. - 2020 - The role of artificial intelligence in achieving t.pdf:application/pdf},\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def read_bib_file(bib_file_path):\n",
    "    with open(bib_file_path, 'r', encoding='utf-8') as bib_file:\n",
    "        content = bib_file.read()\n",
    "    return content\n",
    "\n",
    "bib_file_path = 'biblio.bib'\n",
    "bib_content = read_bib_file(bib_file_path)\n",
    "\n",
    "print(bib_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    if (\"  \" in bib_content) or (\"\\t\" in bib_content):\n",
    "        bib_content = bib_content.replace(\"  \", \" \")\n",
    "        bib_content = bib_content.replace(\"\\t\", \"\")\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'% First Draft of a Report on the EDVAC\\n% DOI da IEEE del 1993: https://ieeexplore.ieee.org/document/238389\\n@article{von_neumann_first_1993,\\ntitle = {First draft of a report on the {EDVAC}},\\nvolume = {15},\\nissn = {1058-6180},\\nurl = {http://ieeexplore.ieee.org/document/238389/},\\ndoi = {10.1109/85.238389},\\nnumber = {4},\\nurldate = {2023-02-16},\\njournal = {IEEE Annals of the History of Computing},\\nauthor = {von Neumann, J.},\\nyear = {1993},\\npages = {27--75},\\n}\\n\\n@article{talati2020mmpu,\\ntitle={mmpu—a real processing-in-memory architecture to combat the von neumann bottleneck},\\nauthor={Talati, Nishil and Ben-Hur, Rotem and Wald, Nimrod and Haj-Ali, Ameer and Reuben, John and Kvatinsky, Shahar},\\njournal={Applications of Emerging Memory Technology: Beyond Storage},\\npages={191--213},\\nyear={2020},\\npublisher={Springer}\\n}\\n\\n% Schwartz Green AI (primissimo articolo su green ai, 2019)\\n% nell\\'introduzione\\n@misc{schwartz_green_2019,\\ntitle = {Green {AI}},\\nurl = {http://arxiv.org/abs/1907.10597},\\nabstract = {The computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018 [2]. These computations have a surprisingly large carbon footprint [38]. Ironically, deep learning was inspired by the human brain, which is remarkably energy efficient. Moreover, the financial cost of the computations can make it difficult for academics, students, and researchers, in particular those from emerging economies, to engage in deep learning research. This position paper advocates a practical solution by making efficiency an evaluation criterion for research alongside accuracy and related measures. In addition, we propose reporting the financial cost or \"price tag\" of developing, training, and running models to provide baselines for the investigation of increasingly efficient methods. Our goal is to make AI both greener and more inclusive---enabling any inspired undergraduate with a laptop to write high-quality research papers. Green AI is an emerging focus at the Allen Institute for AI.},\\nurldate = {2023-02-08},\\npublisher = {arXiv},\\nauthor = {Schwartz, Roy and Dodge, Jesse and Smith, Noah A. and Etzioni, Oren},\\nmonth = {aug},\\nyear = {2019},\\nnote = {arXiv:1907.10597 [cs, stat]},\\nkeywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computers and Society, Computer Science - Machine Learning, Statistics - Methodology},\\n} \\n\\n\\n% Schwartz Green AI 2020 (trovato 8-9/2)\\n% nell\\'introduzione\\n@article{schwartz_green_2020,\\ntitle = {Green {AI}},\\nvolume = {63},\\nissn = {0001-0782, 1557-7317},\\nurl = {https://dl.acm.org/doi/10.1145/3381831},\\ndoi = {10.1145/3381831},\\nabstract = {Creating efficiency in AI research will decrease its carbon footprint and increase its inclusivity as deep learning study should not require the deepest pockets.},\\nlanguage = {en},\\nnumber = {12},\\nurldate = {2023-02-08},\\njournal = {Communications of the ACM},\\nauthor = {Schwartz, Roy and Dodge, Jesse and Smith, Noah A. and Etzioni, Oren},\\nmonth = {nov},\\nyear = {2020},\\npages = {54--63},\\nfile = {Testo integrale:C\\\\:\\\\\\\\Users\\\\\\\\alice\\\\\\\\Zotero\\\\\\\\storage\\\\\\\\N4U29MDB\\\\\\\\Schwartz et al. - 2020 - Green AI.pdf:application/pdf},\\n}\\n\\n\\n% Energy and Policy Considerations for Deep Learning in NLP\\n% nell\\'introduzione\\n@article{strubell_energy_2019,\\ntitle = {Energy and {Policy} {Considerations} for {Deep} {Learning} in {NLP}},\\ncopyright = {arXiv.org perpetual, non-exclusive license},\\nurl = {https://arxiv.org/abs/1906.02243},\\ndoi = {10.48550/ARXIV.1906.02243},\\nurldate = {2023-02-09},\\nauthor = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},\\nyear = {2019},\\nnote = {Publisher: arXiv Version Number: 1},\\nkeywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},\\n}\\n\\n\\n@article{poerner2020inexpensive,\\ntitle={Inexpensive domain adaptation of pretrained language models: Case studies on biomedical NER and covid-19 QA},\\nauthor={Poerner, Nina and Waltinger, Ulli and Sch{\\\\\"u}tze, Hinrich},\\njournal={arXiv preprint arXiv:2004.03354},\\nyear={2020}\\n}\\n\\n\\n% Articolo corto leggero su usi vari ai (es. agricoltura...), uno dei prmiissimi.\\n% per introduzione\\n@article{pachot_towards_2022,\\ntitle = {Towards {Sustainable} {Artificial} {Intelligence}: {An} {Overview} of {Environmental} {Protection} {Uses} and {Issues}},\\ncopyright = {arXiv.org perpetual, non-exclusive license},\\nshorttitle = {Towards {Sustainable} {Artificial} {Intelligence}},\\nurl = {https://arxiv.org/abs/2212.11738},\\ndoi = {10.48550/ARXIV.2212.11738},\\nabstract = {Artificial Intelligence (AI) is used to create more sustainable production methods and model climate change, making it a valuable tool in the fight against environmental degradation. This paper describes the paradox of an energy-consuming technology serving the ecological challenges of tomorrow. The study provides an overview of the sectors that use AI-based solutions for environmental protection. It draws on numerous examples from AI for Green players to present use cases and concrete examples. In the second part of the study, the negative impacts of AI on the environment and the emerging technological solutions to support Green AI are examined. It is also shown that the research on less energy-consuming AI is motivated more by cost and energy autonomy constraints than by environmental considerations. This leads to a rebound effect that favors an increase in the complexity of models. Finally, the need to integrate environmental indicators into algorithms is discussed. The environmental dimension is part of the broader ethical problem of AI, and addressing it is crucial for ensuring the sustainability of AI in the long term.},\\nurldate = {2023-02-10},\\nauthor = {Pachot, Arnault and Patissier, Céline},\\nyear = {2022},\\nnote = {Publisher: arXiv\\nVersion Number: 1},\\nkeywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences},\\n}\\n\\n% nell\\'introduzione\\n% red ai\\n@article{xu_survey_2021,\\ntitle = {A {Survey} on {Green} {Deep} {Learning}},\\ncopyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},\\nurl = {https://arxiv.org/abs/2111.05193},\\ndoi = {10.48550/ARXIV.2111.05193},\\nabstract = {In recent years, larger and deeper models are springing up and continuously pushing state-of-the-art (SOTA) results across various fields like natural language processing (NLP) and computer vision (CV). However, despite promising results, it needs to be noted that the computations required by SOTA models have been increased at an exponential rate. Massive computations not only have a surprisingly large carbon footprint but also have negative effects on research inclusiveness and deployment on real-world applications. Green deep learning is an increasingly hot research field that appeals to researchers to pay attention to energy usage and carbon emission during model training and inference. The target is to yield novel results with lightweight and efficient technologies. Many technologies can be used to achieve this goal, like model compression and knowledge distillation. This paper focuses on presenting a systematic review of the development of Green deep learning technologies. We classify these approaches into four categories: (1) compact networks, (2) energy-efficient training strategies, (3) energy-efficient inference approaches, and (4) efficient data usage. For each category, we discuss the progress that has been achieved and the unresolved challenges.},\\nurldate = {2023-02-20},\\nauthor = {Xu, Jingjing and Zhou, Wangchunshu and Fu, Zhiyi and Zhou, Hao and Li, Lei},\\nyear = {2021},\\nnote = {Publisher: arXiv\\nVersion Number: 2},\\nkeywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG)},\\n}\\n@article{lin2022survey,\\ntitle={A survey of transformers},\\nauthor={Lin, Tianyang and Wang, Yuxin and Liu, Xiangyang and Qiu, Xipeng},\\njournal={AI Open},\\nyear={2022},\\npublisher={Elsevier}\\n}\\n\\n@article{kirmani2022artificial,\\ntitle={Artificial Intelligence-Enabled Science Poetry},\\nauthor={Kirmani, Ahmad R},\\njournal={ACS Energy Letters},\\nvolume={8},\\npages={574--576},\\nyear={2022},\\npublisher={ACS Publications}\\n}\\n\\n@article{goodfellow2020generative,\\ntitle={Generative adversarial networks},\\nauthor={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},\\njournal={Communications of the ACM},\\nvolume={63},\\nnumber={11},\\npages={139--144},\\nyear={2020},\\npublisher={ACM New York, NY, USA}\\n}\\n\\n@article{brock2018large,\\ntitle={Large scale GAN training for high fidelity natural image synthesis},\\nauthor={Brock, Andrew and Donahue, Jeff and Simonyan, Karen},\\njournal={arXiv preprint arXiv:1809.11096},\\nyear={2018}\\n}\\n\\n@article{surameery2023use,\\ntitle={Use chat gpt to solve programming bugs},\\nauthor={Surameery, Nigar M Shafiq and Shakor, Mohammed Y},\\njournal={International Journal of Information Technology \\\\& Computer Engineering (IJITC) ISSN: 2455-5290},\\nvolume={3},\\nnumber={01},\\npages={17--22},\\nyear={2023}\\n}\\n\\n@article{wu2023brief,\\ntitle={A brief overview of ChatGPT: The history, status quo and potential future development},\\nauthor={Wu, Tianyu and He, Shizhu and Liu, Jingping and Sun, Siqi and Liu, Kang and Han, Qing-Long and Tang, Yang},\\njournal={IEEE/CAA Journal of Automatica Sinica},\\nvolume={10},\\nnumber={5},\\npages={1122--1136},\\nyear={2023},\\npublisher={IEEE}\\n}\\n% nell\\'introduzione\\n% red ai\\n@inproceedings{georgiou_green_2022,\\naddress = {Pittsburgh Pennsylvania},\\ntitle = {Green {AI}: do deep learning frameworks have different costs?},\\nisbn = {978-1-4503-9221-1},\\nshorttitle = {Green {AI}},\\nurl = {https://dl.acm.org/doi/10.1145/3510003.3510221},\\ndoi = {10.1145/3510003.3510221},\\nlanguage = {en},\\nurldate = {2023-02-20},\\nbooktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},\\npublisher = {ACM},\\nauthor = {Georgiou, Stefanos and Kechagia, Maria and Sharma, Tushar and Sarro, Federica and Zou, Ying},\\nmonth = {may},\\nyear = {2022},\\npages = {1082--1094},\\n}\\n\\n% nell\\'introduzione\\n% red ai\\n% bib preso da ieee (copiato da online, non usato zotero)\\n@ARTICLE{9810097,\\nauthor={Patterson, David and Gonzalez, Joseph and Hölzle, Urs and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David R. and Texier, Maud and Dean, Jeff},\\njournal={Computer}, \\ntitle={The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink}, \\nyear={2022},\\nvolume={55},\\nnumber={7},\\npages={18-28},\\ndoi={10.1109/MC.2022.3148714}}\\n\\n% Deep Learning\\n@article{lecun_deep_2015,\\ntitle = {Deep learning},\\nvolume = {521},\\nissn = {0028-0836, 1476-4687},\\nurl = {http://www.nature.com/articles/nature14539},\\ndoi = {10.1038/nature14539},\\nlanguage = {en},\\nnumber = {7553},\\nurldate = {2023-02-28},\\njournal = {Nature},\\nauthor = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},\\nmonth = {may},\\nyear = {2015},\\npages = {436--444},\\n}\\n\\n% Reinforcement learning\\n@article{10.1145/3459991,\\nauthor = {Padakandla, Sindhu},\\ntitle = {A Survey of Reinforcement Learning Algorithms for Dynamically Varying Environments},\\nyear = {2021},\\nissue_date = {July 2022},\\npublisher = {Association for Computing Machinery},\\naddress = {New York, NY, USA},\\nvolume = {54},\\nnumber = {6},\\nissn = {0360-0300},\\nurl = {https://doi.org/10.1145/3459991},\\ndoi = {10.1145/3459991},\\nmonth = {jul},\\narticleno = {127},\\nnumpages = {25},\\nkeywords = {Reinforcement learning, sequential decision-making, context detection, meta-learning, non-stationary environments, regret computation, Markov decision processes}\\n}\\n\\n% GANS\\n\\n@article{wang_generative_2022,\\ntitle = {Generative {Adversarial} {Networks} in {Computer} {Vision}: {A} {Survey} and {Taxonomy}},\\nvolume = {54},\\nissn = {0360-0300, 1557-7341},\\nshorttitle = {Generative {Adversarial} {Networks} in {Computer} {Vision}},\\nurl = {https://dl.acm.org/doi/10.1145/3439723},\\ndoi = {10.1145/3439723},\\nabstract = {Generative adversarial networks (GANs) have been extensively studied in the past few years. Arguably their most significant impact has been in the area of computer vision where great advances have been made in challenges such as plausible image generation, image-to-image translation, facial attribute manipulation, and similar domains. Despite the significant successes achieved to date, applying GANs to real-world problems still poses significant challenges, three of which we focus on here. These are as follows: (1) the generation of high quality images, (2) diversity of image generation, and (3) stabilizing training. Focusing on the degree to which popular GAN technologies have made progress against these challenges, we provide a detailed review of the state-of-the-art in GAN-related research in the published scientific literature. We further structure this review through a convenient taxonomy we have adopted based on variations in GAN architectures and loss functions. While several reviews for GANs have been presented to date, none have considered the status of this field based on their progress toward addressing practical challenges relevant to computer vision. Accordingly, we review and critically discuss the most popular architecture-variant, and loss-variant GANs, for tackling these challenges. Our objective is to provide an overview as well as a critical analysis of the status of GAN research in terms of relevant progress toward critical computer vision application requirements. As we do this we also discuss the most compelling applications in computer vision in which GANs have demonstrated considerable success along with some suggestions for future research directions. Codes related to the GAN-variants studied in this work is summarized on https://github.com/sheqi/GAN\\\\_Review.},\\nlanguage = {en},\\nnumber = {2},\\nurldate = {2023-02-28},\\njournal = {ACM Computing Surveys},\\nauthor = {Wang, Zhengwei and She, Qi and Ward, Tomás E.},\\nmonth = {mar},\\nyear = {2022},\\npages = {1--38},\\nfile = {Full text:C\\\\:\\\\\\\\Users\\\\\\\\Enric\\\\\\\\Zotero\\\\\\\\storage\\\\\\\\E25BXL2D\\\\\\\\Wang et al. - 2022 - Generative Adversarial Networks in Computer Vision.pdf:application/pdf},\\n}\\n\\n% Transfer learning\\n@Article{math10193619,\\nAUTHOR = {Yu, Fuchao and Xiu, Xianchao and Li, Yunhui},\\nTITLE = {A Survey on Deep Transfer Learning and Beyond},\\nJOURNAL = {Mathematics},\\nVOLUME = {10},\\nYEAR = {2022},\\nNUMBER = {19},\\nARTICLE-NUMBER = {3619},\\nURL = {https://www.mdpi.com/2227-7390/10/19/3619},\\nISSN = {2227-7390},\\nABSTRACT = {Deep transfer learning (DTL), which incorporates new ideas from deep neural networks into transfer learning (TL), has achieved excellent success in computer vision, text classification, behavior recognition, and natural language processing. As a branch of machine learning, DTL applies end-to-end learning to overcome the drawback of traditional machine learning that regards each dataset individually. Although some valuable and impressive general surveys exist on TL, special attention and recent advances in DTL are lacking. In this survey, we first review more than 50 representative approaches of DTL in the last decade and systematically summarize them into four categories. In particular, we further divide each category into subcategories according to models, functions, and operation objects. In addition, we discuss recent advances in TL in other fields and unsupervised TL. Finally, we provide some possible and exciting future research directions.},\\nDOI = {10.3390/math10193619}\\n}\\n\\n% AutoML\\n@article{automl,\\nauthor = {He, Xin and Zhao, Kaiyong and Chu, Xiaowen},\\nyear = {2021},\\nmonth = {01},\\npages = {106622},\\ntitle = {AutoML: A survey of the state-of-the-art},\\nvolume = {212},\\njournal = {Knowledge-Based Systems},\\ndoi = {10.1016/j.knosys.2020.106622}\\n}\\n\\n% Von Neumann bottleneck\\n@article{\\nbackus_can_1978,\\ntitle = {Can programming be liberated from the von {Neumann} style?: a functional style and its algebra of programs},\\nvolume = {21},\\nissn = {0001-0782, 1557-7317},\\nshorttitle = {Can programming be liberated from the von {Neumann} style?},\\nurl = {https://dl.acm.org/doi/10.1145/359576.359579},\\ndoi = {10.1145/359576.359579},\\nlanguage = {en},\\nnumber = {8},\\nurldate = {2023-02-28},\\njournal = {Communications of the ACM},\\nauthor = {Backus, John},\\nmonth = {aug},\\nyear = {1978},\\npages = {613--641}\\n}\\n\\n\\n\\n@article{vaswani2017attention,\\ntitle={Attention is all you need},\\nauthor={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\\\\L}ukasz and Polosukhin, Illia},\\njournal={Advances in neural information processing systems},\\nvolume={30},\\nyear={2017}\\n}\\n\\n% Connection machine\\n@article{kahle1989connection,\\ntitle={The connection machine model cm-1 architecture},\\nauthor={Kahle, Brewster A and Hillis, W Daniel},\\njournal={IEEE transactions on systems, man, and cybernetics},\\nvolume={19},\\nnumber={4},\\npages={707--713},\\nyear={1989},\\npublisher={IEEE}\\n}\\n\\n% PCA\\n@article{pearson_liii_1901,\\ntitle = {{LIII}. \\\\textit{{On} lines and planes of closest fit to systems of points in space}},\\nvolume = {2},\\nissn = {1941-5982, 1941-5990},\\nurl = {https://www.tandfonline.com/doi/full/10.1080/14786440109462720},\\ndoi = {10.1080/14786440109462720},\\nlanguage = {en},\\nnumber = {11},\\nurldate = {2023-02-28},\\njournal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},\\nauthor = {Pearson, Karl},\\nmonth = {nov},\\nyear = {1901},\\npages = {559--572},\\nfile = {Versione inviata:C\\\\:\\\\\\\\Users\\\\\\\\Enric\\\\\\\\Zotero\\\\\\\\storage\\\\\\\\RM7USJ9H\\\\\\\\Pearson - 1901 - LIII. On lines and planes of closest fit to sys.pdf:application/pdf},\\n}\\n% autoencoders\\n@article{kramer_nonlinear_1991,\\ntitle = {Nonlinear principal component analysis using autoassociative neural networks},\\nvolume = {37},\\nissn = {0001-1541, 1547-5905},\\nurl = {https://onlinelibrary.wiley.com/doi/10.1002/aic.690370209},\\ndoi = {10.1002/aic.690370209},\\nlanguage = {en},\\nnumber = {2},\\nurldate = {2023-02-28},\\njournal = {AIChE Journal},\\nauthor = {Kramer, Mark A.},\\nmonth = {feb},\\nyear = {1991},\\npages = {233--243},\\n}\\n\\n% CNN\\n@article{CNN,\\nauthor = {Hasegawa, Akira and Lo, Shih-Chung and Lin, Jyh-Shyan and Freedman, Matthew and Mun, Seong},\\nyear = {1998},\\nmonth = {04},\\npages = {241-250},\\ntitle = {A Shift-Invariant Neural Network for the Lung Field Segmentation in Chest Radiography},\\nvolume = {18},\\njournal = {VLSI Signal Processing},\\ndoi = {10.1023/A:1007937214367}\\n}\\n\\n% RNN\\n@article{RNN,\\ntitle = {A {Novel} {Connectionist} {System} for {Unconstrained} {Handwriting} {Recognition}},\\nvolume = {31},\\nissn = {0162-8828},\\nurl = {http://ieeexplore.ieee.org/document/4531750/},\\ndoi = {10.1109/TPAMI.2008.137},\\nnumber = {5},\\nurldate = {2023-02-28},\\njournal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\\nauthor = {Graves, A. and Liwicki, M. and Fernandez, S. and Bertolami, R. and Bunke, H. and Schmidhuber, J.},\\nmonth = {may},\\nyear = {2009},\\npages = {855--868},\\nfile = {Versione inviata:C\\\\:\\\\\\\\Users\\\\\\\\Enric\\\\\\\\Zotero\\\\\\\\storage\\\\\\\\FDNR7EB8\\\\\\\\Graves et al. - 2009 - A Novel Connectionist System for Unconstrained Han.pdf:application/pdf},\\n}\\n\\n\\n@article{LSTM,\\ntitle = {Long {Short}-{Term} {Memory}},\\nvolume = {9},\\nissn = {0899-7667, 1530-888X},\\nurl = {https://direct.mit.edu/neco/article/9/8/1735-1780/6109},\\ndoi = {10.1162/neco.1997.9.8.1735},\\nabstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter\\'s (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},\\nlanguage = {en},\\nnumber = {8},\\nurldate = {2023-02-28},\\njournal = {Neural Computation},\\nauthor = {Hochreiter, Sepp and Schmidhuber, Jürgen},\\nmonth = {nov},\\nyear = {1997},\\npages = {1735--1780},\\n}\\n% Definizione di \"carbon footprint\"\\n@article{wiedmann2008definition,\\ntitle={A definition of ‘carbon footprint’},\\nauthor={Wiedmann, Thomas and Minx, Jan},\\njournal={Ecological economics research trends},\\nvolume={1},\\nnumber={2008},\\npages={1--11},\\nyear={2008},\\npublisher={Nova Science Publishers Hauppauge, NY}\\n}\\n\\n% Articolo che descrive curse of dimensionality. Bellman 1962\\n@article{hammer_adaptive_1962,\\ntitle = {Adaptive {Control} {Processes}: {A} {Guided} {Tour} ({R}. {Bellman})},\\nvolume = {4},\\nissn = {0036-1445, 1095-7200},\\nshorttitle = {Adaptive {Control} {Processes}},\\nurl = {http://epubs.siam.org/doi/10.1137/1004050},\\ndoi = {10.1137/1004050},\\nlanguage = {en},\\nnumber = {2},\\nurldate = {2023-03-20},\\njournal = {SIAM Review},\\nauthor = {Hammer, P. C.},\\nmonth = {apr},\\nyear = {1962},\\npages = {163--163},\\n}\\n\\n\\n\\n\\n@article{noauthor_measuring_2022,\\nauthor = {OECD},\\ntitle = {Measuring the environmental impacts of artificial intelligence compute and applications},\\nyear = {2022},\\nnumber = {341},\\nurl = {https://www.oecd-ilibrary.org/content/paper/7babf571-en},\\ndoi = {https://doi.org/https://doi.org/10.1787/7babf571-en}\\n}\\n\\n\\n% Operational energy costs\\n@article{crawford2018anatomy,\\ntitle={Anatomy of an AI System},\\nauthor={Crawford, Kate and Joler, Vladan},\\njournal={Retrieved September},\\nvolume={18},\\npages={2018},\\nyear={2018}\\n}\\n\\n% BERT\\n@Article{Devlin2018_bert,\\nauthor = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},\\ntitle = {Bert: Pre-training of deep bidirectional transformers for language understanding},\\njournal = {arXiv preprint arXiv:1810.04805},\\nyear = {2018},\\n}\\n\\n\\n% History of NLP\\n@Inbook{Jones1994,\\nauthor={Jones, Karen Sparck},\\ntitle={Natural Language Processing: A Historical Review},\\nbookTitle={Current Issues in Computational Linguistics: In Honour of Don Walker},\\nyear={1994},\\npublisher={Springer Netherlands},\\naddress={Dordrecht},\\npages={3--16}\\n}\\n\\n% Shank (MT)\\n@incollection{Schank75,\\nAUTHOR = {R. C. Schank},\\nTITLE = {Conceptual Dependency Theory},\\nYEAR = {1975},\\nBOOKTITLE = {Conceptual Information Processing},\\nEDITOR = {R. C. Schank},\\nPUBLISHER = {North-Holland and Elsevier},\\nADDRESS = {Amsterdam and New York},\\nPAGES = {22-82}, \\nKEYWORDS = {},\\nANNOTE ={ The book presents the Conceptual Dependency Theory which can be used by a machine for conceptual understanding of the given text.}\\n}\\n\\n@inproceedings{johri2021natural,\\ntitle={Natural language processing: History, evolution, application, and future work},\\nauthor={Johri, Prashant and Khatri, Sunil K and Al-Taani, Ahmad T and Sabharwal, Munish and Suvanov, Shakhzod and Kumar, Avneesh},\\nbooktitle={Proceedings of 3rd International Conference on Computing Informatics and Networks: ICCIN 2020},\\npages={365--375},\\nyear={2021},\\norganization={Springer}\\n}\\n\\n% XLNet\\n@misc{https://doi.org/10.48550/arxiv.1906.08237,\\ndoi = {10.48550/ARXIV.1906.08237},\\nurl = {https://arxiv.org/abs/1906.08237},\\nauthor = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},\\nkeywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},\\ntitle = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},\\npublisher = {arXiv},\\nyear = {2019},\\ncopyright = {Creative Commons Attribution 4.0 International}\\n}\\n\\n@article{raffel2020exploring,\\ntitle={Exploring the limits of transfer learning with a unified text-to-text transformer},\\nauthor={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},\\njournal={The Journal of Machine Learning Research},\\nvolume={21},\\nnumber={1},\\npages={5485--5551},\\nyear={2020},\\npublisher={JMLRORG}\\n}\\n\\n% Come funziona BERT\\n@article{rogers2021primer,\\ntitle={A primer in BERTology: What we know about how BERT works},\\nauthor={Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},\\njournal={Transactions of the Association for Computational Linguistics},\\nvolume={8},\\npages={842--866},\\nyear={2021},\\npublisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}\\n}\\n\\n% Proposta di training HW per BERT\\n@misc{https://doi.org/10.48550/arxiv.2008.00177,\\ndoi = {10.48550/ARXIV.2008.00177},\\nurl = {https://arxiv.org/abs/2008.00177},\\nauthor = {Lin, Jiahuang and Li, Xin and Pekhimenko, Gennady},\\ntitle = {Multi-node Bert-pretraining: Cost-efficient Approach},\\npublisher = {arXiv},\\nyear = {2020},\\ncopyright = {arXiv.org perpetual, non-exclusive license}\\n}\\n\\n\\n%%%%%%%%%%%%%%%%%%%%%%%\\n% APPROACHES\\n%%%%%%%%%%%%%%%%%%%%%%\\n% Miglioramento training di BERT\\n@misc{https://doi.org/10.48550/arxiv.2101.00063,\\ndoi = {10.48550/ARXIV.2101.00063},\\nurl = {https://arxiv.org/abs/2101.00063},\\nauthor = {Chen, Xiaohan and Cheng, Yu and Wang, Shuohang and Gan, Zhe and Wang, Zhangyang and Liu, Jingjing},\\nkeywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},\\ntitle = {EarlyBERT: Efficient BERT Training via Early-bird Lottery Tickets},\\npublisher = {arXiv},\\nyear = {2021},\\ncopyright = {arXiv.org perpetual, non-exclusive license}\\n}\\n\\n\\n% Approaches. Algorithms\\n% Parallel training\\n@article{https://doi.org/10.48550/arxiv.1811.03600,\\ndoi = {10.48550/ARXIV.1811.03600},\\nurl = {https://arxiv.org/abs/1811.03600},\\nauthor = {Shallue, Christopher J. and Lee, Jaehoon and Antognini, Joseph and Sohl-Dickstein, Jascha and Frostig, Roy and Dahl, George E.},\\nkeywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},\\ntitle = {Measuring the Effects of Data Parallelism on Neural Network Training},\\npublisher = {arXiv},\\nyear = {2018},\\ncopyright = {arXiv.org perpetual, non-exclusive license}\\n}\\n% Parallel model\\n@article{DBLP:journals/corr/Krizhevsky14,\\nauthor = {Alex Krizhevsky},\\ntitle = {One weird trick for parallelizing convolutional neural networks},\\njournal = {CoRR},\\nvolume = {abs/1404.5997},\\nyear = {2014},\\nurl = {http://arxiv.org/abs/1404.5997},\\neprinttype = {arXiv},\\neprint = {1404.5997},\\ntimestamp = {Mon, 13 Aug 2018 16:48:41 +0200},\\nbiburl = {https://dblp.org/rec/journals/corr/Krizhevsky14.bib},\\nbibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n\\n\\n% data structures - tensor\\n@article{lewis2021large,\\ntitle={Large scale distributed linear algebra with tensor processing units},\\nauthor={Lewis, Adam GM and Beall, Jackson and Ganahl, Martin and Hauru, Markus and Mallick, Shrestha Basu and Vidal, Guifre},\\njournal={arXiv preprint arXiv:2112.09017},\\nyear={2021}\\n}\\n\\n@article{ji_survey_2019,\\ntitle = {A {Survey} on {Tensor} {Techniques} and {Applications} in {Machine} {Learning}},\\nvolume = {7},\\nissn = {2169-3536},\\nurl = {https://ieeexplore.ieee.org/document/8884203/},\\ndoi = {10.1109/ACCESS.2019.2949814},\\nurldate = {2023-02-26},\\njournal = {IEEE Access},\\nauthor = {Ji, Yuwang and Wang, Qiang and Li, Xuan and Liu, Jie},\\nyear = {2019},\\npages = {162950--162990},\\nfile = {Full text:C\\\\:\\\\\\\\Users\\\\\\\\alice\\\\\\\\Zotero\\\\\\\\storage\\\\\\\\JPVL3QXL\\\\\\\\Ji et al. - 2019 - A Survey on Tensor Techniques and Applications in .pdf:application/pdf},\\n}\\n\\n% data structures - tensor\\n@article{rabanser_introduction_2017,\\ntitle = {Introduction to {Tensor} {Decompositions} and their {Applications} in {Machine} {Learning}},\\ncopyright = {arXiv.org perpetual, non-exclusive license},\\nurl = {https://arxiv.org/abs/1711.10781},\\ndoi = {10.48550/ARXIV.1711.10781},\\nabstract = {Tensors are multidimensional arrays of numerical values and therefore generalize matrices to multiple dimensions. While tensors first emerged in the psychometrics community in the \\\\$20{\\\\textasciicircum}\\\\{{\\\\textbackslash}text\\\\{th\\\\}\\\\}\\\\$ century, they have since then spread to numerous other disciplines, including machine learning. Tensors and their decompositions are especially beneficial in unsupervised learning settings, but are gaining popularity in other sub-disciplines like temporal and multi-relational data analysis, too. The scope of this paper is to give a broad overview of tensors, their decompositions, and how they are used in machine learning. As part of this, we are going to introduce basic tensor concepts, discuss why tensors can be considered more rigid than matrices with respect to the uniqueness of their decomposition, explain the most important factorization algorithms and their properties, provide concrete examples of tensor decomposition applications in machine learning, conduct a case study on tensor-based estimation of mixture models, talk about the current state of research, and provide references to available software libraries.},\\nurldate = {2023-02-26},\\nauthor = {Rabanser, Stephan and Shchur, Oleksandr and Günnemann, Stephan},\\nyear = {2017},\\nnote = {Publisher: arXiv\\nVersion Number: 1},\\nkeywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML)},\\n}\\n\\n% data structures - tensor\\n@article{kolda_tensor_2009,\\ntitle = {Tensor {Decompositions} and {Applications}},\\nvolume = {51},\\nissn = {0036-1445, 1095-7200},\\nurl = {http://epubs.siam.org/doi/10.1137/07070111X},\\ndoi = {10.1137/07070111X},\\nlanguage = {en},\\nnumber = {3},\\nurldate = {2023-02-26},\\njournal = {SIAM Review},\\nauthor = {Kolda, Tamara G. and Bader, Brett W.},\\nmonth = {aug},\\nyear = {2009},\\npages = {455--500},\\n}\\n\\n% data structures - tensor\\n@article{bernardi_general_2013,\\ntitle = {General tensor decomposition, moment matrices and applications},\\nvolume = {52},\\nissn = {0747-7171},\\nurl = {https://www.sciencedirect.com/science/article/pii/S0747717112001290},\\ndoi = {https://doi.org/10.1016/j.jsc.2012.05.012},\\nabstract = {The tensor decomposition addressed in this paper may be seen as a generalization of Singular Value Decomposition of matrices. We consider general multilinear and multihomogeneous tensors. We show how to reduce the problem to a truncated moment matrix problem and give a new criterion for flat extension of Quasi-Hankel matrices. We connect this criterion to the commutation characterization of border bases. A new algorithm is described. It applies for general multihomogeneous tensors, extending the approach on binary forms by J.J. Sylvester. An example illustrates the algebraic operations involved in this approach and how the decomposition can be recovered from eigenvector computation.},\\njournal = {Journal of Symbolic Computation},\\nauthor = {Bernardi, A. and Brachat, J. and Comon, P. and Mourrain, B.},\\nyear = {2013},\\nkeywords = {Decomposition, Flat extension, Hankel operator, Moment matrix, Multihomogeneous polynomial, Rank, Tensor},\\npages = {51--71},\\n}\\n@article{10.5555/3455716.3455856,\\nauthor = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},\\ntitle = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},\\nyear = {2020},\\nissue_date = {January 2020},\\npublisher = {JMLR.org},\\nvolume = {21},\\nnumber = {1},\\nissn = {1532-4435},\\nabstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pretraining objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},\\njournal = {J. Mach. Learn. Res.},\\nmonth = {jan},\\narticleno = {140},\\nnumpages = {67},\\nkeywords = {transfer learning, multi-task learning, deep learning, natural language processing, attention based models}\\n}\\n\\n@misc{https://doi.org/10.48550/arxiv.1905.05583,\\ndoi = {10.48550/ARXIV.1905.05583},\\nurl = {https://arxiv.org/abs/1905.05583},\\nauthor = {Sun, Chi and Qiu, Xipeng and Xu, Yige and Huang, Xuanjing},\\nkeywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},\\ntitle = {How to Fine-Tune BERT for Text Classification?},\\npublisher = {arXiv},\\nyear = {2019},\\ncopyright = {arXiv.org perpetual, non-exclusive license}\\n}\\n\\n@article{koutsoukou2021energy,\\ntitle={On the Energy Consumption of Large-Scale Language Models},\\nauthor={Koutsoukou-Argyraki, Angeliki and Ghosh, Debarghya and Arvind, V and Raffel, Colin},\\njournal={arXiv preprint arXiv:2102.06171},\\nyear={2021}\\n}\\n\\n\\n@misc{https://doi.org/10.48550/arxiv.1802.05668,\\ndoi = {10.48550/ARXIV.1802.05668}, \\nurl = {https://arxiv.org/abs/1802.05668}, \\nauthor = {Polino, Antonio and Pascanu, Razvan and Alistarh, Dan}, \\nkeywords = {Neural and Evolutionary Computing (cs.NE), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences}, \\ntitle = {Model compression via distillation and quantization},\\npublisher = {arXiv},\\nyear = {2018},\\ncopyright = {arXiv.org perpetual, non-exclusive license}\\n}\\n\\n% Algorithms\\n@article{mayer2020scalable,\\ntitle={Scalable deep learning on distributed infrastructures: Challenges, techniques, and tools},\\nauthor={Mayer, Ruben and Jacobsen, Hans-Arno},\\njournal={ACM Computing Surveys (CSUR)},\\nvolume={53},\\nnumber={1},\\npages={1--37},\\nyear={2020},\\npublisher={ACM New York, NY, USA}\\n}\\n\\n@article{gou_knowledge_2021,\\ntitle = {Knowledge {Distillation}: {A} {Survey}},\\nvolume = {129},\\nissn = {0920-5691, 1573-1405},\\nshorttitle = {Knowledge {Distillation}},\\nurl = {https://link.springer.com/10.1007/s11263-021-01453-z},\\ndoi = {10.1007/s11263-021-01453-z},\\nlanguage = {en},\\nnumber = {6},\\nurldate = {2023-03-16},\\njournal = {International Journal of Computer Vision},\\nauthor = {Gou, Jianping and Yu, Baosheng and Maybank, Stephen J. and Tao, Dacheng},\\nmonth = {jun},\\nyear = {2021},\\npages = {1789--1819},\\nfile = {Versione accettata:C\\\\:\\\\\\\\Users\\\\\\\\Enric\\\\\\\\Zotero\\\\\\\\storage\\\\\\\\BFZPDPMN\\\\\\\\Gou et al. - 2021 - Knowledge Distillation A Survey.pdf:application/pdf},\\n}\\n\\n% Residual connection\\n@misc{he2015deep,\\ntitle={Deep Residual Learning for Image Recognition}, \\nauthor={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},\\nyear={2015},\\neprint={1512.03385},\\narchivePrefix={arXiv},\\nprimaryClass={cs.CV}\\n}\\n\\n% Depthwise convolutiion for MobiNet\\n@article{howard2017mobilenets,\\ntitle={MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications},\\nauthor={Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},\\njournal={arXiv preprint arXiv:1704.04861},\\nyear={2017}\\n}\\n\\n% Weight sharing\\n@article{lecun1989backpropagation,\\ntitle={Backpropagation applied to handwritten zip code recognition},\\nauthor={LeCun, Yann and Boser, Bernhard and Denker, John S and Henderson, Donnie and Howard, Richard E and Hubbard, Wayne and Jackel, Lawrence D},\\njournal={Neural computation},\\nvolume={1},\\nnumber={4},\\npages={541--551},\\nyear={1989},\\npublisher={MIT Press}\\n}\\n\\n% QUantisation\\n@article{wu2016quantized,\\ntitle={Quantized convolutional neural networks for mobile devices},\\nauthor={Wu, Sheng and Liang, Hong and Zhang, Yiran and Sun, Rui and Wang, Ting and He, Xuming},\\njournal={arXiv preprint arXiv:1603.05279},\\nyear={2016}\\n}\\n\\n% Model binarization\\n@inproceedings{courbariaux2015binaryconnect,\\ntitle={Binaryconnect: Training deep neural networks with binary weights during propagations},\\nauthor={Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},\\nbooktitle={Advances in neural information processing systems},\\npages={3123--3131},\\nyear={2015}\\n}\\n\\n% Structural matrices\\n@inproceedings{sindhwani2015structured,\\ntitle={Structured transforms for small-footprint deep learning},\\nauthor={Sindhwani, Vikas and Sainath, Tara N and Kumar, Sanjiv},\\nbooktitle={Proceedings of the 32nd International Conference on Machine Learning},\\npages={1664--1672},\\nyear={2015},\\norganization={JMLR Workshop and Conference Proceedings}\\n}\\n\\n% Computer architectures\\n% Hardware architecture\\n\\n% Questa citazione sembra svagliata! Non la uso\\n@article{harvardarchitecture,\\ntitle={A Logical Calculus of Ideas Immanent in Nervous Activity},\\nauthor={McCulloch, Warren S. and Pitts, Walter},\\njournal={The Bulletin of Mathematical Biophysics},\\nvolume={5},\\nnumber={4},\\npages={115-133},\\nyear={1943},\\npublisher={Springer}\\n}\\n@article{rosenblatt1960perceptron,\\ntitle={Perceptron simulation experiments},\\nauthor={Rosenblatt, Frank},\\njournal={Proceedings of the IRE},\\nvolume={48},\\nnumber={3},\\npages={301--309},\\nyear={1960},\\npublisher={IEEE}\\n}\\n\\n@article{bravyi2022future,\\ntitle={The future of quantum computing with superconducting qubits},\\nauthor={Bravyi, Sergey and Dial, Oliver and Gambetta, Jay M and Gil, Dario and Nazario, Zaira},\\njournal={Journal of Applied Physics},\\nvolume={132},\\nnumber={16},\\npages={160902},\\nyear={2022},\\npublisher={AIP Publishing LLC}\\n}\\n\\n@inproceedings{elsayed2019review,\\ntitle={A review of quantum computer energy efficiency},\\nauthor={Elsayed, Nelly and Maida, Anthony S and Bayoumi, Magdy},\\nbooktitle={2019 IEEE Green Technologies Conference (GreenTech)},\\npages={1--3},\\nyear={2019},\\norganization={IEEE}\\n}\\n\\n@article{wang2013review,\\ntitle={Review of performance metrics for green data centers: a taxonomy study},\\nauthor={Wang, Lizhe and Khan, Samee U},\\njournal={The journal of supercomputing},\\nvolume={63},\\npages={639--656},\\nyear={2013},\\npublisher={Springer}\\n}\\n\\n@book{vetter2013contemporary,\\ntitle={Contemporary high performance computing: from Petascale toward exascale},\\nauthor={Vetter, Jeffrey S},\\nyear={2013},\\npublisher={CRC Press}\\n}\\n\\n@article{pawson2022myth,\\ntitle={The myth of the Harvard architecture},\\nauthor={Pawson, Richard},\\njournal={IEEE Annals of the History of Computing},\\nvolume={44},\\nnumber={3},\\npages={59--69},\\nyear={2022},\\npublisher={IEEE}\\n}\\n\\n% Modified Hardware architectures\\n@article{hennessy1996computer,\\ntitle={Computer architecture: a quantitative approach},\\nauthor={Hennessy, John L and Patterson, David A},\\nyear={1996},\\npublisher={Morgan Kaufmann}\\n}\\n\\n% Neuromorphjic\\n@article{markram2006blue,\\ntitle={The blue brain project},\\nauthor={Markram, Henry},\\njournal={Nature Reviews Neuroscience},\\nvolume={7},\\nnumber={2},\\npages={153--160},\\nyear={2006},\\npublisher={Nature Publishing Group UK London}\\n}\\n\\n% IBM TrueNorth\\n@article{akopyan2015truenorth,\\ntitle={Truenorth: Design and tool flow of a 65 mw 1 million neuron programmable neurosynaptic chip},\\nauthor={Akopyan, Filipp and Sawada, Jun and Cassidy, Andrew and Alvarez-Icaza, Rodrigo and Arthur, John and Merolla, Paul and Imam, Nabil and Nakamura, Yutaka and Datta, Pallab and Nam, Gi-Joon and others},\\njournal={IEEE transactions on computer-aided design of integrated circuits and systems},\\nvolume={34},\\nnumber={10},\\npages={1537--1557},\\nyear={2015},\\npublisher={IEEE}\\n}\\n\\n% Spiking Neural Networks (SNNs) Architecture:\\n@article{maass1997networks,\\ntitle={Networks of spiking neurons: the third generation of neural network models},\\nauthor={Maass, Wolfgang},\\njournal={Neural networks},\\nvolume={10},\\nnumber={9},\\npages={1659--1671},\\nyear={1997},\\npublisher={Elsevier}\\n}\\n\\n%Liquid State Machines (LSMs) Architecture:\\n@article{maass2002real,\\ntitle={Real-time computing without stable states: A new framework for neural computation based on perturbations},\\nauthor={Maass, Wolfgang and Natschl{\\\\\"a}ger, Thomas and Markram, Henry},\\njournal={Neural computation},\\nvolume={14},\\nnumber={11},\\npages={2531--2560},\\nyear={2002},\\npublisher={MIT Press}\\n}\\n\\n% Cellular Neural Networks (CNNs) Architecture:\\n@inproceedings{chua1988cellular,\\ntitle={Cellular neural networks: theory},\\nauthor={Chua, Leon O and Yang, Lin},\\nbooktitle={IEEE Transactions on Circuits and Systems},\\nvolume={35},\\nnumber={10},\\npages={1257--1272},\\nyear={1988},\\norganization={IEEE}\\n}\\n\\n%Self-Organizing Maps (SOMs) Architecture:\\n@inproceedings{kohonen1982self,\\ntitle={Self-organized formation of topologically correct feature maps},\\nauthor={Kohonen, Teuvo},\\nbooktitle={Biological cybernetics},\\nvolume={43},\\nnumber={1},\\npages={59--69},\\nyear={1982},\\norganization={Springer}\\n}\\n\\n% Hopfield Networks (HNs) Architecture:\\n@article{hopfield1982neural,\\ntitle={Neural networks and physical systems with emergent collective computational abilities},\\nauthor={Hopfield, John J},\\njournal={Proceedings of the national academy of sciences},\\nvolume={79},\\nnumber={8},\\npages={2554--2558},\\nyear={1982},\\npublisher={National Acad Sciences}\\n}\\n\\n\\n\\n% Radial Basis Function Networks (RBFNs) Architecture:\\n@article{park1991universal,\\ntitle={Universal approximation using radial-basis-function networks},\\nauthor={Park, Jong-Soon and Sandberg, Irwin W.},\\njournal={Neural computation},\\nvolume={3},\\nnumber={2},\\npages={246--257},\\nyear={1991},\\npublisher={MIT Press}\\n}\\n\\n\\n\\n% Neural Turing Machines (NTMs) Architecture:\\n@article{graves2014neural,\\ntitle={Neural turing machines},\\nauthor={Graves, Alex and Wayne, Greg and Danihelka, Ivo},\\njournal={arXiv preprint arXiv:1410.5401},\\nyear={2014}\\n}\\n\\n\\n@article{turing_computable_1937,\\ntitle = {On {Computable} {Numbers}, with an {Application} to the {Entscheidungsproblem}},\\nvolume = {s2-42},\\nissn = {00246115},\\nurl = {http://doi.wiley.com/10.1112/plms/s2-42.1.230},\\ndoi = {10.1112/plms/s2-42.1.230},\\nlanguage = {en},\\nnumber = {1},\\nurldate = {2023-03-22},\\njournal = {Proceedings of the London Mathematical Society},\\nauthor = {Turing, A. M.},\\nyear = {1937},\\npages = {230--265},\\n}\\n\\n@article{graves_neural_2014,\\ntitle = {Neural {Turing} {Machines}},\\ncopyright = {arXiv.org perpetual, non-exclusive license},\\nurl = {https://arxiv.org/abs/1410.5401},\\ndoi = {10.48550/ARXIV.1410.5401},\\nabstract = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},\\nurldate = {2023-03-22},\\nauthor = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},\\nyear = {2014},\\nnote = {Publisher: arXiv\\nVersion Number: 2},\\nkeywords = {FOS: Computer and information sciences, Neural and Evolutionary Computing (cs.NE)},\\n}\\n\\n\\n% NTM\\n@article{suresh_memory_2022,\\ntitle = {Memory augmented recurrent neural networks for de-novo drug design},\\nvolume = {17},\\nissn = {1932-6203},\\nurl = {https://dx.plos.org/10.1371/journal.pone.0269461},\\ndoi = {10.1371/journal.pone.0269461},\\nabstract = {A recurrent neural network (RNN) is a machine learning model that learns the relationship between elements of an input series, in addition to inferring a relationship between the data input to the model and target output. Memory augmentation allows the RNN to learn the interrelationships between elements of the input over a protracted length of the input series. Inspired by the success of stack augmented RNN (StackRNN) to generate strings for various applications, we present two memory augmented RNN-based architectures: the Neural Turing Machine (NTM) and the Differentiable Neural Computer (DNC) for the\\nde-novo\\ngeneration of small molecules. We trained a character-level convolutional neural network (CNN) to predict the properties of a generated string and compute a reward or loss in a deep reinforcement learning setup to bias the Generator to produce molecules with the desired property. Further, we compare the performance of these architectures to gain insight to their relative merits in terms of the validity and novelty of the generated molecules and the degree of property bias towards the computational generation of\\nde-novo\\ndrugs. We also compare the performance of these architectures with simpler recurrent neural networks (Vanilla RNN, LSTM, and GRU) without an external memory component to explore the impact of augmented memory in the task of\\nde-novo\\ngeneration of small molecules.},\\nlanguage = {en},\\nnumber = {6},\\nurldate = {2023-03-27},\\njournal = {PLOS ONE},\\nauthor = {Suresh, Naveen and Chinnakonda Ashok Kumar, Neelesh and Subramanian, Srikumar and Srinivasa, Gowri},\\neditor = {Nguyen, Binh P.},\\nmonth = {jun},\\nyear = {2022},\\npages = {e0269461},\\nfile = {Full text:C\\\\:\\\\\\\\Users\\\\\\\\alice\\\\\\\\Zotero\\\\\\\\storage\\\\\\\\C7ULJRHU\\\\\\\\Suresh et al. - 2022 - Memory augmented recurrent neural networks for de-.pdf:application/pdf},\\n}\\n\\n% NTM\\n@misc{gulcehre_dynamic_2017,\\ntitle = {Dynamic {Neural} {Turing} {Machine} with {Soft} and {Hard} {Addressing} {Schemes}},\\nurl = {http://arxiv.org/abs/1607.00036},\\nabstract = {We extend neural Turing machine (NTM) model into a dynamic neural Turing machine (D-NTM) by introducing a trainable memory addressing scheme. This addressing scheme maintains for each memory cell two separate vectors, content and address vectors. This allows the D-NTM to learn a wide variety of location-based addressing strategies including both linear and nonlinear ones. We implement the D-NTM with both continuous, differentiable and discrete, non-differentiable read/write mechanisms. We investigate the mechanisms and effects of learning to read and write into a memory through experiments on Facebook bAbI tasks using both a feedforward and GRUcontroller. The D-NTM is evaluated on a set of Facebook bAbI tasks and shown to outperform NTM and LSTM baselines. We have done extensive analysis of our model and different variations of NTM on bAbI task. We also provide further experimental results on sequential pMNIST, Stanford Natural Language Inference, associative recall and copy tasks.},\\nurldate = {2023-03-27},\\npublisher = {arXiv},\\nauthor = {Gulcehre, Caglar and Chandar, Sarath and Cho, Kyunghyun and Bengio, Yoshua},\\nmonth = {mar},\\nyear = {2017},\\nnote = {arXiv:1607.00036 [cs]},\\nkeywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},\\nfile = {arXiv Fulltext PDF:C\\\\:\\\\\\\\Users\\\\\\\\alice\\\\\\\\Zotero\\\\\\\\storage\\\\\\\\SL9U3D49\\\\\\\\Gulcehre et al. - 2017 - Dynamic Neural Turing Machine with Soft and Hard A.pdf:application/pdf;arXiv.org Snapshot:C\\\\:\\\\\\\\Users\\\\\\\\alice\\\\\\\\Zotero\\\\\\\\storage\\\\\\\\ZY7JUZTU\\\\\\\\1607.html:text/html},\\n}\\n\\n@article{tyagi2022recurrent,\\ntitle={Recurrent Neural Networks: Concepts and Applications},\\nauthor={Tyagi, Amit Kumar and Abraham, Ajith},\\nyear={2022},\\npublisher={CRC Press}\\n}\\n\\n% da qui alcune evoluzioni della NTM\\n\\n@misc{zaremba_reinforcement_2016,\\ntitle = {Reinforcement {Learning} {Neural} {Turing} {Machines} - {Revised}},\\nurl = {http://arxiv.org/abs/1505.00521},\\nabstract = {The Neural Turing Machine (NTM) is more expressive than all previously considered models because of its external memory. It can be viewed as a broader effort to use abstract external Interfaces and to learn a parametric model that interacts with them. The capabilities of a model can be extended by providing it with proper Interfaces that interact with the world. These external Interfaces include memory, a database, a search engine, or a piece of software such as a theorem verifier. Some of these Interfaces are provided by the developers of the model. However, many important existing Interfaces, such as databases and search engines, are discrete. We examine feasibility of learning models to interact with discrete Interfaces. We investigate the following discrete Interfaces: a memory Tape, an input Tape, and an output Tape. We use a Reinforcement Learning algorithm to train a neural network that interacts with such Interfaces to solve simple algorithmic tasks. Our Interfaces are expressive enough to make our model Turing complete.},\\nurldate = {2023-03-27},\\npublisher = {arXiv},\\nauthor = {Zaremba, Wojciech and Sutskever, Ilya},\\nmonth = {jan},\\nyear = {2016},\\nnote = {arXiv:1505.00521 [cs]},\\nkeywords = {Computer Science - Machine Learning},\\nfile = {arXiv Fulltext PDF:C\\\\:\\\\\\\\Users\\\\\\\\alice\\\\\\\\Zotero\\\\\\\\storage\\\\\\\\YLZB4AGR\\\\\\\\Zaremba e Sutskever - 2016 - Reinforcement Learning Neural Turing Machines - Re.pdf:application/pdf},\\n}\\n\\n\\n@inproceedings{greve_evolving_2016,\\naddress = {Denver Colorado USA},\\ntitle = {Evolving {Neural} {Turing} {Machines} for {Reward}-based {Learning}},\\nisbn = {978-1-4503-4206-3},\\nurl = {https://dl.acm.org/doi/10.1145/2908812.2908930},\\ndoi = {10.1145/2908812.2908930},\\nlanguage = {en},\\nurldate = {2023-03-27},\\nbooktitle = {Proceedings of the {Genetic} and {Evolutionary} {Computation} {Conference} 2016},\\npublisher = {ACM},\\nauthor = {Greve, Rasmus Boll and Jacobsen, Emil Juul and Risi, Sebastian},\\nmonth = {jul},\\nyear = {2016},\\npages = {117--124},\\n}\\n\\n\\n\\n@misc{yang_lie_2016,\\ntitle = {Lie {Access} {Neural} {Turing} {Machine}},\\nurl = {http://arxiv.org/abs/1602.08671},\\nabstract = {Following the recent trend in explicit neural memory structures, we present a new design of an external memory, wherein memories are stored in an Euclidean key space \\\\${\\\\textbackslash}mathbb R{\\\\textasciicircum}n\\\\$. An LSTM controller performs read and write via specialized read and write heads. It can move a head by either providing a new address in the key space (aka random access) or moving from its previous position via a Lie group action (aka Lie access). In this way, the \"L\" and \"R\" instructions of a traditional Turing Machine are generalized to arbitrary elements of a fixed Lie group action. For this reason, we name this new model the Lie Access Neural Turing Machine, or LANTM. We tested two different configurations of LANTM against an LSTM baseline in several basic experiments. We found the right configuration of LANTM to outperform the baseline in all of our experiments. In particular, we trained LANTM on addition of \\\\$k\\\\$-digit numbers for \\\\$2 {\\\\textbackslash}le k {\\\\textbackslash}le 16\\\\$, but it was able to generalize almost perfectly to \\\\$17 {\\\\textbackslash}le k {\\\\textbackslash}le 32\\\\$, all with the number of parameters 2 orders of magnitude below the LSTM baseline.},\\nurldate = {2023-03-27},\\npublisher = {arXiv},\\nauthor = {Yang, Greg},\\nmonth = {sep},\\nyear = {2016},\\nnote = {arXiv:1602.08671 [cs]},\\nkeywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},\\nfile = {arXiv Fulltext PDF:C\\\\:\\\\\\\\Users\\\\\\\\alice\\\\\\\\Zotero\\\\\\\\storage\\\\\\\\3F3C3R78\\\\\\\\Yang - 2016 - Lie Access Neural Turing Machine.pdf:application/pdf},\\n}\\n\\n\\n\\n@misc{kurach_neural_2016,\\ntitle = {Neural {Random}-{Access} {Machines}},\\nurl = {http://arxiv.org/abs/1511.06392},\\nurldate = {2023-03-27},\\npublisher = {arXiv},\\nauthor = {Kurach, Karol and Andrychowicz, Marcin and Sutskever, Ilya},\\nmonth = {feb},\\nyear = {2016},\\nnote = {arXiv:1511.06392 [cs]},\\nkeywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},\\nfile = {arXiv Fulltext PDF:C\\\\:\\\\\\\\Users\\\\\\\\alice\\\\\\\\Zotero\\\\\\\\storage\\\\\\\\XPLSQT9K\\\\\\\\Kurach et al. - 2016 - Neural Random-Access Machines.pdf:application/pdf},\\n}\\n\\n\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n% Enrico Cluster 0\\n% SpanBERT Improving Pre training by Representing and Predicting Spans\\n@article{joshi2020spanbert,\\ntitle={Spanbert: Improving pre-training by representing and predicting spans},\\nauthor={Joshi, Mandar and Chen, Danqi and Liu, Yinhan and Weld, Daniel S and Zettlemoyer, Luke and Levy, Omer},\\njournal={Transactions of the Association for Computational Linguistics},\\nvolume={8},\\npages={64--77},\\nyear={2020},\\npublisher={MIT Press}\\n}\\n% BioBERT a pre trained biomedical language representation model for biomedical text mining\\n% ALBERT A Lite BERT for Self supervised Learning of Language Representations\\n@misc{lan2020albert,\\ntitle={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations}, \\nauthor={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},\\nyear={2020},\\neprint={1909.11942},\\narchivePrefix={arXiv},\\nprimaryClass={cs.CL}\\n}\\n\\n@article{chowdhery2022palm,\\ntitle={Palm: Scaling language modeling with pathways},\\nauthor={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},\\njournal={arXiv preprint arXiv:2204.02311},\\nyear={2022}\\n}\\n% Efficient Transformer based Large Scale Language Representations using Hardware friendly Block Structured Pruning INTRUSO?????? NON ESATTAMENTE BERT PERCHE\\' VIENE CITATO 121 VOLTE! https://arxiv.org/pdf/2009.08065.pdf\\n@article{li2020efficient,\\ntitle={Efficient transformer-based large scale language representations using hardware-friendly block structured pruning},\\nauthor={Li, Bingbing and Kong, Zhenglun and Zhang, Tianyun and Li, Ji and Li, Zhengang and Liu, Hang and Ding, Caiwen},\\njournal={arXiv preprint arXiv:2009.08065},\\nyear={2020}\\n}\\n% Revealing the Dark Secrets of BERT https://arxiv.org/pdf/1908.08593.pdf\\n@article{kovaleva2019revealing,\\ntitle={Revealing the dark secrets of BERT},\\nauthor={Kovaleva, Olga and Romanov, Alexey and Rogers, Anna and Rumshisky, Anna},\\njournal={arXiv preprint arXiv:1908.08593},\\nyear={2019}\\n}\\n% A Survey on Green Deep Learning INTRUSO??????? NON ESATTAMENTE PERCHE\\' BERT VIENE CITATO 76 VOLTE\\n@article{xu2021survey,\\ntitle={A survey on green deep learning},\\nauthor={Xu, Jingjing and Zhou, Wangchunshu and Fu, Zhiyi and Zhou, Hao and Li, Lei},\\njournal={arXiv preprint arXiv:2111.05193},\\nyear={2021}\\n}\\n\\n% RoBERTa A Robustly Optimized BERT Pretraining Approach https://arxiv.org/pdf/1907.11692.pdf%5C\\n@article{liu2019roberta,\\ntitle={Roberta: A robustly optimized bert pretraining approach},\\nauthor={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},\\njournal={arXiv preprint arXiv:1907.11692},\\nyear={2019}\\n}\\n% Exploring the Limits of Transfer Learning with a Unified Text to Text Transformer (già citato)\\n%https://www.jmlr.org/papers/volume21/20-074/20-074.pdf INTRUSO??????? NON ESATTAMENTE PERCHE\\' BERT VIENE CITATO CIRCA 100 VOLTE\\n\\n% BERT Pre training of Deep Bidirectional Transformers for Language Understanding\\n%https://arxiv.org/pdf/1810.04805.pdf&usg=ALkJrhhzxlCL6yTht2BRmH9atgvKFxHsxQ\\n@article{devlin2018bert,\\ntitle={Bert: Pre-training of deep bidirectional transformers for language understanding},\\nauthor={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},\\njournal={arXiv preprint arXiv:1810.04805},\\nyear={2018}\\n}\\n\\n\\n% A Primer in BERTology What We Know About How BERT Works\\n%https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=A+Primer+in+BERTology+What+We+Know+About+How+BERT+Works&btnG= GIA\\' CITATO\\n\\n\\n% Knowledge Distillation A Survey\\n%https://link.springer.com/article/10.1007/s11263-021-01453-z\\n@article{gou2021knowledge,\\ntitle={Knowledge distillation: A survey},\\nauthor={Gou, Jianping and Yu, Baosheng and Maybank, Stephen J and Tao, Dacheng},\\njournal={International Journal of Computer Vision},\\nvolume={129},\\npages={1789--1819},\\nyear={2021},\\npublisher={Springer}\\n}\\n% Improving Language Understanding by Generative Pre Training\\n%https://link.springer.com/article/10.1007/s11263-021-01453-z\\n@article{radford2018improving,\\ntitle={Improving language understanding by generative pre-training},\\nauthor={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},\\nyear={2018},\\npublisher={OpenAI}\\n}\\n\\n@article{radford2019language,\\ntitle={Language models are unsupervised multitask learners},\\nauthor={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},\\njournal={OpenAI blog},\\nvolume={1},\\nnumber={8},\\npages={9},\\nyear={2019}\\n}\\n\\n\\n% DistilBERT a distilled version of BERT smaller faster cheaper and lighter\\n%https://arxiv.org/pdf/1910.01108.pdf%3C/p%3E\\n@article{sanh2019distilbert,\\ntitle={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},\\nauthor={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},\\njournal={arXiv preprint arXiv:1910.01108},\\nyear={2019}\\n}\\n\\n% BioBert\\n@article{lee2020biobert,\\ntitle={BioBERT: a pre-trained biomedical language representation model for biomedical text mining},\\nauthor={Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},\\njournal={Bioinformatics},\\nvolume={36},\\nnumber={4},\\npages={1234--1240},\\nyear={2020},\\npublisher={Oxford University Press}\\n}\\n\\n\\n% Unsupervised Cross lingual Representation Learning at Scale\\n%https://arxiv.org/pdf/1911.02116.pdf\\n@article{conneau2019unsupervised,\\ntitle={Unsupervised cross-lingual representation learning at scale},\\nauthor={Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\\\\\\'a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},\\njournal={arXiv preprint arXiv:1911.02116},\\nyear={2019}\\n}\\n% EarlyBERT Efficient BERT Training via Early bird Lottery Tickets\\n%https://arxiv.org/pdf/2101.00063.pdf\\n@article{chen2020earlybert,\\ntitle={Earlybert: Efficient bert training via early-bird lottery tickets},\\nauthor={Chen, Xiaohan and Cheng, Yu and Wang, Shuohang and Gan, Zhe and Wang, Zhangyang and Liu, Jingjing},\\njournal={arXiv preprint arXiv:2101.00063},\\nyear={2020}\\n}\\n\\n@book{domingos2015master,\\ntitle={The master algorithm: How the quest for the ultimate learning machine will remake our world},\\nauthor={Domingos, Pedro},\\nyear={2015},\\npublisher={Basic Books}\\n}\\n\\n@book{minsky2017perceptrons,\\ntitle={Perceptrons, Reissue of the 1988 Expanded Edition with a new foreword by L{\\\\\\'e}on Bottou: An Introduction to Computational Geometry},\\nauthor={Minsky, Marvin and Papert, Seymour A},\\nyear={2017},\\npublisher={MIT press}\\n}\\n\\n\\n\\n% CARBON FOOTPRINT\\n\\n@article{anthony_carbontracker_2020,\\ntitle = {Carbontracker: {Tracking} and {Predicting} the {Carbon} {Footprint} of {Training} {Deep} {Learning} {Models}},\\ncopyright = {arXiv.org perpetual, non-exclusive license},\\nshorttitle = {Carbontracker},\\nurl = {https://arxiv.org/abs/2007.03051},\\ndoi = {10.48550/ARXIV.2007.03051},\\nabstract = {Deep learning (DL) can achieve impressive results across a wide variety of tasks, but this often comes at the cost of training models for extensive periods on specialized hardware accelerators. This energy-intensive workload has seen immense growth in recent years. Machine learning (ML) may become a significant contributor to climate change if this exponential trend continues. If practitioners are aware of their energy and carbon footprint, then they may actively take steps to reduce it whenever possible. In this work, we present Carbontracker, a tool for tracking and predicting the energy and carbon footprint of training DL models. We propose that energy and carbon footprint of model development and training is reported alongside performance metrics using tools like Carbontracker. We hope this will promote responsible computing in ML and encourage research into energy-efficient deep neural networks.},\\nurldate = {2023-06-20},\\nauthor = {Anthony, Lasse F. Wolff and Kanding, Benjamin and Selvan, Raghavendra},\\nyear = {2020},\\nnote = {Publisher: arXiv\\nVersion Number: 1},\\nkeywords = {Computers and Society (cs.CY), FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, Machine Learning (cs.LG), Machine Learning (stat.ML), Signal Processing (eess.SP)},\\n}\\n\\n\\n\\n@inproceedings{zhao_greener_2022,\\naddress = {Lyon, France},\\ntitle = {A {Green}(er) {World} for {A}.{I}.},\\nisbn = {978-1-66549-747-3},\\nurl = {https://ieeexplore.ieee.org/document/9835179/},\\ndoi = {10.1109/IPDPSW55747.2022.00126},\\nurldate = {2023-06-20},\\nbooktitle = {2022 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} {Workshops} ({IPDPSW})},\\npublisher = {IEEE},\\nauthor = {Zhao, Dan and Frey, Nathan C. and McDonald, Joseph and Hubbell, Matthew and Bestor, David and Jones, Michael and Prout, Andrew and Gadepally, Vijay and Samsi, Siddharth},\\nmonth = {may},\\nyear = {2022},\\npages = {742--750},\\nfile = {Versione inviata:C\\\\:\\\\\\\\Users\\\\\\\\alice\\\\\\\\Zotero\\\\\\\\storage\\\\\\\\8346ZE8B\\\\\\\\Zhao et al. - 2022 - A Green(er) World for A.I..pdf:application/pdf},\\n}\\n\\n@article{patterson2021carbon,\\ntitle={Carbon emissions and large neural network training},\\nauthor={Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},\\njournal={arXiv preprint arXiv:2104.10350},\\nyear={2021}\\n}\\n\\n@inproceedings{gupta2021chasing,\\ntitle={Chasing carbon: The elusive environmental footprint of computing},\\nauthor={Gupta, Udit and Kim, Young Geun and Lee, Sylvia and Tse, Jordan and Lee, Hsien-Hsin S and Wei, Gu-Yeon and Brooks, David and Wu, Carole-Jean},\\nbooktitle={2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},\\npages={854--867},\\nyear={2021},\\norganization={IEEE}\\n}\\n\\n\\n\\n\\n@article{wu_sustainable_2021,\\ntitle = {Sustainable {AI}: {Environmental} {Implications}, {Challenges} and {Opportunities}},\\ncopyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},\\nshorttitle = {Sustainable {AI}},\\nurl = {https://arxiv.org/abs/2111.00364},\\ndoi = {10.48550/ARXIV.2111.00364},\\nabstract = {This paper explores the environmental impact of the super-linear growth trends for AI from a holistic perspective, spanning Data, Algorithms, and System Hardware. We characterize the carbon footprint of AI computing by examining the model development cycle across industry-scale machine learning use cases and, at the same time, considering the life cycle of system hardware. Taking a step further, we capture the operational and manufacturing carbon footprint of AI computing and present an end-to-end analysis for what and how hardware-software design and at-scale optimization can help reduce the overall carbon footprint of AI. Based on the industry experience and lessons learned, we share the key challenges and chart out important development directions across the many dimensions of AI. We hope the key messages and insights presented in this paper can inspire the community to advance the field of AI in an environmentally-responsible manner.},\\nurldate = {2023-06-20},\\nauthor = {Wu, Carole-Jean and Raghavendra, Ramya and Gupta, Udit and Acun, Bilge and Ardalani, Newsha and Maeng, Kiwan and Chang, Gloria and Behram, Fiona Aga and Huang, James and Bai, Charles and Gschwind, Michael and Gupta, Anurag and Ott, Myle and Melnikov, Anastasia and Candido, Salvatore and Brooks, David and Chauhan, Geeta and Lee, Benjamin and Lee, Hsien-Hsin S. and Akyildiz, Bugra and Balandat, Maximilian and Spisak, Joe and Jain, Ravi and Rabbat, Mike and Hazelwood, Kim},\\nyear = {2021},\\nnote = {Publisher: arXiv\\nVersion Number: 2},\\nkeywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, Hardware Architecture (cs.AR), Machine Learning (cs.LG)},\\n}\\n\\n\\n@article{patterson_carbon_2022-1,\\ntitle = {The {Carbon} {Footprint} of {Machine} {Learning} {Training} {Will} {Plateau}, {Then} {Shrink}},\\nvolume = {55},\\nissn = {0018-9162, 1558-0814},\\nurl = {https://ieeexplore.ieee.org/document/9810097/},\\ndoi = {10.1109/MC.2022.3148714},\\nnumber = {7},\\nurldate = {2023-06-20},\\njournal = {Computer},\\nauthor = {Patterson, David and Gonzalez, Joseph and Holzle, Urs and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David R. and Texier, Maud and Dean, Jeff},\\nmonth = {jul},\\nyear = {2022},\\npages = {18--28},\\nfile = {Versione inviata:C\\\\:\\\\\\\\Users\\\\\\\\alice\\\\\\\\Zotero\\\\\\\\storage\\\\\\\\YDTBM5BT\\\\\\\\Patterson et al. - 2022 - The Carbon Footprint of Machine Learning Training .pdf:application/pdf},\\n}\\n\\n@article{henderson2020towards,\\ntitle={Towards the systematic reporting of the energy and carbon footprints of machine learning},\\nauthor={Henderson, Peter and Hu, Jieru and Romoff, Joshua and Brunskill, Emma and Jurafsky, Dan and Pineau, Joelle},\\njournal={The Journal of Machine Learning Research},\\nvolume={21},\\nnumber={1},\\npages={10039--10081},\\nyear={2020},\\npublisher={JMLRORG}\\n}\\n\\n\\n\\n@article{perucica_is_2022,\\ntitle = {Is the future of {AI} sustainable? {A} case study of the {European} {Union}},\\nvolume = {16},\\nissn = {1750-6166, 1750-6166},\\nshorttitle = {Is the future of {AI} sustainable?},\\nurl = {https://www.emerald.com/insight/content/doi/10.1108/TG-06-2021-0106/full/html},\\ndoi = {10.1108/TG-06-2021-0106},\\nabstract = {Purpose\\nThe purpose of this paper is to raise awareness on the need for a more comprehensive approach on the interdependence between artificial intelligence (AI) and environmental sustainability. It provides an overview of existing sustainable AI policy initiatives at the national and regional level. More precisely, it discusses whether existing European Union (EU) environmental policies are suitable for the AI era or whether new regulations are needed in this field. Finally, this paper assesses cross-fertilisation opportunities between the EU and non-EU countries.\\n\\n\\nDesign/methodology/approach\\nThis study is based on a qualitative analysis of sustainable applications of AI and the sustainability of AI. Emphasis is laid on the latter, and a “sustainable by design” approach is proposed, which in essence is a prerequisite for transparent, responsible and human-centred AI systems. The analysis primarily focuses on environmental sustainability.\\n\\n\\nFindings\\nThe majority of studies focus on how to use AI to protect the environment with very little attention paid to sustainable design of AI. On the other hand, the EU’s comprehensive approach towards sustainable AI is closest to promoting “sustainable by design” AI. Several ways have been identified in which the EU’s actions can be translated beyond its borders.\\n\\n\\nResearch limitations/implications\\nOne of the largest limitations of this study is its moderate scope. This paper is confined to the EU and as such provides a limited assessment of global policies and measures on the interplay between sustainability and AI. Consequently, the paper did not provide an in-depth analysis of environmental policies worldwide that could help provide a better picture of possible cooperation areas or common grounds. Another limitation of this study is that it primarily focuses on environmental aspects and as such accords little attention to the economic and social pillars of sustainability.\\n\\n\\nSocial implications\\nWith less than 10\\u2009years to go before reaching the sustainable development goal deadline, this study can help stakeholders better understand what is being done worldwide in terms of sustainable AI. Moreover, given that the technology is still in its early phase, this study can inspire a “sustainable by design” approach to the development of AI technologies.\\n\\n\\nOriginality/value\\nAll national AI strategies published by 1 June 2021 were analysed to identify whether and to what extent they prioritise the interplay between environment and AI. Furthermore, the authors also looked at the EU policy and how it aims to address AI from a sustainable perspective.},\\nlanguage = {en},\\nnumber = {3},\\nurldate = {2023-06-25},\\njournal = {Transforming Government: People, Process and Policy},\\nauthor = {Perucica, Natasa and Andjelkovic, Katarina},\\nmonth = jul,\\nyear = {2022},\\npages = {347--358},\\nfile = {Full text:C\\\\:\\\\\\\\Users\\\\\\\\alice\\\\\\\\Zotero\\\\\\\\storage\\\\\\\\DQ7WXAFN\\\\\\\\Perucica e Andjelkovic - 2022 - Is the future of AI sustainable A case study of t.pdf:application/pdf},\\n}\\n\\n@techreport{noauthor_online_2023,\\ntype = {{OECD} {Digital} {Economy} {Papers}},\\ntitle = {Online product safety sweep report},\\nurl = {https://www.oecd-ilibrary.org/science-and-technology/online-product-safety-sweep-report_c1faa51e-en},\\nlanguage = {en},\\nnumber = {354},\\nurldate = {2023-06-25},\\nmonth = jun,\\nyear = {2023},\\ndoi = {10.1787/c1faa51e-en},\\nnote = {Series: OECD Digital Economy Papers\\nVolume: 354},\\n}\\n\\n@article{piorkowski_quantitative_2022,\\ntitle = {Quantitative {AI} {Risk} {Assessments}: {Opportunities} and {Challenges}},\\ncopyright = {arXiv.org perpetual, non-exclusive license},\\nshorttitle = {Quantitative {AI} {Risk} {Assessments}},\\nurl = {https://arxiv.org/abs/2209.06317},\\ndoi = {10.48550/ARXIV.2209.06317},\\nabstract = {Although AI-based systems are increasingly being leveraged to provide value to organizations, individuals, and society, significant attendant risks have been identified. These risks have led to proposed regulations, litigation, and general societal concerns. As with any promising technology, organizations want to benefit from the positive capabilities of AI technology while reducing the risks. The best way to reduce risks is to implement comprehensive AI lifecycle governance where policies and procedures are described and enforced during the design, development, deployment, and monitoring of an AI system. While support for comprehensive governance is beginning to emerge, organizations often need to identify the risks of deploying an already-built model without knowledge of how it was constructed or access to its original developers. Such an assessment will quantitatively assess the risks of an existing model in a manner analogous to how a home inspector might assess the energy efficiency of an already-built home or a physician might assess overall patient health based on a battery of tests. This paper explores the concept of a quantitative AI Risk Assessment, exploring the opportunities, challenges, and potential impacts of such an approach, and discussing how it might improve AI regulations.},\\nurldate = {2023-06-25},\\nauthor = {Piorkowski, David and Hind, Michael and Richards, John},\\nyear = {2022},\\nnote = {Publisher: arXiv\\nVersion Number: 2},\\nkeywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences},\\n}\\n\\n@article{rohde_sustainability_2021,\\ntitle = {Sustainability challenges of {Artificial} {Intelligence} and {Policy} {Implications}},\\nvolume = {36},\\nissn = {1430-8800},\\nurl = {http://oekologisches-wirtschaften.de/index.php/oew/article/view/1792},\\ndoi = {10.14512/OEWO360136},\\nabstract = {Automated decision-making based on Artificial Intelligence is associated with growing expectations and is to contribute to sustainable development goals. Which opportunities and risks for the environment, economy and society are associated with Artificial Intelligence-based applications and how can they be governed?},\\nnumber = {O1},\\nurldate = {2023-06-25},\\njournal = {Ökologisches Wirtschaften - Fachzeitschrift},\\nauthor = {Rohde, Friederike and Gossen, Maike and Wagner, Josephin and Santarius, Tilman},\\nmonth = feb,\\nyear = {2021},\\npages = {36--40},\\nfile = {Full text:C\\\\:\\\\\\\\Users\\\\\\\\alice\\\\\\\\Zotero\\\\\\\\storage\\\\\\\\KMVT7E5M\\\\\\\\Rohde et al. - 2021 - Sustainability challenges of Artificial Intelligen.pdf:application/pdf},\\n}\\n\\n@article{van_wynsberghe_sustainable_2021,\\ntitle = {Sustainable {AI}: {AI} for sustainability and the sustainability of {AI}},\\nvolume = {1},\\nissn = {2730-5953, 2730-5961},\\nshorttitle = {Sustainable {AI}},\\nurl = {https://link.springer.com/10.1007/s43681-021-00043-6},\\ndoi = {10.1007/s43681-021-00043-6},\\nabstract = {Abstract\\n\\nWhile there is a growing effort towards AI\\nfor\\nSustainability (e.g. towards the sustainable development goals) it is time to move beyond that and to address the sustainability\\nof\\ndeveloping and using AI systems. In this paper I propose a definition of Sustainable AI; Sustainable AI is a movement to foster change in the entire lifecycle of AI products (i.e. idea generation, training, re-tuning, implementation, governance) towards greater ecological integrity and social justice. As such, Sustainable AI is focused on more than AI applications; rather, it addresses the whole sociotechnical system of AI. I have suggested here that Sustainable AI is not about how to sustain the development of AI per say but it is about how to develop AI that is compatible with sustaining environmental resources for current and future generations; economic models for societies; and societal values that are fundamental to a given society. I have articulated that the phrase Sustainable AI be understood as having two branches; AI\\nfor\\nsustainability and sustainability\\nof\\nAI (e.g. reduction of carbon emissions and computing power). I propose that Sustainable AI take sustainable development at the core of its definition with three accompanying tensions between AI innovation and equitable resource distribution; inter and intra-generational justice; and, between environment, society, and economy. This paper is not meant to engage with each of the three pillars of sustainability (i.e. social, economic, environment), and as such the pillars of sustainable AI. Rather, this paper is meant to inspire the reader, the policy maker, the AI ethicist, the AI developer to connect with the environment—to remember that there are environmental costs to AI. Further, to direct funding towards sustainable methods\\nof\\nAI.},\\nlanguage = {en},\\nnumber = {3},\\nurldate = {2023-06-25},\\njournal = {AI and Ethics},\\nauthor = {Van Wynsberghe, Aimee},\\nmonth = aug,\\nyear = {2021},\\npages = {213--218},\\nfile = {Full text:C\\\\:\\\\\\\\Users\\\\\\\\alice\\\\\\\\Zotero\\\\\\\\storage\\\\\\\\M4JCFC97\\\\\\\\Van Wynsberghe - 2021 - Sustainable AI AI for sustainability and the sust.pdf:application/pdf},\\n}\\n\\n\\n\\n\\n@article{strubell_energy_2020,\\ntitle = {Energy and {Policy} {Considerations} for {Modern} {Deep} {Learning} {Research}},\\nvolume = {34},\\nissn = {2374-3468, 2159-5399},\\nurl = {https://ojs.aaai.org/index.php/AAAI/article/view/7123},\\ndoi = {10.1609/aaai.v34i09.7123},\\nabstract = {The field of artificial intelligence has experienced a dramatic methodological shift towards large neural networks trained on plentiful data. This shift has been fueled by recent advances in hardware and techniques enabling remarkable levels of computation, resulting in impressive advances in AI across many applications. However, the massive computation required to obtain these exciting results is costly both financially, due to the price of specialized hardware and electricity or cloud compute time, and to the environment, as a result of non-renewable energy used to fuel modern tensor processing hardware. In a paper published this year at ACL, we brought this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training and tuning neural network models for NLP (Strubell, Ganesh, and McCallum 2019). In this extended abstract, we briefly summarize our findings in NLP, incorporating updated estimates and broader information from recent related publications, and provide actionable recommendations to reduce costs and improve equity in the machine learning and artificial intelligence community.},\\nnumber = {09},\\nurldate = {2023-06-25},\\njournal = {Proceedings of the AAAI Conference on Artificial Intelligence},\\nauthor = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},\\nmonth = apr,\\nyear = {2020},\\npages = {13693--13696},\\nfile = {Full text:C\\\\:\\\\\\\\Users\\\\\\\\alice\\\\\\\\Zotero\\\\\\\\storage\\\\\\\\NJWF93XC\\\\\\\\Strubell et al. - 2020 - Energy and Policy Considerations for Modern Deep L.pdf:application/pdf},\\n}\\n\\n\\n@article{lannelongue_green_2021,\\ntitle = {Green {Algorithms}: {Quantifying} the {Carbon} {Footprint} of {Computation}},\\nvolume = {8},\\nissn = {2198-3844, 2198-3844},\\nshorttitle = {Green {Algorithms}},\\nurl = {https://onlinelibrary.wiley.com/doi/10.1002/advs.202100707},\\ndoi = {10.1002/advs.202100707},\\nlanguage = {en},\\nnumber = {12},\\nurldate = {2023-06-25},\\njournal = {Advanced Science},\\nauthor = {Lannelongue, Loïc and Grealey, Jason and Inouye, Michael},\\nmonth = jun,\\nyear = {2021},\\npages = {2100707},\\nfile = {Full text:C\\\\:\\\\\\\\Users\\\\\\\\alice\\\\\\\\Zotero\\\\\\\\storage\\\\\\\\WPD7NS96\\\\\\\\Lannelongue et al. - 2021 - Green Algorithms Quantifying the Carbon Footprint.pdf:application/pdf},\\n}\\n\\n@article{lacoste_quantifying_2019,\\ntitle = {Quantifying the {Carbon} {Emissions} of {Machine} {Learning}},\\ncopyright = {Creative Commons Attribution Share Alike 4.0 International},\\nurl = {https://arxiv.org/abs/1910.09700},\\ndoi = {10.48550/ARXIV.1910.09700},\\nabstract = {From an environmental standpoint, there are a few crucial aspects of training a neural network that have a major impact on the quantity of carbon that it emits. These factors include: the location of the server used for training and the energy grid that it uses, the length of the training procedure, and even the make and model of hardware on which the training takes place. In order to approximate these emissions, we present our Machine Learning Emissions Calculator, a tool for our community to better understand the environmental impact of training ML models. We accompany this tool with an explanation of the factors cited above, as well as concrete actions that individual practitioners and organizations can take to mitigate their carbon emissions.},\\nurldate = {2023-06-25},\\nauthor = {Lacoste, Alexandre and Luccioni, Alexandra and Schmidt, Victor and Dandres, Thomas},\\nyear = {2019},\\nnote = {Publisher: arXiv\\nVersion Number: 2},\\nkeywords = {Computers and Society (cs.CY), FOS: Computer and information sciences, Machine Learning (cs.LG)},\\n}\\n\\n@article{barbierato2023multiformalism,\\ntitle={A Multiformalism-Based Model for Performance Evaluation of Green Data Centres},\\nauthor={Barbierato, Enrico and Manini, Daniele and Gribaudo, Marco},\\njournal={Electronics},\\nvolume={12},\\nnumber={10},\\npages={2169},\\nyear={2023},\\npublisher={MDPI}\\n}\\n\\n@misc{shaikh2021energyvis,\\ntitle={EnergyVis: Interactively Tracking and Exploring Energy Consumption for ML Models}, \\nauthor={Omar Shaikh and Jon Saad-Falcon and Austin P Wright and Nilaksh Das and Scott Freitas and Omar Isaac Asensio and Duen Horng Chau},\\nyear={2021},\\neprint={2103.16435},\\narchivePrefix={arXiv},\\nprimaryClass={cs.LG}\\n}\\n\\n@article{frankle2018lottery,\\ntitle={The lottery ticket hypothesis: Finding sparse, trainable neural networks},\\nauthor={Frankle, Jonathan and Carbin, Michael},\\njournal={arXiv preprint arXiv:1803.03635},\\nyear={2018}\\n}\\n\\n@article{verdecchia_systematic_2023,\\ntitle = {A {Systematic} {Review} of {Green} {AI}},\\ncopyright = {arXiv.org perpetual, non-exclusive license},\\nurl = {https://arxiv.org/abs/2301.11047},\\ndoi = {10.48550/ARXIV.2301.11047},\\nabstract = {With the ever-growing adoption of AI-based systems, the carbon footprint of AI is no longer negligible. AI researchers and practitioners are therefore urged to hold themselves accountable for the carbon emissions of the AI models they design and use. This led in recent years to the appearance of researches tackling AI environmental sustainability, a field referred to as Green AI. Despite the rapid growth of interest in the topic, a comprehensive overview of Green AI research is to date still missing. To address this gap, in this paper, we present a systematic review of the Green AI literature. From the analysis of 98 primary studies, different patterns emerge. The topic experienced a considerable growth from 2020 onward. Most studies consider monitoring AI model footprint, tuning hyperparameters to improve model sustainability, or benchmarking models. A mix of position papers, observational studies, and solution papers are present. Most papers focus on the training phase, are algorithm-agnostic or study neural networks, and use image data. Laboratory experiments are the most common research strategy. Reported Green AI energy savings go up to 115\\\\%, with savings over 50\\\\% being rather common. Industrial parties are involved in Green AI studies, albeit most target academic readers. Green AI tool provisioning is scarce. As a conclusion, the Green AI research field results to have reached a considerable level of maturity. Therefore, from this review emerges that the time is suitable to adopt other Green AI research strategies, and port the numerous promising academic results to industrial practice.},\\nurldate = {2023-06-28},\\nauthor = {Verdecchia, Roberto and Sallou, June and Cruz, Luís},\\nyear = {2023},\\nnote = {Publisher: arXiv\\nVersion Number: 3},\\nkeywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences},\\n}\\n\\n@inproceedings{manotas_empirical_2016,\\naddress = {Austin Texas},\\ntitle = {An empirical study of practitioners\\' perspectives on green software engineering},\\nisbn = {978-1-4503-3900-1},\\nurl = {https://dl.acm.org/doi/10.1145/2884781.2884810},\\ndoi = {10.1145/2884781.2884810},\\nlanguage = {en},\\nurldate = {2023-06-28},\\nbooktitle = {Proceedings of the 38th {International} {Conference} on {Software} {Engineering}},\\npublisher = {ACM},\\nauthor = {Manotas, Irene and Bird, Christian and Zhang, Rui and Shepherd, David and Jaspan, Ciera and Sadowski, Caitlin and Pollock, Lori and Clause, James},\\nmonth = may,\\nyear = {2016},\\npages = {237--248},\\nfile = {Full text:C\\\\:\\\\\\\\Users\\\\\\\\alice\\\\\\\\Zotero\\\\\\\\storage\\\\\\\\SEYJN25E\\\\\\\\Manotas et al. - 2016 - An empirical study of practitioners\\' perspectives .pdf:application/pdf},\\n}\\n\\n@inproceedings{verdecchia_data-centric_2022,\\naddress = {Plovdiv, Bulgaria},\\ntitle = {Data-{Centric} {Green} {AI} {An} {Exploratory} {Empirical} {Study}},\\nisbn = {978-1-66548-286-8},\\nurl = {https://ieeexplore.ieee.org/document/9830097/},\\ndoi = {10.1109/ICT4S55073.2022.00015},\\nurldate = {2023-06-28},\\nbooktitle = {2022 {International} {Conference} on {ICT} for {Sustainability} ({ICT4S})},\\npublisher = {IEEE},\\nauthor = {Verdecchia, Roberto and Cruz, Luis and Sallou, June and Lin, Michelle and Wickenden, James and Hotellier, Estelle},\\nmonth = jun,\\nyear = {2022},\\npages = {35--45},\\nfile = {Full text:C\\\\:\\\\\\\\Users\\\\\\\\alice\\\\\\\\Zotero\\\\\\\\storage\\\\\\\\KR37VN3E\\\\\\\\Verdecchia et al. - 2022 - Data-Centric Green AI An Exploratory Empirical Stu.pdf:application/pdf},\\n}\\n\\n\\n\\n@article{vinuesa_role_2020,\\ntitle = {The role of artificial intelligence in achieving the {Sustainable} {Development} {Goals}},\\nvolume = {11},\\nissn = {2041-1723},\\nurl = {https://www.nature.com/articles/s41467-019-14108-y},\\ndoi = {10.1038/s41467-019-14108-y},\\nabstract = {Abstract\\nThe emergence of artificial intelligence (AI) and its progressively wider impact on many sectors requires an assessment of its effect on the achievement of the Sustainable Development Goals. Using a consensus-based expert elicitation process, we find that AI can enable the accomplishment of 134 targets across all the goals, but it may also inhibit 59 targets. However, current research foci overlook important aspects. The fast development of AI needs to be supported by the necessary regulatory insight and oversight for AI-based technologies to enable sustainable development. Failure to do so could result in gaps in transparency, safety, and ethical standards.},\\nlanguage = {en},\\nnumber = {1},\\nurldate = {2023-06-28},\\njournal = {Nature Communications},\\nauthor = {Vinuesa, Ricardo and Azizpour, Hossein and Leite, Iolanda and Balaam, Madeline and Dignum, Virginia and Domisch, Sami and Felländer, Anna and Langhans, Simone Daniela and Tegmark, Max and Fuso Nerini, Francesco},\\nmonth = jan,\\nyear = {2020},\\npages = {233},\\nfile = {Full text:C\\\\:\\\\\\\\Users\\\\\\\\alice\\\\\\\\Zotero\\\\\\\\storage\\\\\\\\QDT5YBMJ\\\\\\\\Vinuesa et al. - 2020 - The role of artificial intelligence in achieving t.pdf:application/pdf},\\n}\\n\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bib_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bib_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['% First Draft of a Report on the EDVAC',\n",
       " '% DOI da IEEE del 1993: https://ieeexplore.ieee.org/document/238389',\n",
       " '@article{von_neumann_first_1993,',\n",
       " 'title = {First draft of a report on the {EDVAC}},',\n",
       " 'volume = {15},',\n",
       " 'issn = {1058-6180},',\n",
       " 'url = {http://ieeexplore.ieee.org/document/238389/},',\n",
       " 'doi = {10.1109/85.238389},',\n",
       " 'number = {4},',\n",
       " 'urldate = {2023-02-16},',\n",
       " 'journal = {IEEE Annals of the History of Computing},',\n",
       " 'author = {von Neumann, J.},',\n",
       " 'year = {1993},',\n",
       " 'pages = {27--75},',\n",
       " '}',\n",
       " '',\n",
       " '@article{talati2020mmpu,',\n",
       " 'title={mmpu—a real processing-in-memory architecture to combat the von neumann bottleneck},',\n",
       " 'author={Talati, Nishil and Ben-Hur, Rotem and Wald, Nimrod and Haj-Ali, Ameer and Reuben, John and Kvatinsky, Shahar},',\n",
       " 'journal={Applications of Emerging Memory Technology: Beyond Storage},',\n",
       " 'pages={191--213},',\n",
       " 'year={2020},',\n",
       " 'publisher={Springer}',\n",
       " '}',\n",
       " '',\n",
       " '% Schwartz Green AI (primissimo articolo su green ai, 2019)',\n",
       " \"% nell'introduzione\",\n",
       " '@misc{schwartz_green_2019,',\n",
       " 'title = {Green {AI}},',\n",
       " 'url = {http://arxiv.org/abs/1907.10597},',\n",
       " 'abstract = {The computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018 [2]. These computations have a surprisingly large carbon footprint [38]. Ironically, deep learning was inspired by the human brain, which is remarkably energy efficient. Moreover, the financial cost of the computations can make it difficult for academics, students, and researchers, in particular those from emerging economies, to engage in deep learning research. This position paper advocates a practical solution by making efficiency an evaluation criterion for research alongside accuracy and related measures. In addition, we propose reporting the financial cost or \"price tag\" of developing, training, and running models to provide baselines for the investigation of increasingly efficient methods. Our goal is to make AI both greener and more inclusive---enabling any inspired undergraduate with a laptop to write high-quality research papers. Green AI is an emerging focus at the Allen Institute for AI.},',\n",
       " 'urldate = {2023-02-08},',\n",
       " 'publisher = {arXiv},',\n",
       " 'author = {Schwartz, Roy and Dodge, Jesse and Smith, Noah A. and Etzioni, Oren},',\n",
       " 'month = {aug},',\n",
       " 'year = {2019},',\n",
       " 'note = {arXiv:1907.10597 [cs, stat]},',\n",
       " 'keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computers and Society, Computer Science - Machine Learning, Statistics - Methodology},',\n",
       " '} ',\n",
       " '',\n",
       " '',\n",
       " '% Schwartz Green AI 2020 (trovato 8-9/2)',\n",
       " \"% nell'introduzione\",\n",
       " '@article{schwartz_green_2020,',\n",
       " 'title = {Green {AI}},',\n",
       " 'volume = {63},',\n",
       " 'issn = {0001-0782, 1557-7317},',\n",
       " 'url = {https://dl.acm.org/doi/10.1145/3381831},',\n",
       " 'doi = {10.1145/3381831},',\n",
       " 'abstract = {Creating efficiency in AI research will decrease its carbon footprint and increase its inclusivity as deep learning study should not require the deepest pockets.},',\n",
       " 'language = {en},',\n",
       " 'number = {12},',\n",
       " 'urldate = {2023-02-08},',\n",
       " 'journal = {Communications of the ACM},',\n",
       " 'author = {Schwartz, Roy and Dodge, Jesse and Smith, Noah A. and Etzioni, Oren},',\n",
       " 'month = {nov},',\n",
       " 'year = {2020},',\n",
       " 'pages = {54--63},',\n",
       " 'file = {Testo integrale:C\\\\:\\\\\\\\Users\\\\\\\\alice\\\\\\\\Zotero\\\\\\\\storage\\\\\\\\N4U29MDB\\\\\\\\Schwartz et al. - 2020 - Green AI.pdf:application/pdf},',\n",
       " '}',\n",
       " '',\n",
       " '',\n",
       " '% Energy and Policy Considerations for Deep Learning in NLP',\n",
       " \"% nell'introduzione\",\n",
       " '@article{strubell_energy_2019,',\n",
       " 'title = {Energy and {Policy} {Considerations} for {Deep} {Learning} in {NLP}},',\n",
       " 'copyright = {arXiv.org perpetual, non-exclusive license},',\n",
       " 'url = {https://arxiv.org/abs/1906.02243},',\n",
       " 'doi = {10.48550/ARXIV.1906.02243},',\n",
       " 'urldate = {2023-02-09},',\n",
       " 'author = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},',\n",
       " 'year = {2019},',\n",
       " 'note = {Publisher: arXiv Version Number: 1},',\n",
       " 'keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},',\n",
       " '}',\n",
       " '',\n",
       " '',\n",
       " '@article{poerner2020inexpensive,',\n",
       " 'title={Inexpensive domain adaptation of pretrained language models: Case studies on biomedical NER and covid-19 QA},',\n",
       " 'author={Poerner, Nina and Waltinger, Ulli and Sch{\\\\\"u}tze, Hinrich},',\n",
       " 'journal={arXiv preprint arXiv:2004.03354},',\n",
       " 'year={2020}',\n",
       " '}',\n",
       " '',\n",
       " '',\n",
       " '% Articolo corto leggero su usi vari ai (es. agricoltura...), uno dei prmiissimi.',\n",
       " '% per introduzione',\n",
       " '@article{pachot_towards_2022,',\n",
       " 'title = {Towards {Sustainable} {Artificial} {Intelligence}: {An} {Overview} of {Environmental} {Protection} {Uses} and {Issues}},',\n",
       " 'copyright = {arXiv.org perpetual, non-exclusive license},',\n",
       " 'shorttitle = {Towards {Sustainable} {Artificial} {Intelligence}},',\n",
       " 'url = {https://arxiv.org/abs/2212.11738},',\n",
       " 'doi = {10.48550/ARXIV.2212.11738},',\n",
       " 'abstract = {Artificial Intelligence (AI) is used to create more sustainable production methods and model climate change, making it a valuable tool in the fight against environmental degradation. This paper describes the paradox of an energy-consuming technology serving the ecological challenges of tomorrow. The study provides an overview of the sectors that use AI-based solutions for environmental protection. It draws on numerous examples from AI for Green players to present use cases and concrete examples. In the second part of the study, the negative impacts of AI on the environment and the emerging technological solutions to support Green AI are examined. It is also shown that the research on less energy-consuming AI is motivated more by cost and energy autonomy constraints than by environmental considerations. This leads to a rebound effect that favors an increase in the complexity of models. Finally, the need to integrate environmental indicators into algorithms is discussed. The environmental dimension is part of the broader ethical problem of AI, and addressing it is crucial for ensuring the sustainability of AI in the long term.},',\n",
       " 'urldate = {2023-02-10},',\n",
       " 'author = {Pachot, Arnault and Patissier, Céline},',\n",
       " 'year = {2022},',\n",
       " 'note = {Publisher: arXiv',\n",
       " 'Version Number: 1},',\n",
       " 'keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences},',\n",
       " '}',\n",
       " '',\n",
       " \"% nell'introduzione\",\n",
       " '% red ai',\n",
       " '@article{xu_survey_2021,',\n",
       " 'title = {A {Survey} on {Green} {Deep} {Learning}},',\n",
       " 'copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},',\n",
       " 'url = {https://arxiv.org/abs/2111.05193},',\n",
       " 'doi = {10.48550/ARXIV.2111.05193},',\n",
       " 'abstract = {In recent years, larger and deeper models are springing up and continuously pushing state-of-the-art (SOTA) results across various fields like natural language processing (NLP) and computer vision (CV). However, despite promising results, it needs to be noted that the computations required by SOTA models have been increased at an exponential rate. Massive computations not only have a surprisingly large carbon footprint but also have negative effects on research inclusiveness and deployment on real-world applications. Green deep learning is an increasingly hot research field that appeals to researchers to pay attention to energy usage and carbon emission during model training and inference. The target is to yield novel results with lightweight and efficient technologies. Many technologies can be used to achieve this goal, like model compression and knowledge distillation. This paper focuses on presenting a systematic review of the development of Green deep learning technologies. We classify these approaches into four categories: (1) compact networks, (2) energy-efficient training strategies, (3) energy-efficient inference approaches, and (4) efficient data usage. For each category, we discuss the progress that has been achieved and the unresolved challenges.},',\n",
       " 'urldate = {2023-02-20},',\n",
       " 'author = {Xu, Jingjing and Zhou, Wangchunshu and Fu, Zhiyi and Zhou, Hao and Li, Lei},',\n",
       " 'year = {2021},',\n",
       " 'note = {Publisher: arXiv',\n",
       " 'Version Number: 2},',\n",
       " 'keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG)},',\n",
       " '}',\n",
       " '@article{lin2022survey,',\n",
       " 'title={A survey of transformers},',\n",
       " 'author={Lin, Tianyang and Wang, Yuxin and Liu, Xiangyang and Qiu, Xipeng},',\n",
       " 'journal={AI Open},',\n",
       " 'year={2022},',\n",
       " 'publisher={Elsevier}',\n",
       " '}',\n",
       " '',\n",
       " '@article{kirmani2022artificial,',\n",
       " 'title={Artificial Intelligence-Enabled Science Poetry},',\n",
       " 'author={Kirmani, Ahmad R},',\n",
       " 'journal={ACS Energy Letters},',\n",
       " 'volume={8},',\n",
       " 'pages={574--576},',\n",
       " 'year={2022},',\n",
       " 'publisher={ACS Publications}',\n",
       " '}',\n",
       " '',\n",
       " '@article{goodfellow2020generative,',\n",
       " 'title={Generative adversarial networks},',\n",
       " 'author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},',\n",
       " 'journal={Communications of the ACM},',\n",
       " 'volume={63},',\n",
       " 'number={11},',\n",
       " 'pages={139--144},',\n",
       " 'year={2020},',\n",
       " 'publisher={ACM New York, NY, USA}',\n",
       " '}',\n",
       " '',\n",
       " '@article{brock2018large,',\n",
       " 'title={Large scale GAN training for high fidelity natural image synthesis},',\n",
       " 'author={Brock, Andrew and Donahue, Jeff and Simonyan, Karen},',\n",
       " 'journal={arXiv preprint arXiv:1809.11096},',\n",
       " 'year={2018}',\n",
       " '}',\n",
       " '',\n",
       " '@article{surameery2023use,',\n",
       " 'title={Use chat gpt to solve programming bugs},',\n",
       " 'author={Surameery, Nigar M Shafiq and Shakor, Mohammed Y},',\n",
       " 'journal={International Journal of Information Technology \\\\& Computer Engineering (IJITC) ISSN: 2455-5290},',\n",
       " 'volume={3},',\n",
       " 'number={01},',\n",
       " 'pages={17--22},',\n",
       " 'year={2023}',\n",
       " '}',\n",
       " '',\n",
       " '@article{wu2023brief,',\n",
       " 'title={A brief overview of ChatGPT: The history, status quo and potential future development},',\n",
       " 'author={Wu, Tianyu and He, Shizhu and Liu, Jingping and Sun, Siqi and Liu, Kang and Han, Qing-Long and Tang, Yang},',\n",
       " 'journal={IEEE/CAA Journal of Automatica Sinica},',\n",
       " 'volume={10},',\n",
       " 'number={5},',\n",
       " 'pages={1122--1136},',\n",
       " 'year={2023},',\n",
       " 'publisher={IEEE}',\n",
       " '}',\n",
       " \"% nell'introduzione\",\n",
       " '% red ai',\n",
       " '@inproceedings{georgiou_green_2022,',\n",
       " 'address = {Pittsburgh Pennsylvania},',\n",
       " 'title = {Green {AI}: do deep learning frameworks have different costs?},',\n",
       " 'isbn = {978-1-4503-9221-1},',\n",
       " 'shorttitle = {Green {AI}},',\n",
       " 'url = {https://dl.acm.org/doi/10.1145/3510003.3510221},',\n",
       " 'doi = {10.1145/3510003.3510221},',\n",
       " 'language = {en},',\n",
       " 'urldate = {2023-02-20},',\n",
       " 'booktitle = {Proceedings of the 44th {International} {Conference} on {Software} {Engineering}},',\n",
       " 'publisher = {ACM},',\n",
       " 'author = {Georgiou, Stefanos and Kechagia, Maria and Sharma, Tushar and Sarro, Federica and Zou, Ying},',\n",
       " 'month = {may},',\n",
       " 'year = {2022},',\n",
       " 'pages = {1082--1094},',\n",
       " '}',\n",
       " '',\n",
       " \"% nell'introduzione\",\n",
       " '% red ai',\n",
       " '% bib preso da ieee (copiato da online, non usato zotero)',\n",
       " '@ARTICLE{9810097,',\n",
       " 'author={Patterson, David and Gonzalez, Joseph and Hölzle, Urs and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David R. and Texier, Maud and Dean, Jeff},',\n",
       " 'journal={Computer}, ',\n",
       " 'title={The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink}, ',\n",
       " 'year={2022},',\n",
       " 'volume={55},',\n",
       " 'number={7},',\n",
       " 'pages={18-28},',\n",
       " 'doi={10.1109/MC.2022.3148714}}',\n",
       " '',\n",
       " '% Deep Learning',\n",
       " '@article{lecun_deep_2015,',\n",
       " 'title = {Deep learning},',\n",
       " 'volume = {521},',\n",
       " 'issn = {0028-0836, 1476-4687},',\n",
       " 'url = {http://www.nature.com/articles/nature14539},',\n",
       " 'doi = {10.1038/nature14539},',\n",
       " 'language = {en},',\n",
       " 'number = {7553},',\n",
       " 'urldate = {2023-02-28},',\n",
       " 'journal = {Nature},',\n",
       " 'author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},',\n",
       " 'month = {may},',\n",
       " 'year = {2015},',\n",
       " 'pages = {436--444},',\n",
       " '}',\n",
       " '',\n",
       " '% Reinforcement learning',\n",
       " '@article{10.1145/3459991,',\n",
       " 'author = {Padakandla, Sindhu},',\n",
       " 'title = {A Survey of Reinforcement Learning Algorithms for Dynamically Varying Environments},',\n",
       " 'year = {2021},',\n",
       " 'issue_date = {July 2022},',\n",
       " 'publisher = {Association for Computing Machinery},',\n",
       " 'address = {New York, NY, USA},',\n",
       " 'volume = {54},',\n",
       " 'number = {6},',\n",
       " 'issn = {0360-0300},',\n",
       " 'url = {https://doi.org/10.1145/3459991},',\n",
       " 'doi = {10.1145/3459991},',\n",
       " 'month = {jul},',\n",
       " 'articleno = {127},',\n",
       " 'numpages = {25},',\n",
       " 'keywords = {Reinforcement learning, sequential decision-making, context detection, meta-learning, non-stationary environments, regret computation, Markov decision processes}',\n",
       " '}',\n",
       " '',\n",
       " '% GANS',\n",
       " '',\n",
       " '@article{wang_generative_2022,',\n",
       " 'title = {Generative {Adversarial} {Networks} in {Computer} {Vision}: {A} {Survey} and {Taxonomy}},',\n",
       " 'volume = {54},',\n",
       " 'issn = {0360-0300, 1557-7341},',\n",
       " 'shorttitle = {Generative {Adversarial} {Networks} in {Computer} {Vision}},',\n",
       " 'url = {https://dl.acm.org/doi/10.1145/3439723},',\n",
       " 'doi = {10.1145/3439723},',\n",
       " 'abstract = {Generative adversarial networks (GANs) have been extensively studied in the past few years. Arguably their most significant impact has been in the area of computer vision where great advances have been made in challenges such as plausible image generation, image-to-image translation, facial attribute manipulation, and similar domains. Despite the significant successes achieved to date, applying GANs to real-world problems still poses significant challenges, three of which we focus on here. These are as follows: (1) the generation of high quality images, (2) diversity of image generation, and (3) stabilizing training. Focusing on the degree to which popular GAN technologies have made progress against these challenges, we provide a detailed review of the state-of-the-art in GAN-related research in the published scientific literature. We further structure this review through a convenient taxonomy we have adopted based on variations in GAN architectures and loss functions. While several reviews for GANs have been presented to date, none have considered the status of this field based on their progress toward addressing practical challenges relevant to computer vision. Accordingly, we review and critically discuss the most popular architecture-variant, and loss-variant GANs, for tackling these challenges. Our objective is to provide an overview as well as a critical analysis of the status of GAN research in terms of relevant progress toward critical computer vision application requirements. As we do this we also discuss the most compelling applications in computer vision in which GANs have demonstrated considerable success along with some suggestions for future research directions. Codes related to the GAN-variants studied in this work is summarized on https://github.com/sheqi/GAN\\\\_Review.},',\n",
       " 'language = {en},',\n",
       " 'number = {2},',\n",
       " 'urldate = {2023-02-28},',\n",
       " 'journal = {ACM Computing Surveys},',\n",
       " 'author = {Wang, Zhengwei and She, Qi and Ward, Tomás E.},',\n",
       " 'month = {mar},',\n",
       " 'year = {2022},',\n",
       " 'pages = {1--38},',\n",
       " 'file = {Full text:C\\\\:\\\\\\\\Users\\\\\\\\Enric\\\\\\\\Zotero\\\\\\\\storage\\\\\\\\E25BXL2D\\\\\\\\Wang et al. - 2022 - Generative Adversarial Networks in Computer Vision.pdf:application/pdf},',\n",
       " '}',\n",
       " '',\n",
       " '% Transfer learning',\n",
       " '@Article{math10193619,',\n",
       " 'AUTHOR = {Yu, Fuchao and Xiu, Xianchao and Li, Yunhui},',\n",
       " 'TITLE = {A Survey on Deep Transfer Learning and Beyond},',\n",
       " 'JOURNAL = {Mathematics},',\n",
       " 'VOLUME = {10},',\n",
       " 'YEAR = {2022},',\n",
       " 'NUMBER = {19},',\n",
       " 'ARTICLE-NUMBER = {3619},',\n",
       " 'URL = {https://www.mdpi.com/2227-7390/10/19/3619},',\n",
       " 'ISSN = {2227-7390},',\n",
       " 'ABSTRACT = {Deep transfer learning (DTL), which incorporates new ideas from deep neural networks into transfer learning (TL), has achieved excellent success in computer vision, text classification, behavior recognition, and natural language processing. As a branch of machine learning, DTL applies end-to-end learning to overcome the drawback of traditional machine learning that regards each dataset individually. Although some valuable and impressive general surveys exist on TL, special attention and recent advances in DTL are lacking. In this survey, we first review more than 50 representative approaches of DTL in the last decade and systematically summarize them into four categories. In particular, we further divide each category into subcategories according to models, functions, and operation objects. In addition, we discuss recent advances in TL in other fields and unsupervised TL. Finally, we provide some possible and exciting future research directions.},',\n",
       " 'DOI = {10.3390/math10193619}',\n",
       " '}',\n",
       " '',\n",
       " '% AutoML',\n",
       " '@article{automl,',\n",
       " 'author = {He, Xin and Zhao, Kaiyong and Chu, Xiaowen},',\n",
       " 'year = {2021},',\n",
       " 'month = {01},',\n",
       " 'pages = {106622},',\n",
       " 'title = {AutoML: A survey of the state-of-the-art},',\n",
       " 'volume = {212},',\n",
       " 'journal = {Knowledge-Based Systems},',\n",
       " 'doi = {10.1016/j.knosys.2020.106622}',\n",
       " '}',\n",
       " '',\n",
       " '% Von Neumann bottleneck',\n",
       " '@article{',\n",
       " 'backus_can_1978,',\n",
       " 'title = {Can programming be liberated from the von {Neumann} style?: a functional style and its algebra of programs},',\n",
       " 'volume = {21},',\n",
       " 'issn = {0001-0782, 1557-7317},',\n",
       " 'shorttitle = {Can programming be liberated from the von {Neumann} style?},',\n",
       " 'url = {https://dl.acm.org/doi/10.1145/359576.359579},',\n",
       " 'doi = {10.1145/359576.359579},',\n",
       " 'language = {en},',\n",
       " 'number = {8},',\n",
       " 'urldate = {2023-02-28},',\n",
       " 'journal = {Communications of the ACM},',\n",
       " 'author = {Backus, John},',\n",
       " 'month = {aug},',\n",
       " 'year = {1978},',\n",
       " 'pages = {613--641}',\n",
       " '}',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '@article{vaswani2017attention,',\n",
       " 'title={Attention is all you need},',\n",
       " 'author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\\\\L}ukasz and Polosukhin, Illia},',\n",
       " 'journal={Advances in neural information processing systems},',\n",
       " 'volume={30},',\n",
       " 'year={2017}',\n",
       " '}',\n",
       " '',\n",
       " '% Connection machine',\n",
       " '@article{kahle1989connection,',\n",
       " 'title={The connection machine model cm-1 architecture},',\n",
       " 'author={Kahle, Brewster A and Hillis, W Daniel},',\n",
       " 'journal={IEEE transactions on systems, man, and cybernetics},',\n",
       " 'volume={19},',\n",
       " 'number={4},',\n",
       " 'pages={707--713},',\n",
       " 'year={1989},',\n",
       " 'publisher={IEEE}',\n",
       " '}',\n",
       " '',\n",
       " '% PCA',\n",
       " '@article{pearson_liii_1901,',\n",
       " 'title = {{LIII}. \\\\textit{{On} lines and planes of closest fit to systems of points in space}},',\n",
       " 'volume = {2},',\n",
       " 'issn = {1941-5982, 1941-5990},',\n",
       " 'url = {https://www.tandfonline.com/doi/full/10.1080/14786440109462720},',\n",
       " 'doi = {10.1080/14786440109462720},',\n",
       " 'language = {en},',\n",
       " 'number = {11},',\n",
       " 'urldate = {2023-02-28},',\n",
       " 'journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},',\n",
       " 'author = {Pearson, Karl},',\n",
       " 'month = {nov},',\n",
       " 'year = {1901},',\n",
       " 'pages = {559--572},',\n",
       " 'file = {Versione inviata:C\\\\:\\\\\\\\Users\\\\\\\\Enric\\\\\\\\Zotero\\\\\\\\storage\\\\\\\\RM7USJ9H\\\\\\\\Pearson - 1901 - LIII. On lines and planes of closest fit to sys.pdf:application/pdf},',\n",
       " '}',\n",
       " '% autoencoders',\n",
       " '@article{kramer_nonlinear_1991,',\n",
       " 'title = {Nonlinear principal component analysis using autoassociative neural networks},',\n",
       " 'volume = {37},',\n",
       " 'issn = {0001-1541, 1547-5905},',\n",
       " 'url = {https://onlinelibrary.wiley.com/doi/10.1002/aic.690370209},',\n",
       " 'doi = {10.1002/aic.690370209},',\n",
       " 'language = {en},',\n",
       " 'number = {2},',\n",
       " 'urldate = {2023-02-28},',\n",
       " 'journal = {AIChE Journal},',\n",
       " 'author = {Kramer, Mark A.},',\n",
       " 'month = {feb},',\n",
       " 'year = {1991},',\n",
       " 'pages = {233--243},',\n",
       " '}',\n",
       " '',\n",
       " '% CNN',\n",
       " '@article{CNN,',\n",
       " 'author = {Hasegawa, Akira and Lo, Shih-Chung and Lin, Jyh-Shyan and Freedman, Matthew and Mun, Seong},',\n",
       " 'year = {1998},',\n",
       " 'month = {04},',\n",
       " 'pages = {241-250},',\n",
       " 'title = {A Shift-Invariant Neural Network for the Lung Field Segmentation in Chest Radiography},',\n",
       " 'volume = {18},',\n",
       " 'journal = {VLSI Signal Processing},',\n",
       " 'doi = {10.1023/A:1007937214367}',\n",
       " '}',\n",
       " '',\n",
       " '% RNN',\n",
       " '@article{RNN,',\n",
       " 'title = {A {Novel} {Connectionist} {System} for {Unconstrained} {Handwriting} {Recognition}},',\n",
       " 'volume = {31},',\n",
       " 'issn = {0162-8828},',\n",
       " 'url = {http://ieeexplore.ieee.org/document/4531750/},',\n",
       " 'doi = {10.1109/TPAMI.2008.137},',\n",
       " 'number = {5},',\n",
       " 'urldate = {2023-02-28},',\n",
       " 'journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},',\n",
       " 'author = {Graves, A. and Liwicki, M. and Fernandez, S. and Bertolami, R. and Bunke, H. and Schmidhuber, J.},',\n",
       " 'month = {may},',\n",
       " 'year = {2009},',\n",
       " 'pages = {855--868},',\n",
       " 'file = {Versione inviata:C\\\\:\\\\\\\\Users\\\\\\\\Enric\\\\\\\\Zotero\\\\\\\\storage\\\\\\\\FDNR7EB8\\\\\\\\Graves et al. - 2009 - A Novel Connectionist System for Unconstrained Han.pdf:application/pdf},',\n",
       " '}',\n",
       " '',\n",
       " '',\n",
       " '@article{LSTM,',\n",
       " 'title = {Long {Short}-{Term} {Memory}},',\n",
       " 'volume = {9},',\n",
       " 'issn = {0899-7667, 1530-888X},',\n",
       " 'url = {https://direct.mit.edu/neco/article/9/8/1735-1780/6109},',\n",
       " 'doi = {10.1162/neco.1997.9.8.1735},',\n",
       " \"abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},\",\n",
       " 'language = {en},',\n",
       " 'number = {8},',\n",
       " 'urldate = {2023-02-28},',\n",
       " 'journal = {Neural Computation},',\n",
       " 'author = {Hochreiter, Sepp and Schmidhuber, Jürgen},',\n",
       " 'month = {nov},',\n",
       " 'year = {1997},',\n",
       " 'pages = {1735--1780},',\n",
       " '}',\n",
       " '% Definizione di \"carbon footprint\"',\n",
       " '@article{wiedmann2008definition,',\n",
       " 'title={A definition of ‘carbon footprint’},',\n",
       " 'author={Wiedmann, Thomas and Minx, Jan},',\n",
       " 'journal={Ecological economics research trends},',\n",
       " 'volume={1},',\n",
       " 'number={2008},',\n",
       " 'pages={1--11},',\n",
       " 'year={2008},',\n",
       " 'publisher={Nova Science Publishers Hauppauge, NY}',\n",
       " '}',\n",
       " '',\n",
       " '% Articolo che descrive curse of dimensionality. Bellman 1962',\n",
       " '@article{hammer_adaptive_1962,',\n",
       " 'title = {Adaptive {Control} {Processes}: {A} {Guided} {Tour} ({R}. {Bellman})},',\n",
       " 'volume = {4},',\n",
       " 'issn = {0036-1445, 1095-7200},',\n",
       " 'shorttitle = {Adaptive {Control} {Processes}},',\n",
       " 'url = {http://epubs.siam.org/doi/10.1137/1004050},',\n",
       " 'doi = {10.1137/1004050},',\n",
       " 'language = {en},',\n",
       " 'number = {2},',\n",
       " 'urldate = {2023-03-20},',\n",
       " 'journal = {SIAM Review},',\n",
       " 'author = {Hammer, P. C.},',\n",
       " 'month = {apr},',\n",
       " 'year = {1962},',\n",
       " 'pages = {163--163},',\n",
       " '}',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '@article{noauthor_measuring_2022,',\n",
       " 'author = {OECD},',\n",
       " 'title = {Measuring the environmental impacts of artificial intelligence compute and applications},',\n",
       " 'year = {2022},',\n",
       " 'number = {341},',\n",
       " 'url = {https://www.oecd-ilibrary.org/content/paper/7babf571-en},',\n",
       " 'doi = {https://doi.org/https://doi.org/10.1787/7babf571-en}',\n",
       " '}',\n",
       " '',\n",
       " '',\n",
       " '% Operational energy costs',\n",
       " '@article{crawford2018anatomy,',\n",
       " 'title={Anatomy of an AI System},',\n",
       " 'author={Crawford, Kate and Joler, Vladan},',\n",
       " 'journal={Retrieved September},',\n",
       " 'volume={18},',\n",
       " 'pages={2018},',\n",
       " 'year={2018}',\n",
       " '}',\n",
       " '',\n",
       " '% BERT',\n",
       " '@Article{Devlin2018_bert,',\n",
       " 'author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},',\n",
       " 'title = {Bert: Pre-training of deep bidirectional transformers for language understanding},',\n",
       " 'journal = {arXiv preprint arXiv:1810.04805},',\n",
       " 'year = {2018},',\n",
       " '}',\n",
       " '',\n",
       " '',\n",
       " '% History of NLP',\n",
       " '@Inbook{Jones1994,',\n",
       " 'author={Jones, Karen Sparck},',\n",
       " 'title={Natural Language Processing: A Historical Review},',\n",
       " 'bookTitle={Current Issues in Computational Linguistics: In Honour of Don Walker},',\n",
       " 'year={1994},',\n",
       " 'publisher={Springer Netherlands},',\n",
       " 'address={Dordrecht},',\n",
       " 'pages={3--16}',\n",
       " '}',\n",
       " '',\n",
       " '% Shank (MT)',\n",
       " '@incollection{Schank75,',\n",
       " 'AUTHOR = {R. C. Schank},',\n",
       " 'TITLE = {Conceptual Dependency Theory},',\n",
       " 'YEAR = {1975},',\n",
       " 'BOOKTITLE = {Conceptual Information Processing},',\n",
       " 'EDITOR = {R. C. Schank},',\n",
       " 'PUBLISHER = {North-Holland and Elsevier},',\n",
       " 'ADDRESS = {Amsterdam and New York},',\n",
       " 'PAGES = {22-82}, ',\n",
       " 'KEYWORDS = {},',\n",
       " 'ANNOTE ={ The book presents the Conceptual Dependency Theory which can be used by a machine for conceptual understanding of the given text.}',\n",
       " '}',\n",
       " '',\n",
       " '@inproceedings{johri2021natural,',\n",
       " 'title={Natural language processing: History, evolution, application, and future work},',\n",
       " 'author={Johri, Prashant and Khatri, Sunil K and Al-Taani, Ahmad T and Sabharwal, Munish and Suvanov, Shakhzod and Kumar, Avneesh},',\n",
       " 'booktitle={Proceedings of 3rd International Conference on Computing Informatics and Networks: ICCIN 2020},',\n",
       " 'pages={365--375},',\n",
       " 'year={2021},',\n",
       " 'organization={Springer}',\n",
       " '}',\n",
       " '',\n",
       " '% XLNet',\n",
       " '@misc{https://doi.org/10.48550/arxiv.1906.08237,',\n",
       " 'doi = {10.48550/ARXIV.1906.08237},',\n",
       " 'url = {https://arxiv.org/abs/1906.08237},',\n",
       " 'author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},',\n",
       " 'keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},',\n",
       " 'title = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},',\n",
       " 'publisher = {arXiv},',\n",
       " 'year = {2019},',\n",
       " 'copyright = {Creative Commons Attribution 4.0 International}',\n",
       " '}',\n",
       " '',\n",
       " '@article{raffel2020exploring,',\n",
       " 'title={Exploring the limits of transfer learning with a unified text-to-text transformer},',\n",
       " 'author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},',\n",
       " 'journal={The Journal of Machine Learning Research},',\n",
       " 'volume={21},',\n",
       " 'number={1},',\n",
       " 'pages={5485--5551},',\n",
       " 'year={2020},',\n",
       " 'publisher={JMLRORG}',\n",
       " '}',\n",
       " '',\n",
       " '% Come funziona BERT',\n",
       " '@article{rogers2021primer,',\n",
       " 'title={A primer in BERTology: What we know about how BERT works},',\n",
       " 'author={Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},',\n",
       " 'journal={Transactions of the Association for Computational Linguistics},',\n",
       " 'volume={8},',\n",
       " 'pages={842--866},',\n",
       " 'year={2021},',\n",
       " 'publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}',\n",
       " '}',\n",
       " '',\n",
       " '% Proposta di training HW per BERT',\n",
       " '@misc{https://doi.org/10.48550/arxiv.2008.00177,',\n",
       " 'doi = {10.48550/ARXIV.2008.00177},',\n",
       " 'url = {https://arxiv.org/abs/2008.00177},',\n",
       " 'author = {Lin, Jiahuang and Li, Xin and Pekhimenko, Gennady},',\n",
       " 'title = {Multi-node Bert-pretraining: Cost-efficient Approach},',\n",
       " 'publisher = {arXiv},',\n",
       " 'year = {2020},',\n",
       " 'copyright = {arXiv.org perpetual, non-exclusive license}',\n",
       " '}',\n",
       " '',\n",
       " '',\n",
       " '%%%%%%%%%%%%%%%%%%%%%%%',\n",
       " '% APPROACHES',\n",
       " '%%%%%%%%%%%%%%%%%%%%%%',\n",
       " '% Miglioramento training di BERT',\n",
       " '@misc{https://doi.org/10.48550/arxiv.2101.00063,',\n",
       " 'doi = {10.48550/ARXIV.2101.00063},',\n",
       " 'url = {https://arxiv.org/abs/2101.00063},',\n",
       " 'author = {Chen, Xiaohan and Cheng, Yu and Wang, Shuohang and Gan, Zhe and Wang, Zhangyang and Liu, Jingjing},',\n",
       " 'keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},',\n",
       " 'title = {EarlyBERT: Efficient BERT Training via Early-bird Lottery Tickets},',\n",
       " 'publisher = {arXiv},',\n",
       " 'year = {2021},',\n",
       " 'copyright = {arXiv.org perpetual, non-exclusive license}',\n",
       " '}',\n",
       " '',\n",
       " '',\n",
       " '% Approaches. Algorithms',\n",
       " '% Parallel training',\n",
       " '@article{https://doi.org/10.48550/arxiv.1811.03600,',\n",
       " 'doi = {10.48550/ARXIV.1811.03600},',\n",
       " 'url = {https://arxiv.org/abs/1811.03600},',\n",
       " 'author = {Shallue, Christopher J. and Lee, Jaehoon and Antognini, Joseph and Sohl-Dickstein, Jascha and Frostig, Roy and Dahl, George E.},',\n",
       " 'keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},',\n",
       " 'title = {Measuring the Effects of Data Parallelism on Neural Network Training},',\n",
       " 'publisher = {arXiv},',\n",
       " 'year = {2018},',\n",
       " 'copyright = {arXiv.org perpetual, non-exclusive license}',\n",
       " '}',\n",
       " '% Parallel model',\n",
       " '@article{DBLP:journals/corr/Krizhevsky14,',\n",
       " 'author = {Alex Krizhevsky},',\n",
       " 'title = {One weird trick for parallelizing convolutional neural networks},',\n",
       " 'journal = {CoRR},',\n",
       " 'volume = {abs/1404.5997},',\n",
       " 'year = {2014},',\n",
       " 'url = {http://arxiv.org/abs/1404.5997},',\n",
       " 'eprinttype = {arXiv},',\n",
       " 'eprint = {1404.5997},',\n",
       " 'timestamp = {Mon, 13 Aug 2018 16:48:41 +0200},',\n",
       " 'biburl = {https://dblp.org/rec/journals/corr/Krizhevsky14.bib},',\n",
       " 'bibsource = {dblp computer science bibliography, https://dblp.org}',\n",
       " '}',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '% data structures - tensor',\n",
       " '@article{lewis2021large,',\n",
       " 'title={Large scale distributed linear algebra with tensor processing units},',\n",
       " 'author={Lewis, Adam GM and Beall, Jackson and Ganahl, Martin and Hauru, Markus and Mallick, Shrestha Basu and Vidal, Guifre},',\n",
       " 'journal={arXiv preprint arXiv:2112.09017},',\n",
       " 'year={2021}',\n",
       " '}',\n",
       " '',\n",
       " '@article{ji_survey_2019,',\n",
       " 'title = {A {Survey} on {Tensor} {Techniques} and {Applications} in {Machine} {Learning}},',\n",
       " 'volume = {7},',\n",
       " 'issn = {2169-3536},',\n",
       " 'url = {https://ieeexplore.ieee.org/document/8884203/},',\n",
       " 'doi = {10.1109/ACCESS.2019.2949814},',\n",
       " 'urldate = {2023-02-26},',\n",
       " 'journal = {IEEE Access},',\n",
       " 'author = {Ji, Yuwang and Wang, Qiang and Li, Xuan and Liu, Jie},',\n",
       " 'year = {2019},',\n",
       " 'pages = {162950--162990},',\n",
       " 'file = {Full text:C\\\\:\\\\\\\\Users\\\\\\\\alice\\\\\\\\Zotero\\\\\\\\storage\\\\\\\\JPVL3QXL\\\\\\\\Ji et al. - 2019 - A Survey on Tensor Techniques and Applications in .pdf:application/pdf},',\n",
       " '}',\n",
       " '',\n",
       " '% data structures - tensor',\n",
       " '@article{rabanser_introduction_2017,',\n",
       " 'title = {Introduction to {Tensor} {Decompositions} and their {Applications} in {Machine} {Learning}},',\n",
       " 'copyright = {arXiv.org perpetual, non-exclusive license},',\n",
       " 'url = {https://arxiv.org/abs/1711.10781},',\n",
       " 'doi = {10.48550/ARXIV.1711.10781},',\n",
       " 'abstract = {Tensors are multidimensional arrays of numerical values and therefore generalize matrices to multiple dimensions. While tensors first emerged in the psychometrics community in the \\\\$20{\\\\textasciicircum}\\\\{{\\\\textbackslash}text\\\\{th\\\\}\\\\}\\\\$ century, they have since then spread to numerous other disciplines, including machine learning. Tensors and their decompositions are especially beneficial in unsupervised learning settings, but are gaining popularity in other sub-disciplines like temporal and multi-relational data analysis, too. The scope of this paper is to give a broad overview of tensors, their decompositions, and how they are used in machine learning. As part of this, we are going to introduce basic tensor concepts, discuss why tensors can be considered more rigid than matrices with respect to the uniqueness of their decomposition, explain the most important factorization algorithms and their properties, provide concrete examples of tensor decomposition applications in machine learning, conduct a case study on tensor-based estimation of mixture models, talk about the current state of research, and provide references to available software libraries.},',\n",
       " 'urldate = {2023-02-26},',\n",
       " 'author = {Rabanser, Stephan and Shchur, Oleksandr and Günnemann, Stephan},',\n",
       " 'year = {2017},',\n",
       " 'note = {Publisher: arXiv',\n",
       " 'Version Number: 1},',\n",
       " 'keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML)},',\n",
       " '}',\n",
       " '',\n",
       " '% data structures - tensor',\n",
       " '@article{kolda_tensor_2009,',\n",
       " 'title = {Tensor {Decompositions} and {Applications}},',\n",
       " 'volume = {51},',\n",
       " 'issn = {0036-1445, 1095-7200},',\n",
       " 'url = {http://epubs.siam.org/doi/10.1137/07070111X},',\n",
       " 'doi = {10.1137/07070111X},',\n",
       " 'language = {en},',\n",
       " 'number = {3},',\n",
       " 'urldate = {2023-02-26},',\n",
       " 'journal = {SIAM Review},',\n",
       " 'author = {Kolda, Tamara G. and Bader, Brett W.},',\n",
       " 'month = {aug},',\n",
       " 'year = {2009},',\n",
       " 'pages = {455--500},',\n",
       " '}',\n",
       " '',\n",
       " '% data structures - tensor',\n",
       " '@article{bernardi_general_2013,',\n",
       " 'title = {General tensor decomposition, moment matrices and applications},',\n",
       " 'volume = {52},',\n",
       " 'issn = {0747-7171},',\n",
       " 'url = {https://www.sciencedirect.com/science/article/pii/S0747717112001290},',\n",
       " 'doi = {https://doi.org/10.1016/j.jsc.2012.05.012},',\n",
       " 'abstract = {The tensor decomposition addressed in this paper may be seen as a generalization of Singular Value Decomposition of matrices. We consider general multilinear and multihomogeneous tensors. We show how to reduce the problem to a truncated moment matrix problem and give a new criterion for flat extension of Quasi-Hankel matrices. We connect this criterion to the commutation characterization of border bases. A new algorithm is described. It applies for general multihomogeneous tensors, extending the approach on binary forms by J.J. Sylvester. An example illustrates the algebraic operations involved in this approach and how the decomposition can be recovered from eigenvector computation.},',\n",
       " 'journal = {Journal of Symbolic Computation},',\n",
       " 'author = {Bernardi, A. and Brachat, J. and Comon, P. and Mourrain, B.},',\n",
       " 'year = {2013},',\n",
       " 'keywords = {Decomposition, Flat extension, Hankel operator, Moment matrix, Multihomogeneous polynomial, Rank, Tensor},',\n",
       " 'pages = {51--71},',\n",
       " '}',\n",
       " '@article{10.5555/3455716.3455856,',\n",
       " 'author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},',\n",
       " 'title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},',\n",
       " 'year = {2020},',\n",
       " 'issue_date = {January 2020},',\n",
       " 'publisher = {JMLR.org},',\n",
       " 'volume = {21},',\n",
       " 'number = {1},',\n",
       " 'issn = {1532-4435},',\n",
       " 'abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pretraining objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},',\n",
       " 'journal = {J. Mach. Learn. Res.},',\n",
       " 'month = {jan},',\n",
       " 'articleno = {140},',\n",
       " 'numpages = {67},',\n",
       " 'keywords = {transfer learning, multi-task learning, deep learning, natural language processing, attention based models}',\n",
       " '}',\n",
       " '',\n",
       " '@misc{https://doi.org/10.48550/arxiv.1905.05583,',\n",
       " 'doi = {10.48550/ARXIV.1905.05583},',\n",
       " 'url = {https://arxiv.org/abs/1905.05583},',\n",
       " 'author = {Sun, Chi and Qiu, Xipeng and Xu, Yige and Huang, Xuanjing},',\n",
       " 'keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},',\n",
       " 'title = {How to Fine-Tune BERT for Text Classification?},',\n",
       " 'publisher = {arXiv},',\n",
       " 'year = {2019},',\n",
       " 'copyright = {arXiv.org perpetual, non-exclusive license}',\n",
       " '}',\n",
       " '',\n",
       " '@article{koutsoukou2021energy,',\n",
       " 'title={On the Energy Consumption of Large-Scale Language Models},',\n",
       " 'author={Koutsoukou-Argyraki, Angeliki and Ghosh, Debarghya and Arvind, V and Raffel, Colin},',\n",
       " 'journal={arXiv preprint arXiv:2102.06171},',\n",
       " 'year={2021}',\n",
       " '}',\n",
       " '',\n",
       " '',\n",
       " '@misc{https://doi.org/10.48550/arxiv.1802.05668,',\n",
       " 'doi = {10.48550/ARXIV.1802.05668}, ',\n",
       " 'url = {https://arxiv.org/abs/1802.05668}, ',\n",
       " 'author = {Polino, Antonio and Pascanu, Razvan and Alistarh, Dan}, ',\n",
       " 'keywords = {Neural and Evolutionary Computing (cs.NE), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences}, ',\n",
       " 'title = {Model compression via distillation and quantization},',\n",
       " 'publisher = {arXiv},',\n",
       " 'year = {2018},',\n",
       " 'copyright = {arXiv.org perpetual, non-exclusive license}',\n",
       " '}',\n",
       " '',\n",
       " '% Algorithms',\n",
       " '@article{mayer2020scalable,',\n",
       " 'title={Scalable deep learning on distributed infrastructures: Challenges, techniques, and tools},',\n",
       " 'author={Mayer, Ruben and Jacobsen, Hans-Arno},',\n",
       " 'journal={ACM Computing Surveys (CSUR)},',\n",
       " 'volume={53},',\n",
       " 'number={1},',\n",
       " 'pages={1--37},',\n",
       " 'year={2020},',\n",
       " 'publisher={ACM New York, NY, USA}',\n",
       " '}',\n",
       " '',\n",
       " '@article{gou_knowledge_2021,',\n",
       " 'title = {Knowledge {Distillation}: {A} {Survey}},',\n",
       " 'volume = {129},',\n",
       " 'issn = {0920-5691, 1573-1405},',\n",
       " 'shorttitle = {Knowledge {Distillation}},',\n",
       " 'url = {https://link.springer.com/10.1007/s11263-021-01453-z},',\n",
       " 'doi = {10.1007/s11263-021-01453-z},',\n",
       " 'language = {en},',\n",
       " 'number = {6},',\n",
       " 'urldate = {2023-03-16},',\n",
       " 'journal = {International Journal of Computer Vision},',\n",
       " 'author = {Gou, Jianping and Yu, Baosheng and Maybank, Stephen J. and Tao, Dacheng},',\n",
       " 'month = {jun},',\n",
       " 'year = {2021},',\n",
       " 'pages = {1789--1819},',\n",
       " 'file = {Versione accettata:C\\\\:\\\\\\\\Users\\\\\\\\Enric\\\\\\\\Zotero\\\\\\\\storage\\\\\\\\BFZPDPMN\\\\\\\\Gou et al. - 2021 - Knowledge Distillation A Survey.pdf:application/pdf},',\n",
       " '}',\n",
       " '',\n",
       " '% Residual connection',\n",
       " '@misc{he2015deep,',\n",
       " 'title={Deep Residual Learning for Image Recognition}, ',\n",
       " 'author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},',\n",
       " 'year={2015},',\n",
       " 'eprint={1512.03385},',\n",
       " 'archivePrefix={arXiv},',\n",
       " 'primaryClass={cs.CV}',\n",
       " '}',\n",
       " '',\n",
       " '% Depthwise convolutiion for MobiNet',\n",
       " '@article{howard2017mobilenets,',\n",
       " 'title={MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications},',\n",
       " 'author={Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},',\n",
       " 'journal={arXiv preprint arXiv:1704.04861},',\n",
       " 'year={2017}',\n",
       " '}',\n",
       " '',\n",
       " '% Weight sharing',\n",
       " '@article{lecun1989backpropagation,',\n",
       " 'title={Backpropagation applied to handwritten zip code recognition},',\n",
       " 'author={LeCun, Yann and Boser, Bernhard and Denker, John S and Henderson, Donnie and Howard, Richard E and Hubbard, Wayne and Jackel, Lawrence D},',\n",
       " 'journal={Neural computation},',\n",
       " 'volume={1},',\n",
       " 'number={4},',\n",
       " 'pages={541--551},',\n",
       " 'year={1989},',\n",
       " 'publisher={MIT Press}',\n",
       " '}',\n",
       " '',\n",
       " '% QUantisation',\n",
       " '@article{wu2016quantized,',\n",
       " 'title={Quantized convolutional neural networks for mobile devices},',\n",
       " 'author={Wu, Sheng and Liang, Hong and Zhang, Yiran and Sun, Rui and Wang, Ting and He, Xuming},',\n",
       " 'journal={arXiv preprint arXiv:1603.05279},',\n",
       " 'year={2016}',\n",
       " '}',\n",
       " '',\n",
       " '% Model binarization',\n",
       " '@inproceedings{courbariaux2015binaryconnect,',\n",
       " 'title={Binaryconnect: Training deep neural networks with binary weights during propagations},',\n",
       " 'author={Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},',\n",
       " 'booktitle={Advances in neural information processing systems},',\n",
       " 'pages={3123--3131},',\n",
       " 'year={2015}',\n",
       " '}',\n",
       " '',\n",
       " '% Structural matrices',\n",
       " '@inproceedings{sindhwani2015structured,',\n",
       " 'title={Structured transforms for small-footprint deep learning},',\n",
       " 'author={Sindhwani, Vikas and Sainath, Tara N and Kumar, Sanjiv},',\n",
       " 'booktitle={Proceedings of the 32nd International Conference on Machine Learning},',\n",
       " 'pages={1664--1672},',\n",
       " 'year={2015},',\n",
       " 'organization={JMLR Workshop and Conference Proceedings}',\n",
       " '}',\n",
       " '',\n",
       " '% Computer architectures',\n",
       " '% Hardware architecture',\n",
       " '',\n",
       " '% Questa citazione sembra svagliata! Non la uso',\n",
       " '@article{harvardarchitecture,',\n",
       " 'title={A Logical Calculus of Ideas Immanent in Nervous Activity},',\n",
       " 'author={McCulloch, Warren S. and Pitts, Walter},',\n",
       " 'journal={The Bulletin of Mathematical Biophysics},',\n",
       " 'volume={5},',\n",
       " 'number={4},',\n",
       " 'pages={115-133},',\n",
       " 'year={1943},',\n",
       " 'publisher={Springer}',\n",
       " '}',\n",
       " '@article{rosenblatt1960perceptron,',\n",
       " 'title={Perceptron simulation experiments},',\n",
       " 'author={Rosenblatt, Frank},',\n",
       " 'journal={Proceedings of the IRE},',\n",
       " 'volume={48},',\n",
       " 'number={3},',\n",
       " 'pages={301--309},',\n",
       " 'year={1960},',\n",
       " 'publisher={IEEE}',\n",
       " '}',\n",
       " '',\n",
       " '@article{bravyi2022future,',\n",
       " 'title={The future of quantum computing with superconducting qubits},',\n",
       " 'author={Bravyi, Sergey and Dial, Oliver and Gambetta, Jay M and Gil, Dario and Nazario, Zaira},',\n",
       " 'journal={Journal of Applied Physics},',\n",
       " 'volume={132},',\n",
       " 'number={16},',\n",
       " 'pages={160902},',\n",
       " 'year={2022},',\n",
       " 'publisher={AIP Publishing LLC}',\n",
       " '}',\n",
       " '',\n",
       " '@inproceedings{elsayed2019review,',\n",
       " 'title={A review of quantum computer energy efficiency},',\n",
       " 'author={Elsayed, Nelly and Maida, Anthony S and Bayoumi, Magdy},',\n",
       " 'booktitle={2019 IEEE Green Technologies Conference (GreenTech)},',\n",
       " 'pages={1--3},',\n",
       " 'year={2019},',\n",
       " 'organization={IEEE}',\n",
       " '}',\n",
       " '',\n",
       " '@article{wang2013review,',\n",
       " 'title={Review of performance metrics for green data centers: a taxonomy study},',\n",
       " 'author={Wang, Lizhe and Khan, Samee U},',\n",
       " 'journal={The journal of supercomputing},',\n",
       " 'volume={63},',\n",
       " 'pages={639--656},',\n",
       " 'year={2013},',\n",
       " 'publisher={Springer}',\n",
       " '}',\n",
       " '',\n",
       " '@book{vetter2013contemporary,',\n",
       " 'title={Contemporary high performance computing: from Petascale toward exascale},',\n",
       " 'author={Vetter, Jeffrey S},',\n",
       " 'year={2013},',\n",
       " 'publisher={CRC Press}',\n",
       " '}',\n",
       " '',\n",
       " '@article{pawson2022myth,',\n",
       " 'title={The myth of the Harvard architecture},',\n",
       " 'author={Pawson, Richard},',\n",
       " 'journal={IEEE Annals of the History of Computing},',\n",
       " 'volume={44},',\n",
       " 'number={3},',\n",
       " 'pages={59--69},',\n",
       " 'year={2022},',\n",
       " 'publisher={IEEE}',\n",
       " '}',\n",
       " '',\n",
       " '% Modified Hardware architectures',\n",
       " '@article{hennessy1996computer,',\n",
       " 'title={Computer architecture: a quantitative approach},',\n",
       " 'author={Hennessy, John L and Patterson, David A},',\n",
       " 'year={1996},',\n",
       " 'publisher={Morgan Kaufmann}',\n",
       " '}',\n",
       " '',\n",
       " '% Neuromorphjic',\n",
       " '@article{markram2006blue,',\n",
       " 'title={The blue brain project},',\n",
       " 'author={Markram, Henry},',\n",
       " 'journal={Nature Reviews Neuroscience},',\n",
       " 'volume={7},',\n",
       " 'number={2},',\n",
       " 'pages={153--160},',\n",
       " 'year={2006},',\n",
       " 'publisher={Nature Publishing Group UK London}',\n",
       " '}',\n",
       " '',\n",
       " '% IBM TrueNorth',\n",
       " '@article{akopyan2015truenorth,',\n",
       " 'title={Truenorth: Design and tool flow of a 65 mw 1 million neuron programmable neurosynaptic chip},',\n",
       " 'author={Akopyan, Filipp and Sawada, Jun and Cassidy, Andrew and Alvarez-Icaza, Rodrigo and Arthur, John and Merolla, Paul and Imam, Nabil and Nakamura, Yutaka and Datta, Pallab and Nam, Gi-Joon and others},',\n",
       " 'journal={IEEE transactions on computer-aided design of integrated circuits and systems},',\n",
       " 'volume={34},',\n",
       " 'number={10},',\n",
       " 'pages={1537--1557},',\n",
       " 'year={2015},',\n",
       " 'publisher={IEEE}',\n",
       " '}',\n",
       " '',\n",
       " '% Spiking Neural Networks (SNNs) Architecture:',\n",
       " '@article{maass1997networks,',\n",
       " 'title={Networks of spiking neurons: the third generation of neural network models},',\n",
       " 'author={Maass, Wolfgang},',\n",
       " 'journal={Neural networks},',\n",
       " 'volume={10},',\n",
       " 'number={9},',\n",
       " 'pages={1659--1671},',\n",
       " 'year={1997},',\n",
       " 'publisher={Elsevier}',\n",
       " '}',\n",
       " '',\n",
       " '%Liquid State Machines (LSMs) Architecture:',\n",
       " '@article{maass2002real,',\n",
       " 'title={Real-time computing without stable states: A new framework for neural computation based on perturbations},',\n",
       " 'author={Maass, Wolfgang and Natschl{\\\\\"a}ger, Thomas and Markram, Henry},',\n",
       " 'journal={Neural computation},',\n",
       " 'volume={14},',\n",
       " 'number={11},',\n",
       " 'pages={2531--2560},',\n",
       " 'year={2002},',\n",
       " 'publisher={MIT Press}',\n",
       " '}',\n",
       " '',\n",
       " '% Cellular Neural Networks (CNNs) Architecture:',\n",
       " '@inproceedings{chua1988cellular,',\n",
       " 'title={Cellular neural networks: theory},',\n",
       " 'author={Chua, Leon O and Yang, Lin},',\n",
       " 'booktitle={IEEE Transactions on Circuits and Systems},',\n",
       " 'volume={35},',\n",
       " 'number={10},',\n",
       " 'pages={1257--1272},',\n",
       " 'year={1988},',\n",
       " 'organization={IEEE}',\n",
       " '}',\n",
       " '',\n",
       " '%Self-Organizing Maps (SOMs) Architecture:',\n",
       " '@inproceedings{kohonen1982self,',\n",
       " 'title={Self-organized formation of topologically correct feature maps},',\n",
       " 'author={Kohonen, Teuvo},',\n",
       " 'booktitle={Biological cybernetics},',\n",
       " 'volume={43},',\n",
       " 'number={1},',\n",
       " 'pages={59--69},',\n",
       " 'year={1982},',\n",
       " 'organization={Springer}',\n",
       " '}',\n",
       " '',\n",
       " '% Hopfield Networks (HNs) Architecture:',\n",
       " '@article{hopfield1982neural,',\n",
       " 'title={Neural networks and physical systems with emergent collective computational abilities},',\n",
       " 'author={Hopfield, John J},',\n",
       " 'journal={Proceedings of the national academy of sciences},',\n",
       " 'volume={79},',\n",
       " 'number={8},',\n",
       " 'pages={2554--2558},',\n",
       " 'year={1982},',\n",
       " 'publisher={National Acad Sciences}',\n",
       " '}',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '% Radial Basis Function Networks (RBFNs) Architecture:',\n",
       " '@article{park1991universal,',\n",
       " 'title={Universal approximation using radial-basis-function networks},',\n",
       " 'author={Park, Jong-Soon and Sandberg, Irwin W.},',\n",
       " 'journal={Neural computation},',\n",
       " 'volume={3},',\n",
       " 'number={2},',\n",
       " 'pages={246--257},',\n",
       " 'year={1991},',\n",
       " 'publisher={MIT Press}',\n",
       " '}',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '% Neural Turing Machines (NTMs) Architecture:',\n",
       " '@article{graves2014neural,',\n",
       " 'title={Neural turing machines},',\n",
       " 'author={Graves, Alex and Wayne, Greg and Danihelka, Ivo},',\n",
       " 'journal={arXiv preprint arXiv:1410.5401},',\n",
       " 'year={2014}',\n",
       " '}',\n",
       " '',\n",
       " '',\n",
       " '@article{turing_computable_1937,',\n",
       " 'title = {On {Computable} {Numbers}, with an {Application} to the {Entscheidungsproblem}},',\n",
       " 'volume = {s2-42},',\n",
       " 'issn = {00246115},',\n",
       " 'url = {http://doi.wiley.com/10.1112/plms/s2-42.1.230},',\n",
       " 'doi = {10.1112/plms/s2-42.1.230},',\n",
       " 'language = {en},',\n",
       " 'number = {1},',\n",
       " 'urldate = {2023-03-22},',\n",
       " 'journal = {Proceedings of the London Mathematical Society},',\n",
       " 'author = {Turing, A. M.},',\n",
       " ...]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = bib_content.strip().split('\\n')\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filtered_list = [string for string in lines if \"title\" in string]\n",
    "# print(filtered_list)\n",
    "\n",
    "\n",
    "filtered_list = [string for string in lines if string.startswith(\"title\")]\n",
    "\n",
    "# print(filtered_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First draft of a report on the EDVAC,', 'mmpu—a real processing-in-memory architecture to combat the von neumann bottleneck,', 'Green AI,', 'Green AI,', 'Energy and Policy Considerations for Deep Learning in NLP,', 'Inexpensive domain adaptation of pretrained language models: Case studies on biomedical NER and covid-19 QA,', 'Towards Sustainable Artificial Intelligence: An Overview of Environmental Protection Uses and Issues,', 'A Survey on Green Deep Learning,', 'A survey of transformers,', 'Artificial Intelligence-Enabled Science Poetry,', 'Generative adversarial networks,', 'Large scale GAN training for high fidelity natural image synthesis,', 'Use chat gpt to solve programming bugs,', 'A brief overview of ChatGPT: The history, status quo and potential future development,', 'Green AI: do deep learning frameworks have different costs?,', 'The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink, ', 'Deep learning,', 'A Survey of Reinforcement Learning Algorithms for Dynamically Varying Environments,', 'Generative Adversarial Networks in Computer Vision: A Survey and Taxonomy,', 'AutoML: A survey of the state-of-the-art,', 'Can programming be liberated from the von Neumann style?: a functional style and its algebra of programs,', 'Attention is all you need,', 'The connection machine model cm-1 architecture,', 'LIII. \\\\textitOn lines and planes of closest fit to systems of points in space,', 'Nonlinear principal component analysis using autoassociative neural networks,', 'A Shift-Invariant Neural Network for the Lung Field Segmentation in Chest Radiography,', 'A Novel Connectionist System for Unconstrained Handwriting Recognition,', 'Long Short-Term Memory,', 'A definition of ‘carbon footprint’,', 'Adaptive Control Processes: A Guided Tour (R. Bellman),', 'Measuring the environmental impacts of artificial intelligence compute and applications,', 'Anatomy of an AI System,', 'Bert: Pre-training of deep bidirectional transformers for language understanding,', 'Natural Language Processing: A Historical Review,', 'Natural language processing: History, evolution, application, and future work,', 'XLNet: Generalized Autoregressive Pretraining for Language Understanding,', 'Exploring the limits of transfer learning with a unified text-to-text transformer,', 'A primer in BERTology: What we know about how BERT works,', 'Multi-node Bert-pretraining: Cost-efficient Approach,', 'EarlyBERT: Efficient BERT Training via Early-bird Lottery Tickets,', 'Measuring the Effects of Data Parallelism on Neural Network Training,', 'One weird trick for parallelizing convolutional neural networks,', 'Large scale distributed linear algebra with tensor processing units,', 'A Survey on Tensor Techniques and Applications in Machine Learning,', 'Introduction to Tensor Decompositions and their Applications in Machine Learning,', 'Tensor Decompositions and Applications,', 'General tensor decomposition, moment matrices and applications,', 'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,', 'How to Fine-Tune BERT for Text Classification?,', 'On the Energy Consumption of Large-Scale Language Models,', 'Model compression via distillation and quantization,', 'Scalable deep learning on distributed infrastructures: Challenges, techniques, and tools,', 'Knowledge Distillation: A Survey,', 'Deep Residual Learning for Image Recognition, ', 'MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications,', 'Backpropagation applied to handwritten zip code recognition,', 'Quantized convolutional neural networks for mobile devices,', 'Binaryconnect: Training deep neural networks with binary weights during propagations,', 'Structured transforms for small-footprint deep learning,', 'A Logical Calculus of Ideas Immanent in Nervous Activity,', 'Perceptron simulation experiments,', 'The future of quantum computing with superconducting qubits,', 'A review of quantum computer energy efficiency,', 'Review of performance metrics for green data centers: a taxonomy study,', 'Contemporary high performance computing: from Petascale toward exascale,', 'The myth of the Harvard architecture,', 'Computer architecture: a quantitative approach,', 'The blue brain project,', 'Truenorth: Design and tool flow of a 65 mw 1 million neuron programmable neurosynaptic chip,', 'Networks of spiking neurons: the third generation of neural network models,', 'Real-time computing without stable states: A new framework for neural computation based on perturbations,', 'Cellular neural networks: theory,', 'Self-organized formation of topologically correct feature maps,', 'Neural networks and physical systems with emergent collective computational abilities,', 'Universal approximation using radial-basis-function networks,', 'Neural turing machines,', 'On Computable Numbers, with an Application to the Entscheidungsproblem,', 'Neural Turing Machines,', 'Memory augmented recurrent neural networks for de-novo drug design,', 'Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes,', 'Recurrent Neural Networks: Concepts and Applications,', 'Reinforcement Learning Neural Turing Machines - Revised,', 'Evolving Neural Turing Machines for Reward-based Learning,', 'Lie Access Neural Turing Machine,', 'Neural Random-Access Machines,', 'Spanbert: Improving pre-training by representing and predicting spans,', 'ALBERT: A Lite BERT for Self-supervised Learning of Language Representations, ', 'Palm: Scaling language modeling with pathways,', 'Efficient transformer-based large scale language representations using hardware-friendly block structured pruning,', 'Revealing the dark secrets of BERT,', 'A survey on green deep learning,', 'Roberta: A robustly optimized bert pretraining approach,', 'Bert: Pre-training of deep bidirectional transformers for language understanding,', 'Knowledge distillation: A survey,', 'Improving language understanding by generative pre-training,', 'Language models are unsupervised multitask learners,', 'DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter,', 'BioBERT: a pre-trained biomedical language representation model for biomedical text mining,', 'Unsupervised cross-lingual representation learning at scale,', 'Earlybert: Efficient bert training via early-bird lottery tickets,', 'The master algorithm: How the quest for the ultimate learning machine will remake our world,', \"Perceptrons, Reissue of the 1988 Expanded Edition with a new foreword by L\\\\'eon Bottou: An Introduction to Computational Geometry,\", 'Carbontracker: Tracking and Predicting the Carbon Footprint of Training Deep Learning Models,', 'A Green(er) World for A.I.,', 'Carbon emissions and large neural network training,', 'Chasing carbon: The elusive environmental footprint of computing,', 'Sustainable AI: Environmental Implications, Challenges and Opportunities,', 'The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink,', 'Towards the systematic reporting of the energy and carbon footprints of machine learning,', 'Is the future of AI sustainable? A case study of the European Union,', 'Online product safety sweep report,', 'Quantitative AI Risk Assessments: Opportunities and Challenges,', 'Sustainability challenges of Artificial Intelligence and Policy Implications,', 'Sustainable AI: AI for sustainability and the sustainability of AI,', 'Energy and Policy Considerations for Modern Deep Learning Research,', 'Green Algorithms: Quantifying the Carbon Footprint of Computation,', 'Quantifying the Carbon Emissions of Machine Learning,', 'A Multiformalism-Based Model for Performance Evaluation of Green Data Centres,', 'EnergyVis: Interactively Tracking and Exploring Energy Consumption for ML Models, ', 'The lottery ticket hypothesis: Finding sparse, trainable neural networks,', 'A Systematic Review of Green AI,', \"An empirical study of practitioners' perspectives on green software engineering,\", 'Data-Centric Green AI An Exploratory Empirical Study,', 'The role of artificial intelligence in achieving the Sustainable Development Goals,']\n"
     ]
    }
   ],
   "source": [
    "substrings_to_remove = ['{', '}', \"\\ttitle = \", 'title=', 'title = ']\n",
    "\n",
    "for substring_to_remove in substrings_to_remove:\n",
    "\n",
    "    filtered_list = [string.replace(substring_to_remove, \"\") for string in filtered_list]\n",
    "\n",
    "print(filtered_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First draft of a report on the EDVAC,\n",
      "mmpu—a real processing-in-memory architecture to combat the von neumann bottleneck,\n",
      "Green AI,\n",
      "Green AI,\n",
      "Energy and Policy Considerations for Deep Learning in NLP,\n",
      "Inexpensive domain adaptation of pretrained language models: Case studies on biomedical NER and covid-19 QA,\n",
      "Towards Sustainable Artificial Intelligence: An Overview of Environmental Protection Uses and Issues,\n",
      "A Survey on Green Deep Learning,\n",
      "A survey of transformers,\n",
      "Artificial Intelligence-Enabled Science Poetry,\n",
      "Generative adversarial networks,\n",
      "Large scale GAN training for high fidelity natural image synthesis,\n",
      "Use chat gpt to solve programming bugs,\n",
      "A brief overview of ChatGPT: The history, status quo and potential future development,\n",
      "Green AI: do deep learning frameworks have different costs?,\n",
      "The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink, \n",
      "Deep learning,\n",
      "A Survey of Reinforcement Learning Algorithms for Dynamically Varying Environments,\n",
      "Generative Adversarial Networks in Computer Vision: A Survey and Taxonomy,\n",
      "AutoML: A survey of the state-of-the-art,\n",
      "Can programming be liberated from the von Neumann style?: a functional style and its algebra of programs,\n",
      "Attention is all you need,\n",
      "The connection machine model cm-1 architecture,\n",
      "LIII. \\textitOn lines and planes of closest fit to systems of points in space,\n",
      "Nonlinear principal component analysis using autoassociative neural networks,\n",
      "A Shift-Invariant Neural Network for the Lung Field Segmentation in Chest Radiography,\n",
      "A Novel Connectionist System for Unconstrained Handwriting Recognition,\n",
      "Long Short-Term Memory,\n",
      "A definition of ‘carbon footprint’,\n",
      "Adaptive Control Processes: A Guided Tour (R. Bellman),\n",
      "Measuring the environmental impacts of artificial intelligence compute and applications,\n",
      "Anatomy of an AI System,\n",
      "Bert: Pre-training of deep bidirectional transformers for language understanding,\n",
      "Natural Language Processing: A Historical Review,\n",
      "Natural language processing: History, evolution, application, and future work,\n",
      "XLNet: Generalized Autoregressive Pretraining for Language Understanding,\n",
      "Exploring the limits of transfer learning with a unified text-to-text transformer,\n",
      "A primer in BERTology: What we know about how BERT works,\n",
      "Multi-node Bert-pretraining: Cost-efficient Approach,\n",
      "EarlyBERT: Efficient BERT Training via Early-bird Lottery Tickets,\n",
      "Measuring the Effects of Data Parallelism on Neural Network Training,\n",
      "One weird trick for parallelizing convolutional neural networks,\n",
      "Large scale distributed linear algebra with tensor processing units,\n",
      "A Survey on Tensor Techniques and Applications in Machine Learning,\n",
      "Introduction to Tensor Decompositions and their Applications in Machine Learning,\n",
      "Tensor Decompositions and Applications,\n",
      "General tensor decomposition, moment matrices and applications,\n",
      "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,\n",
      "How to Fine-Tune BERT for Text Classification?,\n",
      "On the Energy Consumption of Large-Scale Language Models,\n",
      "Model compression via distillation and quantization,\n",
      "Scalable deep learning on distributed infrastructures: Challenges, techniques, and tools,\n",
      "Knowledge Distillation: A Survey,\n",
      "Deep Residual Learning for Image Recognition, \n",
      "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications,\n",
      "Backpropagation applied to handwritten zip code recognition,\n",
      "Quantized convolutional neural networks for mobile devices,\n",
      "Binaryconnect: Training deep neural networks with binary weights during propagations,\n",
      "Structured transforms for small-footprint deep learning,\n",
      "A Logical Calculus of Ideas Immanent in Nervous Activity,\n",
      "Perceptron simulation experiments,\n",
      "The future of quantum computing with superconducting qubits,\n",
      "A review of quantum computer energy efficiency,\n",
      "Review of performance metrics for green data centers: a taxonomy study,\n",
      "Contemporary high performance computing: from Petascale toward exascale,\n",
      "The myth of the Harvard architecture,\n",
      "Computer architecture: a quantitative approach,\n",
      "The blue brain project,\n",
      "Truenorth: Design and tool flow of a 65 mw 1 million neuron programmable neurosynaptic chip,\n",
      "Networks of spiking neurons: the third generation of neural network models,\n",
      "Real-time computing without stable states: A new framework for neural computation based on perturbations,\n",
      "Cellular neural networks: theory,\n",
      "Self-organized formation of topologically correct feature maps,\n",
      "Neural networks and physical systems with emergent collective computational abilities,\n",
      "Universal approximation using radial-basis-function networks,\n",
      "Neural turing machines,\n",
      "On Computable Numbers, with an Application to the Entscheidungsproblem,\n",
      "Neural Turing Machines,\n",
      "Memory augmented recurrent neural networks for de-novo drug design,\n",
      "Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes,\n",
      "Recurrent Neural Networks: Concepts and Applications,\n",
      "Reinforcement Learning Neural Turing Machines - Revised,\n",
      "Evolving Neural Turing Machines for Reward-based Learning,\n",
      "Lie Access Neural Turing Machine,\n",
      "Neural Random-Access Machines,\n",
      "Spanbert: Improving pre-training by representing and predicting spans,\n",
      "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations, \n",
      "Palm: Scaling language modeling with pathways,\n",
      "Efficient transformer-based large scale language representations using hardware-friendly block structured pruning,\n",
      "Revealing the dark secrets of BERT,\n",
      "A survey on green deep learning,\n",
      "Roberta: A robustly optimized bert pretraining approach,\n",
      "Bert: Pre-training of deep bidirectional transformers for language understanding,\n",
      "Knowledge distillation: A survey,\n",
      "Improving language understanding by generative pre-training,\n",
      "Language models are unsupervised multitask learners,\n",
      "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter,\n",
      "BioBERT: a pre-trained biomedical language representation model for biomedical text mining,\n",
      "Unsupervised cross-lingual representation learning at scale,\n",
      "Earlybert: Efficient bert training via early-bird lottery tickets,\n",
      "The master algorithm: How the quest for the ultimate learning machine will remake our world,\n",
      "Perceptrons, Reissue of the 1988 Expanded Edition with a new foreword by L\\'eon Bottou: An Introduction to Computational Geometry,\n",
      "Carbontracker: Tracking and Predicting the Carbon Footprint of Training Deep Learning Models,\n",
      "A Green(er) World for A.I.,\n",
      "Carbon emissions and large neural network training,\n",
      "Chasing carbon: The elusive environmental footprint of computing,\n",
      "Sustainable AI: Environmental Implications, Challenges and Opportunities,\n",
      "The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink,\n",
      "Towards the systematic reporting of the energy and carbon footprints of machine learning,\n",
      "Is the future of AI sustainable? A case study of the European Union,\n",
      "Online product safety sweep report,\n",
      "Quantitative AI Risk Assessments: Opportunities and Challenges,\n",
      "Sustainability challenges of Artificial Intelligence and Policy Implications,\n",
      "Sustainable AI: AI for sustainability and the sustainability of AI,\n",
      "Energy and Policy Considerations for Modern Deep Learning Research,\n",
      "Green Algorithms: Quantifying the Carbon Footprint of Computation,\n",
      "Quantifying the Carbon Emissions of Machine Learning,\n",
      "A Multiformalism-Based Model for Performance Evaluation of Green Data Centres,\n",
      "EnergyVis: Interactively Tracking and Exploring Energy Consumption for ML Models, \n",
      "The lottery ticket hypothesis: Finding sparse, trainable neural networks,\n",
      "A Systematic Review of Green AI,\n",
      "An empirical study of practitioners' perspectives on green software engineering,\n",
      "Data-Centric Green AI An Exploratory Empirical Study,\n",
      "The role of artificial intelligence in achieving the Sustainable Development Goals,\n"
     ]
    }
   ],
   "source": [
    "for el in filtered_list:\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('titoli.txt', 'w') as f:\n",
    "    for line in filtered_list:\n",
    "        f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
